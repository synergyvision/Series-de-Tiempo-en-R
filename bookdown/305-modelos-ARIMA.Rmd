# Modelos ARIMA

En este capítulo examinaremos el problema de encontrar un modelo apropiado para un conjunto determinado de observaciones $\{x_1,\ldots,x_n\}$ que no son necesariamente generados por una serie de tiempo estacionaria. Si los datos (a) no muestran desviaciones aparentes de la estacionariedad y (b) tienen una función de autocovarianza en rápida disminución, intentamos ajustar un modelo ARMA a los datos medios corregidos utilizando las técnicas desarrolladas en el capítulo de modelos ARMA. De lo contrario, buscamos primero una transformación de los datos que genere una nueva serie con las propiedades (a) y (b). Esto puede lograrse frecuentemente mediante la diferenciación, lo que nos lleva a considerar la clase de modelos ARIMA (siglas en inglés: autoregressive integrated moving-average).

En muchas situaciones, las series de tiempo pueden pensarse o ver como la composición de dos componentes, una componente de tendencia no estacionaria y una componente estacionaria de media cero. Por ejemplo, consideremos el modelo

\begin{equation}
x_t = \mu_t+y_t,
(\#eq:eq-modelo-base-1-diferencia)
\end{equation}

donde $\mu_t=\beta_0+\beta_1t$ y $y_t$ es estacionario. Si diferenciamos este proceso, obtenemos un proceso estacionario, en efecto

\begin{eqnarray*}
\nabla x_t &=& x_t-x_{t-1} = (\mu_t+y_t)-(\mu_{t-1}+y_{t-1}) \\
    &=& (\beta_0+\beta_1t+y_t)-(\beta_0+\beta_1(t-1)+y_{t-1}) \\
    &=& \beta_1 + y_t-y_{t-1} \\
    &=& \beta_1+\nabla y_t.
\end{eqnarray*}

El cual claramente es estacionario.

Otro modelo que lleva a la primera diferenciación es el caso en el cual $\mu_t$ en la ecuación \@ref(eq:eq-modelo-base-1-diferencia), es un proceso estocástico y que varía lentamente de acuerdo a un paseo laeatorio. Esto es,

$$\mu_t=\mu_{t-1}+v_t,$$

donde $v_t$ es estacionario. Tenemos entonces

$$x_t=\mu_{t-1}+v_t+y_t.$$

En este caso,

\begin{eqnarray*}
\nabla x_t &=& x_t-x_{t-1} \\
    &=& (\mu_{t-1}+v_t+y_t)-(\mu_{t-2}+v_{t-1}+y_{t-1}) \\
    &=& v_t+\nabla y_t,
\end{eqnarray*}

es estacionario. Si $\mu_t$ en \@ref(eq:eq-modelo-base-1-diferencia) es un polinomio de grado $k$, $\mu_t=\sum_{i=0}^k\beta_it^i$, entonces la serie diferenciada $\nabla^ky_t$ es estacinaia. Por ejemplo, sea $\mu_t=\beta_0+\beta_1t+\beta_2t^2$, luego

$$x_t=\mu_t+y_t = \beta_0+\beta_1t+\beta_2t^2+y_t.$$

Diferenciando una vez se tiene

\begin{eqnarray*}
\nabla x_t &=& (\mu_t+y_t)-(\mu_{t-1}+y_{t-1}) \\
    &=& (\beta_0+\beta_1t+\beta_2t^2+y_t) - (\beta_0+\beta_1(t-1)+\beta_2(t-1)^2+y_{t-1}) \\
    &=& \beta_0+\beta_1t+\beta_2t^2+y_t-\beta_0-\beta_1t-\beta_1-\beta_2(t^2-2t+1)-y_{t-1} \\
    &=& \beta_1+\beta_2+2\beta_2t+\nabla y_t.
\end{eqnarray*}

Volvemos a diferenciar

\begin{eqnarray*}
\nabla(\nabla x_t) &=& (\beta_1+\beta_2+2\beta_2t+(y_t-y_{t-1})) - (\beta_1+\beta_2+2\beta_2(t-1)+(y_{t-1}-y_{t-2})) \\
    &=& \beta_1+\beta_2+2\beta_2t+y_t-y_{t-1}-\beta_1-\beta_2-2\beta_2t+2\beta_2-y_{t-1}+y_{t-2} \\
    &=& 2\beta_2+\nabla(\nabla y_t) \\
    &=& 2\beta_2+\nabla^2y_t.
\end{eqnarray*}

El cual es estacionario. Los modelos estocásticos con tendencia llevan a diferenciaciones de orden superior. Por ejemplo, supongamos 

$$\mu_t=\mu_{t-1}+v_t\text{ y }v_t=v_{t-1}+e_t$$

donde $e_t$ es estacionario. Entonces $\nabla x_t = v_t+\nabla y_t$ no es estacionario, pero 

$$\nabla^2x_t = e_t+\nabla^2y_t,$$

si es estacionario.

Los modelos ARMA integrados o modelos ARIMA, es una extensión de los modelos ARMA que incluyen diferenciación. Formalmente, tenemos la siguiente definición.

```{definition, defi-modelo-ARIMA}
Un proceso $x_t$ es un proceso ARIMA(p,d,q) si 

$$\nabla^dx_t = (1-B)^dx_t$$
  
es un proceso ARMA(p,q). En general, escribimos el modelo como 

\begin{equation}
  \phi(B)(1-B)^dx_t = \theta(B)w_t.
(\#eq:eq-modelo-ARIMA)
\end{equation}

Si $\mathbb{E}(\nabla^dx_t)=\mu$, escribimos el modelo como

$$\phi(B)(1-B)^dx_t=\delta+\theta(B)w_t,$$

donde $\delta=\mu(1-\phi_1-\cdots-\phi_p)$.
```

----

Debido a la no estacionaridad, debemosser cuidadosos cuando realizamos predicciones.

## Construcción de modelos ARIMA

Hay algunos pasos básicos para ajustar modelos ARIMA a series de tiempo:

- Gráfico de los datos.

- Posible transformación de los datos (diferenciación, logaritmo, etc.).

- Identificación del orden de dependencia del modelo.

- Estimación del (los) parámetro(s).

- Diagnóstico.

- Elección del modelo.

Primero, como en todo análisis de datos, debemos realizar un gráfico de serie de tiempo de los datos e inspeccionar el mismo para ver cualquier anomalía. Si, por ejemplo, la variabilidad crece en el tiempo, será necesario que transformemos los datos para estabilizar la varianza. En este caso, las transformaciones de potencia de la clase Box-Cox resultan útiles. Por ejemplo, supongamos un proceso que evoluciona como un cambio porcentual bastante pequeño, como una inversión. Supongamos que

$$x_t=(1+p_t)x_{t-1},$$

donde $x_t$ es el valor de la inversión en tiempo $t$ y $p_t$ es el porcentaje de cambio del periodo $t-1$ al $t$, el cual puede ser negativo. Tomando logaritmo tenemos

$$\ln(x_t) = \ln(1+p_t)+\ln(x_{t-1})$$

o

$$\nabla\ln(x_t) = \ln(1+p_t).$$

Si el porcentaje de cambio $p_t$ se mantiene relativamente pequeño en magnitud, entonces $\ln(1+p_t)\approx p_t$ [^nota13] y entonces

$$\nabla\ln(x_t)\approx p_t,$$
[^nota13:]$\ln(1+p)=p-\frac{p^2}{2}+\frac{p^3}{3}-\cdots$ para $-1<p\leq1$. Si $p$ es un porcentaje de cambio pequeño, entonces los términos de orden superior en el desarrollo son despreciables.

será un proceso relativamente estable. A menudo $\nabla\ln(x_t)$ se llama el retorno o tasa de crecimiento. Después de una transformación apropiada de los datos, el siguiente paso es identificar los valores preliminares del orden autoregresivo $p$, el orden de diferenciación $d$ y el orden de promedio móvil $q$.

Ya hemos abordado en parte el problema de la elección del orden de diferenciación. El gráfico de la serie de tiempo nos ayudará a deterinar si hace falta una diferenciación. Si se requiere una diferenciación, entonces diferenciamos los datos una vez $(d=1)$, e inspeccionamos el gráfico de $\nabla x_t$. Si hace falta otra diferenciación, entonces volvemos a diferenciar $(d=2)$ e inspeccionamos nuevamente el gráfico, esta vez el de $\nabla^2x_t$. Debemos tener cuidado de no sobre diferenciar pues esto puede introducir dependencia donde no la hay. Por ejemplo, $x_t=w_t$ es no correlacionado, pero $\nabla x_t=w_t-w_{t-1}$ es un proceso $MA(1)$. Además del gráfico de la serie de tiempo, la ACF muestral nos puede ayudar a ver si es necesaria una diferenciación. Dado que el polinomio $\phi(z)(1-z)^d$ tiene raíz unitaria, la ACF muestral $\hat{\rho}(h)$, no decaerá a cero tan rápido cuando $h$ crece. Entonces, un lento decaimiento en $\hat{\rho}(h)$ es un indicativo de que se necesitará una diferenciación.

Una vez que hemos establecido el valor preliminar de $d$, el siguiente paso es ver la ACF y PACF muestrales de $\nabla^dx_t$ para el valor de $d$ elegido. Usando la tabla resumen para la elección de modelos ARMA (véase la tabla al final de la sección [Propiedades de los modelos ARMA(p,q)]) como guía, escogemos los valores preliminares de $p$ y $q$.

Dado que estamos trabajando con estimaciones, dos modelos que luzcan diferentes pueden ser de hecho bastante similares, por lo tanto, nodebemos preocuparnos, de momento, en ser muy precisos al ajustar un modelo. En este punto, unos pocos valores preliminares de $p,d$ y $q$ serán suficientes y nos permitirán iniciar las estimaciones de los parámetros. A continuación daremos un ejemplo de uso de los pasos previamente descritos.

```{example, name="Análisis de datos G.N.P.", ejem-gnp-data}
Consideremos los datos del Producto Nacional Bruto trimestral de EE.UU en miles de millónes de dólares, desde el primer trimestre de 1947 hasta el tercer trimestre de 2002. Son $n=223$ observaciones, losdatos han sido ajustados estacionalmente. El archivo de datos es "gnp96.txt", y fueron obtenidos del *Federal Reserve Bank of St. Louis* (http://research.stlouisfed.org/). El gráfico \@ref(fig:fig-gnp-data) muestra la serie de tiempo correspondiente.
```

```{r, fig.cap="Producto Nacional Bruto trimestral de EE.UU (en miles de millónes de dólares), desde el 1er trimestre de 1947 hasta el 3er trimestre de 2002.", fig-gnp-data}
# Lectura de los datos
gnp=read.table("data/gnp96.txt")
# Grafico de la serie
plot(gnp,type="l", xlab="Años",ylab="G.N.P.",col="blue")
```

Dado que la serie presenta una fuerte tendencia creciente, no es claro si la varianza crece con el tiempo. Por lo tanto para propósito de demostrar como usar la ACF muestral, en la figura \@ref(fig:fig-acf-gnp) mostramos la misma. Como el decaimiento es lento, esto nos sugiere que una diferenciación es posible.

```{r, fig.cap="ACF muestral para la serie de datos G.N.P.", fig-acf-gnp}
# ACF muestral de GNP
acf(gnp[,2], 50)
```

La figura \@ref(fig:fig-gnp-dif-1) muestra la primera diferenciación, allí podemos observar que la variabilidad en la segunda mitad de datos es mayor que en la primera mitad. Además, parece que la tendencia creciente todavía está presente, porlo tanto, tomando en cuenta los pasos descritos al inicio de esta sección primero transformamos losdatos y luego diferenciamos, así obtenemos $y_t=\nabla\ln(x_t)$. 

```{r, fig.cap="Primera diferenciación de la serie de tiempo G.N.P.", fig-gnp-dif-1}
# Primera diferencia
gnpdif=diff(gnp[,2])
plot(gnp[2:223,1],gnpdif, type="l",xlab="Años",ylab="diff(G.N.P.)",
     col="blue")
```

La figura \@ref(fig:fig-gnp-log-dif-1) muestra la serie transformada y diferenciada, podemos ver allí que el proceso parece ser estable. Más aún, podemos interpretar los valores de $y_t$ como el porcentaje de crecimiento trimestral del Producto Nacional Bruto de EE.UU.

```{r,fig.cap="Serie de tiempo de G.N.P. transformada (log) y diferenciada una vez", fig-gnp-log-dif-1}
# Transformacion y primera diferencia
gnpgr = diff(log(gnp[,2]))
plot(gnp[2:223,1],gnpgr,type = "l",xlab="Años",ylab="diff(G.N.P.)",
     col="blue")
```

Graficamos ahora las ACF y PACF muestral de $y_t$. Observando las ACF y PACF parece que la ACF se corta en paso 2 y la PACF decae, lo que nos sugiere un modelo $MA(2)$ para la tasa de crecimiento del P.N.B., o un modelo ARIMA(0,1,2) para $y_t$. Pero en lugar de enfocarnos en un solo modelo, si detallamos las ACF y PACF muestral, parece sugerir que la ACF decrece y la PACF se corta en paso 1, lo que sugiere un modelo $AR(1)$ para la tasa de cambio o un modelo ARIMA(1,1,0) para $y_t$. Podemos decir entonces que un modelo ARIMA(1,1,2) es una primera elección de ajuste.

```{r,fig.cap="ACF y PACF muestrales para las serie de tiempo G.N.P., transformada y diferenciada", fig-acf-pacf-gnp}
# ACF y PACF de la transformacion
par(mfrow=c(2,1))
acf(gnpgr, 24)
pacf(gnpgr,24)
```

A modo de entrenamiento vamos a realizar primeramente los ajustes de los modelos $MA(2)$ y $AR(1)$ por separado. Para ello nos valemos de R, usaremos la función 'sarima' del paquete 'astsa'. Iniciamos con el modelo $AR(1)$.

```{r}
sarima(gnpgr, 1, 0, 0) # AR(1)
```

El modelo $AR(1)$ estimado es $x_t=\mu(1-\phi_1)+\phi_1x_{t-1}+\hat{w}_t$

\begin{equation}
x_t = 0.0083_{(0.001)}(1-0.3467)+0.3467_{(0.063)}x_{t-1}+\hat{w}_t
(\#eq:eq-modelo-AR1-gnp)
\end{equation}

donde $\hat{\sigma}_w=0.0095$ con 220 grados de libertad; note que la constante en \@ref(eq:eq-modelo-AR1-gnp) es $0.0083(1-0.3467)=0.005$. Los valores entre parentesis son los errores estándar estimados.

Ahora usando EMV fijamos un modelo $MA(2)$ para la tasa de crecimiento $x_t$, siendo el modelo estimado

\begin{equation}
x_t = 0.008_{(0.001)}+0.303_{(0.065)}\hat{w}_{t-1}+0204_{(0.064)}\hat{w}_{t-2}+\hat{w}_t
(\#eq:eq-modelo-MA2-gnp)
\end{equation}

donde $\hat{\sigma}_w=0.0094$ con 219 grados de libertad. Los valores entre parentesis corresponden a los errores estándar estimados. Aunque la constante es muy pequeña, su valor es significativo, no incluir una constante lleva a conclusiones erróneas sobre la naturaleza de la economía estadounidense. Si no  incluimos una constane, asumiríamos que la tasa de crecimiento trimestral promedio es cero, mientras que en realidad la tasa de crecimiento trimestral promedio del P.N.B. de EE.UU es de alrededor del 1% (véase la gráfica \@ref(fig:fig-gnp-data)).

```{r,fig.cap="Modelo ARIMA(0,0,2) para la serie G.N.P. y residuales", fig-modelo-arima002-gnp}
gnpgr.ma=sarima(gnpgr, 0, 0, 2) # MA(2)
# Psi-pesos
ARMAtoMA(ar=.3467, ma=0, 10) # prints psi-weights
```

----

El siguiente paso en el ajuste de modelos es el diagnóstico. Esta investigación incluye el análisis de residuales así como la comparación de modelos. De nuevo, el primer paso envuelve un gráfico de las innovaciones (o residuales) $x_t-\hat{x}_t^{t-1}$ o de las innovaciones estandarizadas

\begin{equation}
 e_t = (x_t-\hat{x}_t^{t-1})/\sqrt{\hat{P}_t^{t-1}}
(\#eq:eq-innovaciones-estandar)
\end{equation}

donde $\hat{x}_t^{t-1}$ es la predicción de un paso de $x_t$ basado en el modelo ajustado y $\hat{P}_t^{t-1}$ es varianza del error estimado de un paso. Si el modelo se ajusta bien, el residual estandarizado debe comportarse como una sucesión iid de media cero y varianza uno, así que debemos observar bien el gráfico de la serie de tiempo para ver si hay desviación evidente de esta suposición. Por ejemplo, es posible en el caso no-gaussiano tener un proceso no-correlacionado para el cual valores contiguos en tiempo sean altamente dependientes. Como ejemplo, podemos mencionar la familia de modelos GARCH que discutiremos en el capítulo siguiente.

Para determinar o investigar sobre la normaidad marginal nos valemos del histograma de los residuales, así visualmente podemos ver si el mismo se parece o ajusta a la curva de densidad normal. Además de esto, un gráfico de probabilidad normal o un gráfico de cuantiles (qq-plot) nos puede ayudar a identificar la desviación de la normalidad.

También podemos inspeccionar la autocorrelación muestral de los residuales $\hat{\rho}_e(h)$, para ver algún patrón o valores grandes. Recordemos que, para un ruido blanco, las autocorrelaciones muestrales son aproximadamente independientes y normalmente distribuidas con media cero y varianza $1/n$. Por consiguiente, una buena forma de inspeccionar la estructura de correlación de los residuales es graficar $\hat{\rho}_e(h)$ vs $h$ junto con las cotas de error $\pm2/\sqrt{n}$. Tome en cuenta, sin embargo, que los residuales de un modelo ajustado, no tendrán necesariamente las propiedades de un ruido blanco y la varianza de $\hat{\rho}_e(h)$ puede ser mucho menor que $1/n$.

Además de graficar $\hat{\rho}_e(h)$, podemos realizar una prueba de hipótesis general que tome en consideración las magnitudes de $\hat{\rho}_e(h)$ como grupo. Por ejemplo, puede ser el caso que individualmente cada $\hat{\rho}_e(h)$ sea pequeño en magnitud, digamos menr que $2/\sqrt{n}$ en magnitud, es decir $|\hat{\rho}_e(h)|<2/\sqrt{n}$, pero colectivamente, los valores sean grandes. El estadístico de Ljung-Box-Pierce dado por 

\begin{equation}
 Q = n(n+2)\sum_{h=1}^H\frac{\hat{\rho}_e^2(h)}{n-h}
 (\#eq:eq-estadistico-ljung-box-pierce)
\end{equation}

es útil para realizar esta prueba de hipótesis. El  valor $H$ en \@ref(eq:eq-estadistico-ljung-box-pierce) se elige de manera arbitraria, en general se usa $H=20$. Bajo la hipótesis nula de que el modelo es adecuado, asintóticamente, (cuando $n\to\infty$), $Q$ se distribuye como una chi-cuadrado con $H-p-q$ grados de libertad, esto es $Q\sim\chi_{H-p-q}^2$. Entonces, rechazamos la hipótesis nula a nivel $\alpha$ si el valor  de $Q$ es mayor que la $\chi_{H-p-q}^2(1-\alpha)$.

```{example, name="Diagnóstico para la tasa de crecimiento del P.N.B", diagnostico-tasa-crecimiento-gnp}
Enfoquémonos en el modelo $MA(2)$ ajustado del ejemplo \@ref(exm:ejem-gnp-data), el análisis de los residuales de $AR(1)$ es similar. La figura \@ref(fig:fig-modelo-arima002-gnp) muestra el gráfico de los residuales estandarizados (parte superior), la ACF de los residuales (parte media izquierda) (Note que R incluye la correlación en paso cero que siempre es uno) y los valores del estadístico $Q$, dado en \@ref(eq:eq-estadistico-ljung-box-pierce) desde paso $H=1$ hasta $H=20$ (parte inferior). 
```

```{r}
tsdiag(gnpgr.ma,gof.lag=20)
```

Observando el gráfico de los residuales estandarizados en \@ref(fig:fig:diagnostico-residual-MA2-gnp), no muestra un patrón obvio. Note que no hay valores atípicos pero si algunos pocos valores mayores que 3 desviaciones estándar. La ACF de los residuales no muestra una aparente desviación de la suposición del modelo, y el estadístico $Q$ no es significativo para los primeros 20 pasos calculados.

Finalmente la figura \@ref(fig:fig-histograma-qq-plot-residual-gnp) muestra un histograma de los residuales (parte superior) y un gráfico qq-plot de los residuales (parte inferior). En la misma podemos observar que los residuales están cercanos a la normalidad excepto para algunos valores extremos en la cola

```{r,fig.cap="Histograma de los residuales para P.N.B. (parte superior), y qq-plot de los residuales (parte inferior), para el modelo MA(2)", fig-histograma-qq-plot-residual-gnp}
par(mfrow=c(2,1))
hist(gnpgr.ma$resid,br=12)
qqnorm(gnpgr.ma$resid)
```

Para concluir, realizamos la prueba de Shapiro-Wilk (referencia), la cual nos da un $p$-valor de 0.003, lo que indica que los residuales no son normal. Por lo tanto, el modelo parece ajustarse bien salvo que debemos usar una distribución con una cola más pesada que la distribución normal. El comando en R para la prueba de Shapiro-Wilk es

```{r}
shapiro.test(gnpgr.ma$resid)
```
----

Podemos hacer lo mismo ahora para el modelo ARIMA(1,1,2)
```{r, fig.cap="diagnóstico para el modelo ARIMA de B.N.P", fig-diagnostico-gnp-arima}
gnpgr.arima=arima(gnpgr,order=c(1,1,2))
tsdiag(gnpgr.arima$resid,gof.lag=20)
```

Explicación ...

```{r, fig.cap="Histograma de los residuales para P.N.B. (parte superior), y qq-plot de los residuales (parte inferior), para el modelo ARIMA(1,1,2)",fig-histograma-qq-plot-residual-gnp-arima}
par(mfrow=c(2,1))
hist(gnpgr.arima$resid)
qqnorm(gnpgr.arima$resid)
shapiro.test(gnpgr.arima)
```

Explicación...

----

Como explicamos previamente, debemos tener cuidado con sobreajustar un modelo; no siempre es el caso que más es mejor. Sobreajustar nos lleva a estimadores menos preciso, y agregar más parámetros puede ajustar mejor los datos pero puede llevar a malas predicciones. Este resultado se ilustra en el ejemplo siguiente.

```{example, name="Un problema de sobreajuste", ejem-problema-sobreajuste}
La figura \@ref(fig-sobreajuste-poblacion-usa), muestra  la población de los EE.UU., según el censo oficial cada 10 años de 1910 hasta 1990 (puntos azules). Si usamos estos nueve puntos para predecir la población a futuro de los EE.UU. podemos usar un polinomio de grado 8 para ajustar las 9 observaciones; lo cual como se observa en la gráfica es perfecta. El modelo en este caso es 

$$x_t=\beta_0+\beta_1t+\beta_2t^2+\cdots+\beta_8t^8+w_t$$

El modelo fijado, el cual es graficado hasta el año 2010, (linea continua roja), pasa a través de los 9 puntos. El modelo predice que la población de los EE.UU. estará cercana a cero en el año 2000, y cruzará el cero en algún mes del año 2002, lo cual es falso.
```

```{r}
uspop=read.table("data/USPOP2.txt", header = TRUE)
fit.usp=lm(Pob~t+I(t^2)+I(t^3)+I(t^4)+I(t^5)+I(t^6)+I(t^7)+I(t^8),
           data=uspop)
plot(uspop,type="p",lty=19,col="blue",xlim=c(1910,2010))
lines(x = uspop$t, y=predict(fit.usp), col = "red", lwd = 2)
```
----

El paso final en el ajuste de modelos es la elección del modelo. Esto es, debemos decidir que modelo mantendremos para la predicción. La técnica más popular es calcular los índices AIC, AICc y SIC (BIC), descritos en (referencias).

```{example, name="Elección del modelo para la serie P.N.B. de EE.UU.", ejem-eleccion-modelo-gnp}
Volviendo al análisis del P.N.B. de EE.UU., visto en los ejemplos () y (), recordemos que los modelos son AR(1), MA(2) y ARIMA(1,1,2). Para escoger el modelo final, comparemos los valores del AIC, AICc y SIC para los 3 modelos
```
```{r}
n=length(gnpgr)
# Modelos AR
kar=length(gnpgr.ar$coef)
sar=gnpgr.ar$sigma2
# Modelos MA
kma=length(gnpgr.ma$coef)
sma=gnpgr.ma$sigma2
# Modelo ARIMA
karima=length(gnpgr.arima$coef)
sarima=gnpgr.arima$sigma2
# AIC
log(sar)+(n+2*kar)/n
log(sma)+(n+2*kma)/n
log(sarima)+(n+2*karima)/n
# AICc
log(sar)+(n+kar)/(n-kar-2)
log(sma)+(n+kma)/(n-kma-2)
log(sarima)+(n+karima)/(n-karima-2)
# BIC (SIC)
log(sar)+kar*log(n)/n
log(sma)+kma*log(n)/n
log(sarima)+karima*log(n)/n
```
----


## Modelos SARIMA

En esta sección vamos a introducir diversas modificaciones a los modelos ARIMA para que se ajusten a comportamiento estacional y no-estacionario. 
