<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 6 Modelos ARMA | Series de Tiempo en R</title>
  <meta name="description" content="Capítulo 6 Modelos ARMA | Series de Tiempo en R" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 6 Modelos ARMA | Series de Tiempo en R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://synergy.vision/Series-de-Tiempo-en-R/" />
  <meta property="og:image" content="http://synergy.vision/Series-de-Tiempo-en-R/images/cover.png" />
  
  <meta name="github-repo" content="synergyvision/Series-de-Tiempo-en-R/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 6 Modelos ARMA | Series de Tiempo en R" />
  
  
  <meta name="twitter:image" content="http://synergy.vision/Series-de-Tiempo-en-R/images/cover.png" />

<meta name="author" content="Synergy Vision" />


<meta name="date" content="2022-11-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modelos-ma.html"/>
<link rel="next" href="referencias.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="images/logovision-black.png" width="160"></img></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#por-qué-leer-este-libro"><i class="fa fa-check"></i>¿Por qué leer este libro?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#estructura-del-libro"><i class="fa fa-check"></i>Estructura del libro</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#información-sobre-los-programas-y-convenciones"><i class="fa fa-check"></i>Información sobre los programas y convenciones</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prácticas-interactivas-con-r"><i class="fa fa-check"></i>Prácticas interactivas con R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agradecimientos"><i class="fa fa-check"></i>Agradecimientos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="1" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introducción.html"><a href="introducción.html#conceptos-financieros-básicos"><i class="fa fa-check"></i><b>1.1</b> Conceptos financieros básicos</a></li>
<li class="chapter" data-level="1.2" data-path="introducción.html"><a href="introducción.html#conceptos-básicos"><i class="fa fa-check"></i><b>1.2</b> Conceptos básicos</a></li>
<li class="chapter" data-level="1.3" data-path="introducción.html"><a href="introducción.html#ejemplos"><i class="fa fa-check"></i><b>1.3</b> Ejemplos</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introducción.html"><a href="introducción.html#clasificación-de-las-series-de-tiempo"><i class="fa fa-check"></i><b>1.3.1</b> Clasificación de las series de tiempo</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introducción.html"><a href="introducción.html#componentes-de-una-serie-de-tiempo"><i class="fa fa-check"></i><b>1.4</b> Componentes de una serie de tiempo</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introducción.html"><a href="introducción.html#el-modelo-aditivo-de-componentes-de-series-de-tiempo"><i class="fa fa-check"></i><b>1.4.1</b> El Modelo Aditivo de Componentes de Series de Tiempo</a></li>
<li class="chapter" data-level="1.4.2" data-path="introducción.html"><a href="introducción.html#el-modelo-multiplicativo-de-componentes-de-series-de-tiempo"><i class="fa fa-check"></i><b>1.4.2</b> El Modelo Multiplicativo de Componentes de Series de Tiempo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="características-de-series-de-tiempo.html"><a href="características-de-series-de-tiempo.html"><i class="fa fa-check"></i><b>2</b> Características de series de tiempo</a><ul>
<li class="chapter" data-level="2.1" data-path="características-de-series-de-tiempo.html"><a href="características-de-series-de-tiempo.html#medidas-de-dependencia-para-series-de-tiempo"><i class="fa fa-check"></i><b>2.1</b> Medidas de dependencia para series de tiempo</a></li>
<li class="chapter" data-level="2.2" data-path="características-de-series-de-tiempo.html"><a href="características-de-series-de-tiempo.html#estimación-de-la-tendencia"><i class="fa fa-check"></i><b>2.2</b> Estimación de la Tendencia</a><ul>
<li class="chapter" data-level="2.2.1" data-path="características-de-series-de-tiempo.html"><a href="características-de-series-de-tiempo.html#estimación-de-la-tendencia-en-ausencia-de-estacionalidad"><i class="fa fa-check"></i><b>2.2.1</b> Estimación de la tendencia en ausencia de estacionalidad</a></li>
<li class="chapter" data-level="2.2.2" data-path="características-de-series-de-tiempo.html"><a href="características-de-series-de-tiempo.html#estimación-de-la-tendencia-y-la-estacionalidad"><i class="fa fa-check"></i><b>2.2.2</b> Estimación de la tendencia y la estacionalidad</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="características-de-series-de-tiempo.html"><a href="características-de-series-de-tiempo.html#estimación-de-la-tendencia-por-regresión-clásica"><i class="fa fa-check"></i><b>2.3</b> Estimación de la tendencia por regresión clásica</a><ul>
<li class="chapter" data-level="2.3.1" data-path="características-de-series-de-tiempo.html"><a href="características-de-series-de-tiempo.html#regresión-clásica"><i class="fa fa-check"></i><b>2.3.1</b> Regresión Clásica</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modelos-de-series-de-tiempo.html"><a href="modelos-de-series-de-tiempo.html"><i class="fa fa-check"></i><b>3</b> Modelos de series de tiempo</a><ul>
<li class="chapter" data-level="3.1" data-path="modelos-de-series-de-tiempo.html"><a href="modelos-de-series-de-tiempo.html#modelos-estocásticos"><i class="fa fa-check"></i><b>3.1</b> Modelos Estocásticos</a><ul>
<li class="chapter" data-level="3.1.1" data-path="modelos-de-series-de-tiempo.html"><a href="modelos-de-series-de-tiempo.html#procesos-estocásticos"><i class="fa fa-check"></i><b>3.1.1</b> Procesos Estocásticos</a></li>
<li class="chapter" data-level="3.1.2" data-path="modelos-de-series-de-tiempo.html"><a href="modelos-de-series-de-tiempo.html#momentos-varianza-covarianza-y-correlación"><i class="fa fa-check"></i><b>3.1.2</b> Momentos, Varianza, Covarianza y Correlación</a></li>
<li class="chapter" data-level="3.1.3" data-path="modelos-de-series-de-tiempo.html"><a href="modelos-de-series-de-tiempo.html#variación-de-un-proceso"><i class="fa fa-check"></i><b>3.1.3</b> Variación de un proceso</a></li>
<li class="chapter" data-level="3.1.4" data-path="modelos-de-series-de-tiempo.html"><a href="modelos-de-series-de-tiempo.html#martingalas"><i class="fa fa-check"></i><b>3.1.4</b> Martingalas</a></li>
<li class="chapter" data-level="3.1.5" data-path="modelos-de-series-de-tiempo.html"><a href="modelos-de-series-de-tiempo.html#propiedad-de-markov"><i class="fa fa-check"></i><b>3.1.5</b> Propiedad de Markov</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="modelos-de-series-de-tiempo.html"><a href="modelos-de-series-de-tiempo.html#modelos-lineales"><i class="fa fa-check"></i><b>3.2</b> Modelos lineales</a><ul>
<li class="chapter" data-level="3.2.1" data-path="modelos-de-series-de-tiempo.html"><a href="modelos-de-series-de-tiempo.html#proceso-de-ruido-blanco"><i class="fa fa-check"></i><b>3.2.1</b> Proceso de Ruido Blanco</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modelos-ar.html"><a href="modelos-ar.html"><i class="fa fa-check"></i><b>4</b> Modelos AR</a><ul>
<li class="chapter" data-level="4.1" data-path="modelos-ar.html"><a href="modelos-ar.html#modelo-ar1"><i class="fa fa-check"></i><b>4.1</b> Modelo AR(1)</a></li>
<li class="chapter" data-level="4.2" data-path="modelos-ar.html"><a href="modelos-ar.html#modelo-ar2"><i class="fa fa-check"></i><b>4.2</b> Modelo AR(2)</a></li>
<li class="chapter" data-level="4.3" data-path="modelos-ar.html"><a href="modelos-ar.html#procesos-arp"><i class="fa fa-check"></i><b>4.3</b> Procesos AR(p)</a></li>
<li class="chapter" data-level="4.4" data-path="modelos-ar.html"><a href="modelos-ar.html#función-de-autocorrelación-parcial"><i class="fa fa-check"></i><b>4.4</b> Función de Autocorrelación Parcial</a></li>
<li class="chapter" data-level="4.5" data-path="modelos-ar.html"><a href="modelos-ar.html#criterios-de-información"><i class="fa fa-check"></i><b>4.5</b> Criterios de Información</a></li>
<li class="chapter" data-level="4.6" data-path="modelos-ar.html"><a href="modelos-ar.html#estimación-de-parámetros."><i class="fa fa-check"></i><b>4.6</b> Estimación de Parámetros.</a></li>
<li class="chapter" data-level="4.7" data-path="modelos-ar.html"><a href="modelos-ar.html#predicciones-con-modelos-ar"><i class="fa fa-check"></i><b>4.7</b> Predicciones con modelos AR</a><ul>
<li class="chapter" data-level="4.7.1" data-path="modelos-ar.html"><a href="modelos-ar.html#predicción-de-un-paso"><i class="fa fa-check"></i><b>4.7.1</b> Predicción de un paso</a></li>
<li class="chapter" data-level="4.7.2" data-path="modelos-ar.html"><a href="modelos-ar.html#predicción-de-dos-pasos"><i class="fa fa-check"></i><b>4.7.2</b> Predicción de dos pasos</a></li>
<li class="chapter" data-level="4.7.3" data-path="modelos-ar.html"><a href="modelos-ar.html#predicción-de-múltiples-pasos"><i class="fa fa-check"></i><b>4.7.3</b> Predicción de múltiples pasos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modelos-ma.html"><a href="modelos-ma.html"><i class="fa fa-check"></i><b>5</b> Modelos MA</a><ul>
<li class="chapter" data-level="5.1" data-path="modelos-ma.html"><a href="modelos-ma.html#propiedades-de-los-modelos-ma"><i class="fa fa-check"></i><b>5.1</b> Propiedades de los modelos MA</a><ul>
<li class="chapter" data-level="5.1.1" data-path="modelos-ma.html"><a href="modelos-ma.html#estacionaridad"><i class="fa fa-check"></i><b>5.1.1</b> Estacionaridad</a></li>
<li class="chapter" data-level="5.1.2" data-path="modelos-ma.html"><a href="modelos-ma.html#función-de-autocorrelación-acf"><i class="fa fa-check"></i><b>5.1.2</b> Función de autocorrelación (ACF)</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="modelos-ma.html"><a href="modelos-ma.html#identificación-del-orden-de-un-ma"><i class="fa fa-check"></i><b>5.2</b> Identificación del orden de un MA</a></li>
<li class="chapter" data-level="5.3" data-path="modelos-ma.html"><a href="modelos-ma.html#estimación"><i class="fa fa-check"></i><b>5.3</b> Estimación</a></li>
<li class="chapter" data-level="5.4" data-path="modelos-ma.html"><a href="modelos-ma.html#predicciones-usando-modelos-ma"><i class="fa fa-check"></i><b>5.4</b> Predicciones usando modelos MA</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-arma.html"><a href="modelos-arma.html"><i class="fa fa-check"></i><b>6</b> Modelos ARMA</a><ul>
<li class="chapter" data-level="6.1" data-path="modelos-arma.html"><a href="modelos-arma.html#propiedades-de-los-modelos-armapq"><i class="fa fa-check"></i><b>6.1</b> Propiedades de los modelos ARMA(p,q)</a></li>
<li class="chapter" data-level="6.2" data-path="modelos-arma.html"><a href="modelos-arma.html#ecuaciones-en-diferencias"><i class="fa fa-check"></i><b>6.2</b> Ecuaciones en Diferencias</a></li>
<li class="chapter" data-level="6.3" data-path="modelos-arma.html"><a href="modelos-arma.html#la-densidad-espectral"><i class="fa fa-check"></i><b>6.3</b> La Densidad Espectral</a></li>
<li class="chapter" data-level="6.4" data-path="modelos-arma.html"><a href="modelos-arma.html#periodograma-y-transformada-discreta-de-fourier"><i class="fa fa-check"></i><b>6.4</b> Periodograma y Transformada Discreta de Fourier</a></li>
<li class="chapter" data-level="6.5" data-path="modelos-arma.html"><a href="modelos-arma.html#estimación-espectral-no-paramétrica"><i class="fa fa-check"></i><b>6.5</b> Estimación Espectral No-paramétrica</a></li>
<li class="chapter" data-level="6.6" data-path="modelos-arma.html"><a href="modelos-arma.html#procesos-de-incremento-ortogonal-sobre--pipi"><i class="fa fa-check"></i><b>6.6</b> Procesos de Incremento Ortogonal sobre <span class="math inline">\([-\pi,\pi]\)</span></a></li>
<li class="chapter" data-level="6.7" data-path="modelos-arma.html"><a href="modelos-arma.html#integración-con-respecto-a-un-proceso-de-incremento-ortogonal"><i class="fa fa-check"></i><b>6.7</b> Integración con Respecto a un Proceso de Incremento Ortogonal</a><ul>
<li class="chapter" data-level="6.7.1" data-path="modelos-arma.html"><a href="modelos-arma.html#propiedades-de-la-integral-estocástica"><i class="fa fa-check"></i><b>6.7.1</b> Propiedades de la Integral Estocástica</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="modelos-arma.html"><a href="modelos-arma.html#la-representación-espectral"><i class="fa fa-check"></i><b>6.8</b> La Representación Espectral</a></li>
</ul></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Series de Tiempo en R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modelos-arma" class="section level1">
<h1><span class="header-section-number">Capítulo 6</span> Modelos ARMA</h1>
<p>En el capítulo 2, introdujimos las funciones de autocorrelación y correlación cruzada (ACFs y CCFs) como herramientas para clarificar las relaciones que pueden ocurrir dentro y entre las series de tiempo en varios rezagos. Además, explicamos cómo construir modelos lineales basado en la teoría clásica de regresión para la explotación de las asociaciones indicadas por los valores grandes de la ACF o CCF. Los métodos de este capítulo, en dominio del tiempo, o de regresión, son apropiados cuando se trata de posiblemente no estacionaridad con series de tiempo cortas; estas series son la regla y no la excepción en muchas aplicaciones.</p>
<p>La regresión clásica es a menudo insuficiente para explicar todas las dinámicas interesantes de una serie de tiempo. Por ejemplo, la ACF de los residuos del ajuste de regresión lineal simple a los datos globales de la temperatura (véase el Ejemplo <a href="características-de-series-de-tiempo.html#exm:ejem-temperatura-global">2.6</a> del capítulo 2, Sección 2.3.1) revela una estructura adicional en los datos que la regresión no captura. En lugar de ello, la introducción de correlación como un fenómeno que se puede generar a través de relaciones lineales retardadas lleva a proponer los modelos autorregresivo (AR) y autorregresivo de promedio móvil (ARMA). Añadiendo modelos no estacionarios a la combinación conduce al modelo autorregresivo integrado de media móvil (ARIMA) popularizado en el destacado trabajo de Box y Jenkins (1970). El método de Box-Jenkins para la identificación de un posible modelo ARIMA se da en el siguiente capítulo junto con técnicas para la estimación de parámetros y la previsión para estos modelos.</p>
<p>El modelo de regresión clásico del Capítulo 3 fue desarrollado para el caso estático, es decir, que sólo permite que la variable dependiente sea influenciada por los valores actuales de las variables independientes. En el caso de series de tiempo, es deseable permitir que la variable dependiente sea influenciada por los valores pasados de las variables independientes y posiblemente por sus propios valores pasados. Si el presente puede ser modelado plausiblemente en términos de sólo los valores pasados de las entradas independientes, tenemos la atractiva posibilidad de que la predicción será posible.</p>
<p>Ahora procederemos con un desarrollo más general de modelos autoregresivos, de promedio móvil y mezcla de ambos modelos para series de tiempo estacionarias.</p>

<div class="definition">
<p><span id="def:defi-modelo-ARMA" class="definition"><strong>Definición 6.1  </strong></span>Una serie de tiempo <span class="math inline">\(\{x_t; t=0,\pm1,\pm2,\ldots\}\)</span> es un <em>proceso autoregresivo de promedio móvil</em>, denotado <span class="math inline">\(ARMA(p,q)\)</span>, si es estacionario y</p>
<span class="math display" id="eq:eq-modelo-ARMA">\[\begin{equation}
    x_t=\phi_1x_{t-1}+\cdots+\phi_px_{t-p}+w_t+\theta_1w_{t-1}+\cdots+\theta_qw_{t-q}
\tag{6.1}
\end{equation}\]</span>
<p>con <span class="math inline">\(\phi_p\neq0,\theta_q\neq0\)</span> y <span class="math inline">\(\sigma_w^2&gt;0\)</span>. Los parámetros <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span> son llamados ordenes autoregresivos y de promedio móvil respectivamente. Si <span class="math inline">\(x_t\)</span> tiene media <span class="math inline">\(\mu\)</span> distinto de cero, hacemos <span class="math inline">\(\alpha=\mu(1-\phi_1-\cdots-\phi_p)\)</span> y escribimos el modelo como</p>
<span class="math display" id="eq:eq-modelo-ARMA-media-no-cero">\[\begin{equation}
    x_t=\alpha+\phi_1x_{t-1}+\cdots+\phi_px_{t-p}+w_t+\theta_1w_{t-1}+\cdots+\theta_qw_{t-q}
\tag{6.2}
\end{equation}\]</span>
A menos que se declare lo contrario, <span class="math inline">\(\{w_t;t=0,\pm1,\pm2,\ldots\}\)</span> es una sucesión de ruido blanco gaussiano.
</div>

<hr />
<p>Como se observó previamente, cuando <span class="math inline">\(q=0\)</span>, el modelo es llamado modelo autoregresivo de orden <span class="math inline">\(p\)</span>, AR(p), y cuando <span class="math inline">\(p=0\)</span> el modelo es llamado modelo de promedio móvil de orden <span class="math inline">\(q\)</span> MA(q). Como ayuda en la investigación de los modelos ARMA, será útil escribir estos usando el operador AR <a href="modelos-ar.html#eq:eq-operador-autoregresivo">(4.5)</a> y el operador MA <a href="modelos-ma.html#eq:eq-operador-promedio-movil">(5.3)</a>. En particular el modelo ARMA(p,q) en <a href="modelos-arma.html#eq:eq-modelo-ARMA">(6.1)</a> se puede escribir en forma concisa como</p>
<span class="math display" id="eq:eq-modelo-ARMA-conciso">\[\begin{equation}
    \phi(B)x_t=\theta(B)w_t
\tag{6.3}
\end{equation}\]</span>
<p>Antes de discutir las condiciones bajo la cual <a href="modelos-arma.html#eq:eq-modelo-ARMA">(6.1)</a> es causal e invertible, veamos un potencial problema con el modelo ARMA.</p>

<div class="example">
<p><span id="exm:ejem-modelo-econometria" class="example"><strong>Ejemplo 6.1  </strong></span>En econometría, es usual considerar modelos dinámicos del siguiente tipo; llamemos <span class="math inline">\(x_1,\ldots,x_r\)</span> a la característica numérica de interés en un sector económico arbitrario dado (precios, nivel de producción, ingresos, inversiones, etc.), asumiendo por ejemplo que las observaciones son anuales. Para el año <span class="math inline">\(n\)</span> existe un vector asociado <span class="math inline">\(X(n)\)</span> con coordenadas <span class="math inline">\(x_1(n),\ldots,x_r(n)\)</span>. Asumimos que <span class="math inline">\(X(n)\)</span> verifica la recursión lineal del tipo <span class="math display">\[X(n) = A_0X(n)+A_1X(n-1)+\cdots+A_jX(n-j)\]</span> donde <span class="math inline">\(A_0,\ldots,A_j\)</span> son matrices, pero esa relación  es perturbada por un efecto aleatorio <span class="math inline">\(w(n)\)</span> con media cero, no correlacionado para diferentes años. Entonces <span class="math inline">\(X(n)\)</span> llega a ser un vector aleatorio que verifica</p>
<span class="math display" id="eq:eq-vector-aleatorio-ejemplo-econometria">\[\begin{equation}
X(n) = \sum_{k=0}^jA_kX(n-k) + w(n).
\tag{6.4}
\end{equation}\]</span>
<p>Se puede demostrar, con unas pocas restricciones sobre los <span class="math inline">\(A_k\)</span>, que si <span class="math inline">\(w(n)\)</span> es un ruido blanco, entonces existe un proceso estacionario <span class="math inline">\(X(n)\)</span> que satisface <a href="modelos-arma.html#eq:eq-vector-aleatorio-ejemplo-econometria">(6.4)</a> tal que para cada <span class="math inline">\(i=1,\ldots,r\)</span>, <span class="math inline">\(x_i(n)\)</span> es un proceso ARMA.</p>
</div>


<div class="example">
<p><span id="exm:ejem-redundancia-parametro" class="example"><strong>Ejemplo 6.2  (Redundancia de Parámetro)  </strong></span>Considere un proceso de ruido blanco <span class="math inline">\(x_t=w_t\)</span>. Equivalentemente podemos escribir este como <span class="math inline">\(0.5x_{t-1}=0.5w_{t-1}\)</span> utilizando el operador de cambio una vez y multiplicando por 0.5. Ahora, restamos las dos representaciones para obtener</p>
<span class="math display" id="eq:eq-redundancia-parametro-ARMA11">\[\begin{equation}
    x_t-0.5x_{t-1}=w_t-0.5w_{t-1}\text{ ó }x_t=0.5x_{t-1}-0.5w_{t-1}+w_t
\tag{6.5}
\end{equation}\]</span>
<p>el cual luce como un modelo ARMA(1,1). De hecho, <span class="math inline">\(x_t\)</span> es todavía un ruido blanco; nada ha cambiado en esta consideración [esto es, <span class="math inline">\(x_t=w_t\)</span> es la solución de <a href="modelos-arma.html#eq:eq-redundancia-parametro-ARMA11">(6.5)</a>, pero hemos escondido el hecho de que <span class="math inline">\(x_t\)</span> es un ruido blanco debido al parámetro de redundancia o sobre-parametrización.</p>
<p>Escribiendo el parámetro de redundancia en la forma de operador como <span class="math inline">\(\phi(B)x_t=\theta(B)w_t\)</span> o <span class="math display">\[(1-0.5B)x_t=(1-0.5B)w_t\]</span> Aplicando el operador <span class="math inline">\(\phi(B)^{-1}=(1-0.5B)^{-1}\)</span> a ambos lados de la igualdad, obtenemos <span class="math display">\[x_t=(1-0.5B)^{-1}(1-0.5B)x_t=(1-0.5B)^{-1}(1-0.5B)w_t=w_t\]</span> el cual es el modelo original.</p>
<p>Podemos fácilmente detectar el problema de sobre-parametrización con el uso de los operadores o sus polinomios asociados. Esto es, escribimos el polinomio AR <span class="math inline">\(\phi(z)=(1-0.5z)\)</span>, el polinomio MA <span class="math inline">\(\theta(z)=(1-0.5z)\)</span> y note que ambos polinomios tienen un factor común, este es <span class="math inline">\((1-0.5z)\)</span>. Este factor común identifica los parámetros de redundancia de inmediato. Descartando el factor en cada uno, nos queda <span class="math inline">\(\phi(z)=1\)</span> y <span class="math inline">\(\theta(z)=1\)</span> de donde se concluye que <span class="math inline">\(\phi(B)=1\)</span> y <span class="math inline">\(\theta(B)=1\)</span>, y deducimos que el modelo es un ruido blanco. La consideración de parámetros de redundancia será crucial cuando discutamos la estimación de modelo ARMA en general. Como apuntó este ejemplo, podemos fijar un modelo ARMA(1,1) a un ruido blanco y conseguir que los parámetros estimados sean significativos. Si no fuéramos conscientes de la redundancia de parámetros, se podría alegar que los datos están correlacionados, cuando en realidad no lo son.</p>
</div>

<hr />
<p>Los ejemplos <a href="modelos-ar.html#exm:ejem-modelo-AR-explosivo-causal">4.1</a>, <a href="modelos-ma.html#exm:ejem-no-unicidad-MA">5.1</a> y <a href="modelos-arma.html#exm:ejem-redundancia-parametro">6.2</a> señalan un número de problemas con la definición general de los modelos ARMA(p,q) dados por <a href="modelos-arma.html#eq:eq-modelo-ARMA">(6.1)</a> o equivalentemente por <a href="modelos-arma.html#eq:eq-modelo-ARMA-conciso">(6.3)</a>. En resumen, tenemos los siguientes problemas:</p>
<ol style="list-style-type: decimal">
<li><p>Modelos de parámetros redundantes,</p></li>
<li><p>Modelos AR estacionarios que dependen del futuro, y</p></li>
<li><p>Modelos MA que no son únicos.</p></li>
</ol>
<p>Para resolver estos problemas, requeriremos algunas restricciones adicionales sobre los parámetros de los modelos, pero primero, daremos las siguientes definiciones:</p>

<div class="definition">
<p><span id="def:defi-polinomios-AR-MA" class="definition"><strong>Definición 6.2  </strong></span>Los <em>Polinomios AR y MA</em> se definen como</p>
<span class="math display" id="eq:eq-polinomio-AR">\[\begin{equation}
    \phi(z)=1-\phi_1z-\cdots-\phi_pz^p\text{, }\phi_p\neq0
\tag{6.6}
\end{equation}\]</span>
<p>y</p>
<span class="math display" id="eq:eq-polinomio-MA">\[\begin{equation}
    \theta(z)=1+\theta_1z+\cdots+\theta_qz^q\text{, }\theta_q\neq0
\tag{6.7}
\end{equation}\]</span>
respectivamente, donde <span class="math inline">\(z\)</span> es un número complejo.
</div>

<hr />
<p>Para abordar el primer problema (Modelos de parámetros redundantes), de ahora en adelante nos referiremos a un modelo <span class="math inline">\(ARMA(p,q)\)</span> el sentido de su forma más simple. Esto es, además de la definición original dada en la ecuación <a href="modelos-arma.html#eq:eq-modelo-ARMA">(6.1)</a>, requeriremos también que <span class="math inline">\(\phi(z)\)</span> y <span class="math inline">\(\theta(z)\)</span> no tengan factores comunes. Así, el proceso <span class="math inline">\(x_t=0.5x_{t-1}-0.5w_{t-1}+w_t\)</span> discutido en el ejemplo <a href="modelos-arma.html#exm:ejem-redundancia-parametro">6.2</a> no se refiere a un proceso ARMA(1,1) porque en su forma reducida <span class="math inline">\(x_t\)</span> es un ruido blanco.</p>
<p>Para resolver el problema del modelo con dependencia del futuro, introduciremos formalmente el concepto de causalidad.</p>

<div class="definition">
<p><span id="def:defi-causalidad" class="definition"><strong>Definición 6.3  </strong></span>Un modelo ARMA(p,q), <span class="math inline">\(\phi(B)x_t=\theta(B)w_t\)</span>, se dice que es <em>causal</em> si la serie de tiempo <span class="math inline">\(\{x_t:t=0,\pm1,\pm2,\ldots\}\)</span> se puede escribir como un proceso lineal de un lado, esto es</p>
<span class="math display" id="eq:eq-modelo-causal">\[\begin{equation}
    x_t=\sum_{j=0}^{\infty}\psi_jw_{t-j}=\psi(B)w_t
\tag{6.8}
\end{equation}\]</span>
donde <span class="math inline">\(\psi(B)=\sum_{j=0}^{\infty}\psi_jB^j\)</span> y <span class="math inline">\(\sum_{j=0}^{\infty}|\psi_j|&lt;\infty\)</span>; haciendo <span class="math inline">\(\psi_0=1\)</span>
</div>

<hr />
<p>En el ejemplo <a href="modelos-ar.html#exm:ejem-modelo-AR-explosivo-causal">4.1</a> el proceso <span class="math inline">\(AR(1)\)</span> <span class="math inline">\(x_t=\phi z_{t-1}+w_t\)</span> es causal solo cuando <span class="math inline">\(|\phi|&lt;1\)</span>. Equivalentemente, el proceso es causal sólo cuando la raíz de <span class="math inline">\(\phi(z)=1-\phi z\)</span> es mayor que uno en valor absoluto. Esto es, la raíz <span class="math inline">\(z_0\)</span> de <span class="math inline">\(\phi(z)\)</span> es <span class="math inline">\(z_0=1/\phi\)</span> (porque <span class="math inline">\(\phi(z_0)=0\)</span>) y <span class="math inline">\(|z_0|&gt;1\)</span>, porque <span class="math inline">\(|\phi|&lt;1\)</span>.</p>

<div class="definition">
<span id="def:defi-modelo-invertible" class="definition"><strong>Definición 6.4  </strong></span>Un modelo ARMA(p,q) <span class="math inline">\(\phi(B)x_t=\theta(B)w_t\)</span> se dice <em>invertible</em> si la serie de tiempo <span class="math inline">\(\{x_t:t=0,\pm1,\pm2,\ldots\}\)</span> se puede escribir como
<span class="math display" id="eq:eq-modelo-invertible">\[\begin{equation}
    \pi(B)x_t=\sum_{j=0}^{\infty}\pi_jx_{t-j}=w_t
\tag{6.9}
\end{equation}\]</span>
donde <span class="math inline">\(\pi(B)=\sum_{j=0}^{\infty}\pi_jB^j\)</span> y <span class="math inline">\(\sum_{j=0}^{\infty}|\pi_j|&lt;\infty\)</span>; hacemos <span class="math inline">\(\pi_0=1\)</span>
</div>

<div id="propiedades-de-los-modelos-armapq" class="section level2">
<h2><span class="header-section-number">6.1</span> Propiedades de los modelos ARMA(p,q)</h2>

<div class="proposition">
<p><span id="prp:propiedad-causalidad-ARMApq" class="proposition"><strong>Proposición 6.1  (Propiedad 1: Causalidad)  </strong></span>Un modelo ARMA(p,q) es causal si y solo si <span class="math inline">\(\phi(z)\neq0\)</span> para <span class="math inline">\(|z|\leq1\)</span>. El coeficiente del proceso lineal dado en <a href="modelos-arma.html#eq:eq-modelo-causal">(6.8)</a> se puede determinar resolviendo</p>
<p><span class="math display">\[\psi(z)=\sum_{j=0}^{\infty}\psi_jz^j=\frac{\theta(z)}{\phi(z)}\text{, }|z|&lt;1.\]</span></p>
Otra manera de ver la propiedad 1, es que un <em>modelo ARMA es causal sólo cuando las raíces de <span class="math inline">\(\phi(z)\)</span> están fuera del círculo unitario</em>, esto es <span class="math inline">\(\phi(z)=0\)</span> sólo cuando <span class="math inline">\(|z|&gt;1\)</span>.
</div>

<hr />

<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span> Supongamos primero que las raíces de <span class="math inline">\(\phi(z)\)</span>, digamos <span class="math inline">\(z_1,\ldots,z_p\)</span>, están fuera del círculo unitario. Escribimos las raíces en el siguiente orden <span class="math inline">\(1&lt;|&lt;_1|\leq|z_2|\leq\ldots\leq|z_p|\)</span>, note que <span class="math inline">\(z_1,\ldots,z_p\)</span> no son necesariamente únicas, y hacemos <span class="math inline">\(|z_1|=1+\epsilon\)</span>, para algún <span class="math inline">\(\epsilon&gt;0\)</span>. Entonces, <span class="math inline">\(\phi(z)\neq0\)</span> siempre que <span class="math inline">\(|z|&lt;|z_1|=1+\epsilon\)</span> y por consiguiente, <span class="math inline">\(\phi^{-1}(z)\)</span> existe y tiene un desarrollo en serie de potencias</p>
<p><span class="math display">\[\frac{1}{\phi(z)} = \sum_{j=0}^{\infty}a_jz^j,\quad |z|&lt;1+\epsilon.\]</span></p>
<p>Ahora, elegimos un valor de <span class="math inline">\(\delta\)</span> tal que <span class="math inline">\(0&lt;\delta&lt;\epsilon\)</span>, y hacemos <span class="math inline">\(z=1+\delta\)</span>, el cual está dentro del radio de convergencia. Se sigue entonces que</p>
<span class="math display" id="eq:eq-convergencia-phi">\[\begin{equation}
  \phi^{-1}(1+\delta) = \sum_{j=0}^{\infty}a_j(1+\delta)^j&lt;\infty.
\tag{6.10}
\end{equation}\]</span>
<p>Así, podemos cada término de la suma en <a href="modelos-arma.html#eq:eq-convergencia-phi">(6.10)</a> por una constante, sea esta <span class="math inline">\(|a_j(1+\delta)^j|&lt;c\)</span>, para <span class="math inline">\(c&gt;0\)</span>. Por consiguiente, <span class="math inline">\(|a_j|&lt;c(1+\delta)^{-j}\)</span>, de donde se sigue que</p>
<span class="math display" id="eq:eq-convergencia-aj">\[\begin{equation}
  \sum_{j=0}^{\infty}|a_j|&lt;\infty.
\tag{6.11}
\end{equation}\]</span>
<p>En consecuencia, <span class="math inline">\(\phi^{-1}(B)\)</span> existe y podemos aplicar este a ambos lados del modelo ARMA, <span class="math inline">\(\phi(B)x_t=\theta(B)w_t\)</span>, para obtener</p>
<p><span class="math display">\[x_t = \phi^{-1}(B)\phi(B)x_t = \phi^{-1}(B)\theta(B)w_t.\]</span></p>
<p>Entonces, haciendo <span class="math inline">\(\psi(B)=\phi^{-1}(B)\theta(B)\)</span>, tenemos</p>
<p><span class="math display">\[x_t = \psi(B)w_t = \sum_{j=0}^{\infty}\psi_jw_{t-j},\]</span></p>
<p>donde los <span class="math inline">\(\psi\)</span>-pesos, los cuales son absolutamente sumables, pueden ser evaluados por medio de <span class="math inline">\(\psi(z) = \phi^{-1}(z)\theta(z)\)</span> para <span class="math inline">\(|z|\leq1\)</span>.</p>
<p>Ahora, supongamos que <span class="math inline">\(x_t\)</span> es un proceso causal, esto es, el proceso tiene la representación</p>
<p><span class="math display">\[x_t = \sum_{j=0}^{\infty}\psi_jw_{t-j},\quad\text{con}\quad \sum_{j=0}^{\infty}|\psi_j|&lt;\infty.\]</span></p>
<p>En este caso, escribimos</p>
<p><span class="math display">\[x_t = \psi(B)w_t,\]</span></p>
<p>y multiplicando a ambos lados por <span class="math inline">\(\phi(B)\)</span> nos queda</p>
<span class="math display" id="eq:eq-modelo-causal-conciso">\[\begin{equation}
  \phi(B)x_t = \phi(B)\psi(B)w_t.
\tag{6.12}
\end{equation}\]</span>
<p>Además de <a href="modelos-arma.html#eq:eq-modelo-causal-conciso">(6.12)</a>, el modelo ARMA se puede escribir como</p>
<span class="math display" id="eq:eq-modelo-ARMA-conciso2">\[\begin{equation}
  \phi(B)x_t = \theta(B)w_t.
\tag{6.13}
\end{equation}\]</span>
<p>De <a href="modelos-arma.html#eq:eq-modelo-causal-conciso">(6.12)</a> y <a href="modelos-arma.html#eq:eq-modelo-ARMA-conciso2">(6.13)</a>, notamos que</p>
<span class="math display" id="eq:eq-B15">\[\begin{equation}
  \phi(B)\psi(B)w_t = \theta(B)w_t.
\tag{6.14}
\end{equation}\]</span>
<p>Ahora, sea <span class="math display">\[a(z) = \phi(z)\psi(z) = \sum_{j=0}^{\infty}a_jz^j,\quad |z|\leq1\]</span> y, por consiguiente, podemos escribir <a href="modelos-arma.html#eq:eq-B15">(6.14)</a> como</p>
<span class="math display" id="eq:eq-B16">\[\begin{equation}
  \sum_{j=0}^{\infty}a_jw_{t-j} = \sum_{j=0}^{q}\theta_jw_{t-j}.
\tag{6.15}
\end{equation}\]</span>
<p>A continuación, multiplicamos ambos lados de <a href="modelos-arma.html#eq:eq-B16">(6.15)</a> por <span class="math inline">\(w_{t-h}\)</span>, para <span class="math inline">\(h=0,1,2,\ldots\)</span>, y tomamos esperanza. Haciendo esto, obtenemos</p>
<span class="math display" id="eq:eq-B17">\[\begin{eqnarray}
  a_h &amp;=&amp; \theta_h,\quad h=0,1,\ldots,q \nonumber \\
  a_h &amp;=&amp; 0, \quad h&gt;q. \tag{6.16}
\end{eqnarray}\]</span>
<p>De <a href="modelos-arma.html#eq:eq-B17">(6.16)</a> concluimos que</p>
<span class="math display" id="eq:eq-igualdad-phi-psi-theta">\[\begin{equation}
  \phi(z)\psi(z) = a(z) = \theta(z),\quad |z|\leq1.
\tag{6.17}
\end{equation}\]</span>
<p>Si existe un número en el círculo unitario, digamos <span class="math inline">\(z_0\)</span>, para el cual <span class="math inline">\(\phi(z_0)=0\)</span>, entonces por <a href="modelos-arma.html#eq:eq-igualdad-phi-psi-theta">(6.17)</a>, <span class="math inline">\(\theta(z_0)=0\)</span>. Pero, si existe tal <span class="math inline">\(z_0\)</span> entonces <span class="math inline">\(\phi(z)\)</span> y <span class="math inline">\(\theta(z)\)</span> tienen un factor común lo cual no es posible. En consecuencia, podemos escribir <span class="math inline">\(\psi(z) = \theta(z)/\phi(z)\)</span>. Además, por hipótesis, tenemos que <span class="math inline">\(|\psi(z)|&lt;\infty\)</span> para <span class="math inline">\(|z|\leq1\)</span>, y por lo tanto</p>
<span class="math display" id="eq:eq-B19">\[\begin{equation}
  |\psi(z)| = \left|\frac{\theta(z)}{\phi(z)}\right|&lt;\infty,\text{ para }|z|\leq1.
\tag{6.18}
\end{equation}\]</span>
Finalmente, la ecuación <a href="modelos-arma.html#eq:eq-B19">(6.18)</a> implica que <span class="math inline">\(\phi(z)\neq0\)</span> para <span class="math inline">\(|z|\leq1\)</span>; esto es, las raíces de <span class="math inline">\(\phi(z)\)</span> están fuera del círculo unitario.
</div>

<hr />

<div class="proposition">
<p><span id="prp:propiedad-invertibilidad-ARMApq" class="proposition"><strong>Proposición 6.2  (Propiedad 2: Invertibilidad)  </strong></span>Un modelo ARMA(p,q) es <em>invertible</em> si y solo si <span class="math inline">\(\theta(z)\neq0\)</span> para <span class="math inline">\(|z|\leq1\)</span>. El coeficiente <span class="math inline">\(\pi_j\)</span> de <span class="math inline">\(\pi(B)\)</span> dado en <a href="modelos-arma.html#eq:eq-modelo-invertible">(6.9)</a> se puede determinar al resolver</p>
<p><span class="math display">\[\pi(z)=\sum_{j=0}^{\infty}\pi_jz^j=\frac{\phi(z)}{\theta(z)}\text{, }|z|\leq1.\]</span></p>
Otra manera de escribir la propiedad 2, es que un <em>proceso ARMA es invertible solo cuando las raíces de <span class="math inline">\(\theta(z)\)</span> están fuera del círculo unitario</em>; esto es, <span class="math inline">\(\theta(z)=0\)</span> sólo cuando <span class="math inline">\(|z|&gt;1\)</span>.
</div>

<hr />

<div class="proof">
 <span class="proof"><em>Demostración. </em></span> La demostración de esta propiedad es similar a la propiedad 1, y se deja como ejercicio.
</div>

<hr />

<div class="example">
<p><span id="exm:ejem-redundancia-causalidad-invertibilidad" class="example"><strong>Ejemplo 6.3  (Redundancia de Parámetros, Causalidad e Invertibilidad)  </strong></span></p>
<p>Considere el proceso</p>
<p><span class="math display">\[x_t=0.4x_{t-1}+0.45x_{t-2}+w_t+w_{t-1}+0.25w_{t-2}\]</span></p>
<p>o, en la forma operador</p>
<p><span class="math display">\[(1-0.4B-0.45B^2)x_t=(1+B+0.25B^2)w_t\]</span></p>
<p>A primera vista <span class="math inline">\(x_t\)</span> parece ser un proceso ARMA(2,2). Pero, los polinomios asociados</p>
<span class="math display">\[\begin{eqnarray*}
\phi(z) &amp;=&amp; 1-0.4z-0.45z^2=(1+0.5z)(1-0.9z)
\theta(z) &amp;=&amp; (1+z+0.25z^2)=(1+0.5z)^2
\end{eqnarray*}\]</span>
<p>tienen un factor común que se puede cancelar.</p>
<p>Después de cancelar los factores comunes, los polinomios quedan <span class="math inline">\(\phi(z)=(1-0.9z)\)</span> y <span class="math inline">\(\theta(z)=(1+0.5z)\)</span>, de modo que el modelo es un proceso ARMA(1,1) <span class="math inline">\((1-0.9B)x_t=(1+0.5B)w_t\)</span> o</p>
<span class="math display" id="eq:eq-ejemplo-ARMA11">\[\begin{equation}
  x_t=0.9x_{t-1}+0.5w_{t-1}+w_t
\tag{6.19}
\end{equation}\]</span>
<p>El modelo es causal, porque <span class="math inline">\(\phi(z)=(1-0.9z)=0\)</span> cuando <span class="math inline">\(z=10/9\)</span> que está fuera del círculo unitario. El modelo también es invertible porque la raíz de <span class="math inline">\(\theta(z)=(1+0.5z)\)</span> es <span class="math inline">\(z=-2\)</span> que también está fuera del circulo unitario.</p>
<p>Para escribir el modelo como un proceso lineal, podemos obtener los <span class="math inline">\(\psi\)</span>-pesos usando la proposición <a href="modelos-arma.html#prp:propiedad-causalidad-ARMApq">6.1</a>:</p>
<span class="math display">\[\begin{eqnarray*}
\psi(z) &amp;=&amp; \frac{\theta(z)}{\phi(z)}=\frac{(1+0.5z)}{(1-0.9z)} \\
        &amp;=&amp; (1+0.5z)(1+0.9z+0.9^2z^2+0.9^3z^3+\cdots)\text{ }|z|\leq1
\end{eqnarray*}\]</span>
<p>El coeficiente de <span class="math inline">\(z^j\)</span> en <span class="math inline">\(\psi(z)\)</span> es <span class="math inline">\(\psi_j=(0.5+0.9)0.9^{j-1}\)</span>, para <span class="math inline">\(j\geq1\)</span>, así, podemos escribir () como</p>
<p><span class="math display">\[x_t=w_t+1.4\sum_{j=0}^{\infty}0.9^{j-1}w_{t-j}\]</span></p>
<p>Similarmente, para hallar la representación invertible usando la proposición <a href="modelos-arma.html#prp:propiedad-invertibilidad-ARMApq">6.2</a>:</p>
<p><span class="math display">\[\pi(z)=\frac{\phi(z)}{\theta(z)}=(1-0.9z)(1-0.5z+0.5^2z^2-0.5^3z^3+\cdots)\text{ }|z|\leq1.\]</span></p>
<p>En este caso, los <span class="math inline">\(\pi\)</span>-pesos están dados por <span class="math inline">\(\pi_j=(-1)^j(0.9+0.5)0.5^{j-1}\)</span> para <span class="math inline">\(j\geq1\)</span> y por lo tanto, podemos escribir <a href="modelos-arma.html#eq:eq-ejemplo-ARMA11">(6.19)</a> como</p>
<span class="math display">\[x_t=1.4\sum_{j=0}^{\infty}(-0.5)^{j-1}x_{t-j}+w_t\]</span>
</div>

<hr />
<p>La PACF para los modelos <span class="math inline">\(MA\)</span> se comporta como el ACF para los modelos <span class="math inline">\(AR\)</span>. También, la PACF para modelos <span class="math inline">\(AR\)</span> se comporta como la ACF para modelos <span class="math inline">\(MA\)</span>. Debido a que un modelo ARMA invertible tiene una representación <span class="math inline">\(AR\)</span> infinita, la PACF no tendrá corte. Resumimos estos resultados en la tabla siguiente</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">AR(p)</th>
<th align="center">MA(q)</th>
<th align="center">ARMA(p,q)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ACF</td>
<td align="center">Disminución</td>
<td align="center">Corte después</td>
<td align="center">Disminución</td>
</tr>
<tr class="even">
<td></td>
<td align="center">gradual</td>
<td align="center">de paso <span class="math inline">\(q\)</span></td>
<td align="center">gradual</td>
</tr>
<tr class="odd">
<td>PACF</td>
<td align="center">Corte después</td>
<td align="center">Disminución</td>
<td align="center">Disminución</td>
</tr>
<tr class="even">
<td></td>
<td align="center">de paso <span class="math inline">\(q\)</span></td>
<td align="center">gradual</td>
<td align="center">gradual</td>
</tr>
</tbody>
</table>
</div>
<div id="ecuaciones-en-diferencias" class="section level2">
<h2><span class="header-section-number">6.2</span> Ecuaciones en Diferencias</h2>
<p>El estudio del comportamiento de los procesos ARMA es mucho mejor si se tiene un conocimiento básico de ecuaciones en diferencias, simplemente porque los procesos ARMA son ecuaciones en diferencias. Este tópico será también muy útil para el estudio de los modelos en dominio del tiempo y procesos estocásticos en general. Vamos a dar una breve y heurística reseña del tema junto con algunos ejemplos de la utilidad de la teoría.</p>
<p>Supongamos que tenemos una sucesión de números <span class="math inline">\(u_0,u_1,u_2,\ldots\)</span> tal que</p>
<span class="math display" id="eq:eq-sucesion-u-n">\[\begin{equation}
    u_n-\alpha u_{n-1}=0\text{, }\alpha\neq0\text{, }n=1,2,\ldots
\tag{6.20}
\end{equation}\]</span>
<p>Por ejemplo, recuerde <a href="modelos-ar.html#eq:eq-ACF-AR1">(4.8)</a>, en la cual mostramos que la ACF de un proceso AR(1) es una sucesión <span class="math inline">\(\rho(h)\)</span> que satisface</p>
<p><span class="math display">\[\rho(h)=\phi\rho(h-1)=0\text{, para }h=1,2\ldots\]</span> La ecuación <a href="modelos-arma.html#eq:eq-sucesion-u-n">(6.20)</a> representa un ecuación en diferencias homogénea de orden 1. Para resolver la ecuación, escribimos</p>
<span class="math display">\[\begin{eqnarray*}
  u_1 &amp;=&amp; \alpha u_0 \\
  u_2 &amp;=&amp; \alpha u_1 = \alpha^2u_0 \\
      &amp;\vdots&amp;  \\
  u_n &amp;=&amp; \alpha u_{n-1} = \alpha^nu_0.
\end{eqnarray*}\]</span>
<p>Dando una condición inicial <span class="math inline">\(u_0=c\)</span> podemos resolver <a href="modelos-arma.html#eq:eq-sucesion-u-n">(6.20)</a>, sea esta <span class="math inline">\(u_n=\alpha^nc\)</span>.</p>
<p>En la notación de operador, <a href="modelos-arma.html#eq:eq-sucesion-u-n">(6.20)</a> se puede escribir como <span class="math inline">\((1-\alpha B)u_n=0\)</span>. El polinomio asociado a <a href="modelos-arma.html#eq:eq-sucesion-u-n">(6.20)</a> es <span class="math inline">\(\alpha(z)=1-\alpha z\)</span>, y las raíz <span class="math inline">\(z_0\)</span> del polinomio es <span class="math inline">\(z_0=1/\alpha\)</span>, esto es <span class="math inline">\(\alpha(z_0)=0\)</span>.</p>
<p>Conocemos la solución de <a href="modelos-arma.html#eq:eq-sucesion-u-n">(6.20)</a> con condición inicial <span class="math inline">\(u_0=c\)</span>, esta es</p>
<p><span class="math display">\[u_n=\alpha^nc=(z_0^{-1})^nc.\]</span></p>
<p>Esto es, la solución de la ecuación en diferencias <a href="modelos-arma.html#eq:eq-sucesion-u-n">(6.20)</a> solo depende de la condición inicial y de la inversa de la raíz del polinomio asociado <span class="math inline">\(\alpha(z)\)</span>.</p>
<p>Supóngase ahora que la sucesión satisface</p>
<span class="math display" id="eq:eq-sucesion-u-n-2">\[\begin{equation}
    u_n-\alpha_1u_{n-1}-\alpha_2u_{n-2}=0\text{, }\alpha_2\neq0\text{, }n=2,3,\ldots
\tag{6.21}
\end{equation}\]</span>
<p>Esta ecuación es una ecuación en diferencias homogénea de orden 2. El correspondiente polinomio es</p>
<p><span class="math display">\[\alpha(z)=1-\alpha_1z-\alpha_2z^2\]</span> el cual tiene dos raíces <span class="math inline">\(z_1\)</span> y <span class="math inline">\(z_2\)</span>, tal que <span class="math inline">\(\alpha(z_1)=\alpha(z_2)=0\)</span>.</p>
<p>Consideremos dos casos:</p>
<ul>
<li><strong>Caso 1:</strong> <span class="math inline">\(z_1\neq z_2\)</span>.} La solución general en este caso es
<span class="math display" id="eq:eq-solucio-u-n-raices-distintas">\[\begin{equation}
    u_n=c_1z_1^{-n}+c_2z_2^{-n}
\tag{6.22}
\end{equation}\]</span>
donde <span class="math inline">\(c_1\)</span> y <span class="math inline">\(c_2\)</span> dependen de las condiciones iniciales. Esta afirmación puede ser verificada por la sustitución directa de <a href="modelos-arma.html#eq:eq-solucio-u-n-raices-distintas">(6.22)</a> en <a href="modelos-arma.html#eq:eq-sucesion-u-n-2">(6.21)</a>:
<span class="math display">\[\begin{eqnarray*}
c_1z_1^{-n}+c_2z_2^{-n} &amp;-&amp; \alpha_1\left(c_1z_1^{-(n-1)}+c_2z_2^{-(n-1)}\right)-\alpha_2\left(c_1z_1^{-(n-2)}+c_2z_2^{-(n-2)}\right) \\
     &amp;=&amp; c_1z_1^{-n}(1-\alpha_1z_1-\alpha_2z_1^2)+c_2z_2^{-n}((1-\alpha_1z_2-\alpha_2z_2^2) \\
     &amp;=&amp; c_1z_1^{-n}\alpha(z_1)+c_2z_2^{-n}\alpha(z_2) \\
     &amp;=&amp; 0.
\end{eqnarray*}\]</span>
Dando dos condiciones iniciales <span class="math inline">\(u_0\)</span> y <span class="math inline">\(u_1\)</span> podemos resolver para <span class="math inline">\(c_1\)</span> y <span class="math inline">\(c_2\)</span>:
<span class="math display">\[\begin{eqnarray*}
    u_0 &amp;=&amp; c_1+c_2 \\
    u_1 &amp;=&amp; c_1z_1^{-1}+c_2z_2^{-1}
\end{eqnarray*}\]</span>
<p>donde <span class="math inline">\(z_1\)</span> y <span class="math inline">\(z_2\)</span> se pueden resolver en términos de <span class="math inline">\(\alpha_1\)</span> y <span class="math inline">\(\alpha_2\)</span> usando la fórmula cuadrática por ejemplo.</p></li>
<li><strong>Caso 2:</strong> <span class="math inline">\(z_1=z_2 (=z_0)\)</span>. En este caso la solución general de <a href="modelos-arma.html#eq:eq-sucesion-u-n-2">(6.21)</a> es
<span class="math display" id="eq:eq-solucio-u-n-raices-iguales">\[\begin{equation}
    u_n=z_0^{-n}(c_1+c_2n)
    \tag{6.23}
\end{equation}\]</span>
Esta afirmación se puede verificar por sustitución directa de <a href="modelos-arma.html#eq:eq-solucio-u-n-raices-iguales">(6.23)</a> en <a href="modelos-arma.html#eq:eq-sucesion-u-n-2">(6.21)</a>:
<span class="math display">\[\begin{eqnarray*}
z_0^{-n}(c_1+c_2n) &amp;-&amp; \alpha_1\left(z_0^{-(n-1)}[c_1+c_2(n-1)]\right)-\alpha_2\left(z_0^{-(n-2)}[c_1+c_2(n-2)]\right) \\
     &amp;=&amp; z_0^{-n}(c_1+c_2n)(1-\alpha_1z_0-\alpha_2z_0^2)+c_2z_0^{-n+1}(\alpha_1+2\alpha_2z_0) \\
     &amp;=&amp; c_2z_0^{-n+1}(\alpha_1+2\alpha_2z_0).
\end{eqnarray*}\]</span>
Para demostrar que <span class="math inline">\((\alpha_1+2\alpha_2z_0)=0\)</span>, escribimos <span class="math inline">\(1-\alpha_1z-\alpha_2z^2=(1-z_0^{-1}z)^2\)</span> y derivamos respecto de <span class="math inline">\(z\)</span> en ambos lados de la ecuación para obtener <span class="math inline">\((\alpha_1+2\alpha_2z)=2z_0^{-1}(1-z_0^{-1}z)\)</span>. Entonces <span class="math inline">\((\alpha_1+2\alpha_2z_0)=2z_0^{-1}(1-z_0^{-1}z_0)=0\)</span>. Finalmente, dando dos condiciones iniciales <span class="math inline">\(u_0\)</span> y <span class="math inline">\(u_1\)</span> podemos resolver para <span class="math inline">\(c_1\)</span> y <span class="math inline">\(c_2\)</span>;
<span class="math display">\[\begin{eqnarray*}
    u_0 &amp;=&amp; c_1 \\
    u_1 &amp;=&amp; (c_1+c_2)z_0^{-1}
\end{eqnarray*}\]</span></li>
</ul>
<p>Resumiendo:</p>
En el caso de raíces distintas, la solución de la ecuación en diferencias homogénea de grado 2 es
<span class="math display">\[\begin{eqnarray*}
  u_n &amp;=&amp; z_1^{-n}\times(\text{un polinomio en } n \text{ de grado }m_1-1) \\
      &amp;+&amp; z_2^{-n}\times(\text{un polinomio en } n \text{ de grado }m_2-1)
\end{eqnarray*}\]</span>
<p>donde <span class="math inline">\(m_1\)</span> es la multiplicidad de la raíz <span class="math inline">\(z_1\)</span> y <span class="math inline">\(m_2\)</span> es la multiplicidad de la raíz <span class="math inline">\(z_2\)</span>. En este ejemplo, se tiene <span class="math inline">\(m_1=m_2=1\)</span> y decimos que <span class="math inline">\(c_1\)</span> y $c_2 $ son polinomios de grado cero respectivamente.</p>
<p>En el caso de raíces repetidas, la solución es <span class="math display">\[u_n=z_0^{-n}\times(\text{un polinomio en } n \text{ de grado }m_0-1),\]</span> donde <span class="math inline">\(m_0\)</span> es la multiplicidad de la raíz <span class="math inline">\(z_0\)</span>; esto es <span class="math inline">\(m_0=2\)</span>. En este caso, escribimos el polinomio de grado uno como <span class="math inline">\(c_1+c_2n\)</span>. En ambos casos, resolvimos <span class="math inline">\(c_1\)</span> y <span class="math inline">\(c_2\)</span> dando dos condiciones iniciales <span class="math inline">\(u_0\)</span> y <span class="math inline">\(u_1\)</span>.</p>
<p>Veamos a continuacion algunos ejemplos de uso de las ecuaciones en diferencias, los dos primeros veremos la aplicación a procesos <span class="math inline">\(AR(2)\)</span>, y posteriormente daremos un ejemplo de uso para modelos ARMA.</p>

<div class="example">
<p><span id="exm:ejem-ACF-AR2" class="example"><strong>Ejemplo 6.4  (La ACF de un proceso AR(2))  </strong></span> Supóngase que <span class="math inline">\(x_t=\phi_1x_{t-1}+\phi_2x_{t-2}+w_t\)</span> es un proceso <span class="math inline">\(AR(2)\)</span> causal. Multiplicando ambos lados del modelo por <span class="math inline">\(x_{t-h}\)</span> para <span class="math inline">\(h&gt;0\)</span>, tomando esperanza:</p>
<p><span class="math display">\[\mathbb{E}(x_tx_{t-h})=\phi_1\mathbb{E}(x_{t-1}x_{t-h})+\phi_2\mathbb{E}(x_{t-2}x_{t-h})+\mathbb{E}(w_tx_{t-h})\]</span></p>
<p>El resultado es</p>
<span class="math display" id="eq:eq-autocovarianza-AR2">\[\begin{equation}
    \gamma(h)=\phi_1\gamma(h-1)+\phi_2\gamma(h-2)\text{, para }h=1,2,\ldots
\tag{6.24}
\end{equation}\]</span>
<p>En <a href="modelos-arma.html#eq:eq-autocovarianza-AR2">(6.24)</a> estamos usando el hecho de que <span class="math inline">\(\mathbb{E}(x_t)=0\)</span> y para <span class="math inline">\(h&gt;0\)</span>,</p>
<p><span class="math display">\[\mathbb{E}(w_tx_{t-h})=\mathbb{E}\left(w_t\sum_{j=0}^{\infty}\psi_jw_{t-h-j}\right)=0.\]</span></p>
<p>Dividimos <a href="modelos-arma.html#eq:eq-autocovarianza-AR2">(6.24)</a> por <span class="math inline">\(\gamma(0)\)</span> para obtener la ecuación en diferencia de la ACF del proceso:</p>
<span class="math display" id="eq:eq-ACF-AR2">\[\begin{equation}
  \rho(h)-\phi_1\rho(h-1)-\phi_2\rho(h-2)=0\text{, con }h=1,2,\ldots.
\tag{6.25}
\end{equation}\]</span>
<p>Las condiciones iniciales son <span class="math inline">\(\rho(0)=1\)</span> y <span class="math inline">\(\rho(-1)=\phi_1/(1-\phi_2)\)</span>, lo cual se obtiene evaluando <span class="math inline">\(h=1\)</span> en <a href="modelos-arma.html#eq:eq-ACF-AR2">(6.25)</a> y observando que <span class="math inline">\(\rho(1)=\rho(-1)\)</span>.</p>
<p>Usando los resultados para la ecuación en diferencias homogénea de orden dos, sean <span class="math inline">\(z_1\)</span> y <span class="math inline">\(z_2\)</span> las raíces del polinomio asociado <span class="math inline">\(\phi(z)=1-\phi_1z-\phi_2z^2\)</span>. Como el modelo es causal sabemos que las raíces están fuera del círculo unitario: <span class="math inline">\(|z_1|&gt;1\)</span> y <span class="math inline">\(|z_2|&gt;1\)</span>. Ahora consideremos la solución para los tres casos:</p>
<ul>
<li><p><strong>Caso 1:</strong> Cuando <span class="math inline">\(z_1\)</span> y <span class="math inline">\(z_2\)</span> son reales y distintos, entonces <span class="math display">\[\rho(h)=c_1z_1^{-h}+c_2z_2^{-h},\]</span> de modo que <span class="math inline">\(\rho(h)\to0\)</span> exponencialmente cuando <span class="math inline">\(h\to\infty\)</span>.</p></li>
<li><p><strong>Caso 2:</strong> Cuando <span class="math inline">\(z_1=z_2(=z_0)\)</span> son reales e iguales, entonces <span class="math display">\[\rho(h)=z_0^{-h}(c_1+c_2h),\]</span> de modo que <span class="math inline">\(\rho(h)\to0\)</span> exponencialmente cuando <span class="math inline">\(h\to\infty\)</span>.</p></li>
<li><strong>Caso 3:</strong> Cuando <span class="math inline">\(z_1=\bar{z}_2\)</span> son complejas conjugadas, entonces <span class="math inline">\(c_2=\bar{c}_1\)</span> (porque <span class="math inline">\(\rho(h)\)</span> es real) y <span class="math display">\[\rho(h)=c_1z_1^{-h}+\bar{c}_1\bar{z}_1^{-h}.\]</span> Escribiendo <span class="math inline">\(c_1\)</span> y <span class="math inline">\(z_1\)</span> en coordenadas polares, por ejemplo <span class="math inline">\(z_1=|z_1|e^{i\theta}\)</span> donde <span class="math inline">\(\theta\)</span> es el ángulo cuya tangente es el radio de la parte imaginaria y la parte real de <span class="math inline">\(z_1\)</span>; el rango de <span class="math inline">\(\theta\)</span> es <span class="math inline">\([-\pi,\pi]\)</span>. Entonces, usando el hecho de que <span class="math inline">\(e^{i\alpha}+e^{-i\alpha}=2\cos(\alpha)\)</span> la solución tiene la forma <span class="math display">\[\rho(h)=a|z_1|^{-h}\cos(h\theta+b),\]</span> donde <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> se determinan de las condiciones iniciales. De nuevo <span class="math inline">\(\rho(h)\)</span> tiende a cero exponencialmente cuando <span class="math inline">\(h\to\infty\)</span> pero en forma senosoidal.
</div>
</li>
</ul>
<hr />

<div class="example">
<span id="exm:ejem-camino-muestral-AR2" class="example"><strong>Ejemplo 6.5  (Camino muestral de un proceso AR(2) con raíces complejas)  </strong></span> La Figura <a href="modelos-arma.html#fig:grafico-AR2-simulado">6.1</a> muestra <span class="math inline">\(n=144\)</span> observaciones de un modelo AR(2) <span class="math display">\[x_t=1.5x_{t-1}-0.75x_{t-2}+w_t\]</span> con <span class="math inline">\(\sigma_w^2=1\)</span> y con raíces complejas, así el proceso exhibe un comportamiento pseudo-cíclico con una frecuencia de un ciclo cada 12 puntos de tiempo. El polinomio autoregresivo para este modelo es <span class="math inline">\(\phi(z)=1-1.5z+0.75z^2\)</span>. Las raíces de <span class="math inline">\(\phi(z)\)</span> son <span class="math inline">\(1\pm i/\sqrt{3}\)</span> y <span class="math inline">\(\theta=\tan^{-1}(1/\sqrt{3})=2\pi/12\)</span> radianes por unidad de tiempo. Para convertir el ángulo a ciclos por unidad de tiempo, dividimos por <span class="math inline">\(2\pi\)</span> para obtener <span class="math inline">\(1/12\)</span> ciclos por unidad de tiempo. La ACF para este modelo se muestra en la parte inferior de la Figura <a href="modelos-arma.html#fig:grafico-AR2-simulado">6.1</a>.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Simulación del proceso AR(2)</span>
<span class="kw">set.seed</span>(<span class="dv">5</span>)
ar2=<span class="kw">arima.sim</span>(<span class="kw">list</span>(<span class="dt">order=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">ar=</span><span class="kw">c</span>(<span class="fl">1.5</span>,<span class="op">-</span><span class="fl">0.75</span>)), <span class="dt">n=</span><span class="dv">144</span>)
<span class="co"># Gráfico del proceso AR(2)</span>
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))
<span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">144</span><span class="op">/</span><span class="dv">12</span>,ar2,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Tiempo (una unidad=12ptos)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;AR(2)&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span><span class="op">:</span><span class="dv">12</span>,<span class="dt">lty=</span><span class="st">&quot;dotted&quot;</span>)
<span class="co"># Raices del polinomio asociado</span>
<span class="kw">Arg</span>(<span class="kw">polyroot</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">0.75</span>))[<span class="dv">1</span>])<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>pi)</code></pre></div>
<pre><code>## [1] 0.08333</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Cálculo de la ACF</span>
ACF =<span class="st"> </span><span class="kw">ARMAacf</span>(<span class="dt">ar=</span><span class="kw">c</span>(<span class="fl">1.5</span>,<span class="fl">0.75</span>),<span class="dt">ma=</span><span class="dv">0</span>,<span class="dv">50</span>)
<span class="co"># Gráfico de la ACF</span>
<span class="kw">plot</span>(ACF,<span class="dt">type=</span><span class="st">&quot;h&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;LAG&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>)</code></pre></div>
<div class="figure"><span id="fig:grafico-AR2-simulado"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/grafico-AR2-simulado-1.svg" alt="Modelo AR(2) simulado, n=144, con phi_1=1.5,  phi_2=-0.75 (parte superior) y la función de autocovarianza (parte inferior)"  />
<p class="caption">
Figura 6.1: Modelo AR(2) simulado, n=144, con phi_1=1.5, phi_2=-0.75 (parte superior) y la función de autocovarianza (parte inferior)
</p>
</div>
<hr />
<p>Ahora, daremos la solución para una ecuación en diferencias homogénea general de orden <span class="math inline">\(p\)</span>:</p>
<span class="math display" id="eq:eq-sucesion-u-n-p">\[\begin{equation}
  u_n-\alpha_1u_{n-1}-\cdots-\alpha_pu_{n-p}=0\text{, con } \alpha_p\neq0\text{, }n=p,p+1,\ldots
\tag{6.26}
\end{equation}\]</span>
<p>El polinomio asociado es</p>
<p><span class="math display">\[\alpha(z)=1-\alpha_1z-\alpha_2z^2-\cdots-\alpha_pz^p.\]</span></p>
<p>Suponga que <span class="math inline">\(\alpha(z)\)</span> tiene <span class="math inline">\(r\)</span> raíces distintas, <span class="math inline">\(z_1\)</span> con multiplicidad <span class="math inline">\(m_1\)</span>, <span class="math inline">\(z_2\)</span> con multiplicidad <span class="math inline">\(m_2,\ldots,\)</span> y <span class="math inline">\(z_r\)</span> con multiplicidad <span class="math inline">\(m_r\)</span>, tal que <span class="math inline">\(m_1+m_2+\cdots+m_r=p\)</span>. La solución general para la ecuación <a href="modelos-arma.html#eq:eq-sucesion-u-n-p">(6.26)</a> es</p>
<span class="math display" id="eq:eq-solucion-general-u-n">\[\begin{equation}
  u_n=z_1^{-n}P_1(n)+z_2^{-n}P_2(n)+\cdots+z_r^{-n}P_r(n),
\tag{6.27}
\end{equation}\]</span>
<p>donde <span class="math inline">\(P_j(n)\)</span> para <span class="math inline">\(j=1,2,\ldots,r\)</span> es un polinomio en <span class="math inline">\(n\)</span> de grado <span class="math inline">\(m_j-1\)</span>. Dadas las condiciones iniciales <span class="math inline">\(u_0,u_1,\ldots,u_{p-1}\)</span> podemos resolver <span class="math inline">\(P_j(n)\)</span> explícitamente para <span class="math inline">\(j=1,2,\ldots,r\)</span></p>

<div class="example">
<p><span id="exm:ejem-pesos-ARMA-causal" class="example"><strong>Ejemplo 6.6  (Determinación de los psi-pesos de un proceso ARMA(p,q) causal)  </strong></span> Para un modelo ARMA(p,q) causal <span class="math inline">\(\phi(B)x_t=\theta(B)w_t\)</span> donde los ceros de <span class="math inline">\(\phi(z)\)</span> están fuera del círculo unitario, recordemos que podemos escribir este como</p>
<p><span class="math display">\[x_t=\sum_{j=0}^{\infty}\psi_jw_{t-j}\]</span></p>
<p>donde los <span class="math inline">\(\psi\)</span>-pesos se determinan usando la propiedad 1. (Proposición <a href="modelos-arma.html#prp:propiedad-causalidad-ARMApq">6.1</a>)</p>
<p>Para un modelo <span class="math inline">\(MA(q)\)</span> puro <span class="math inline">\(\psi_0=1,\psi_j=\theta_j\)</span> para <span class="math inline">\(j=1,2,\ldots,q\)</span> y <span class="math inline">\(\psi_j=0\)</span> en otro caso.</p>
<p>Para el caso general de un modelo ARMA(p,q) la tarea de resolver los <span class="math inline">\(\psi\)</span>-pesos es más complicada, como se demostró en el ejemplo <a href="modelos-arma.html#exm:ejem-redundancia-causalidad-invertibilidad">6.3</a>.</p>
<p>La teoría de ecuaciones en diferencias homogénea será útil para resolver este problema.</p>
<p>Para resolver los <span class="math inline">\(\psi\)</span>-pesos en general, debemos igualar los coeficientes en <span class="math inline">\(\psi(z)\phi(z)=\theta(z)\)</span></p>
<p><span class="math display">\[(\psi_0+\psi_1z+\psi_2z^2+\cdots)(1-\phi_1z-\phi_2z^2-\cdots)=(1+\theta_1z+\theta_2z^2+\cdots)\]</span></p>
<p>Los primeros valores serán</p>
<span class="math display">\[\begin{eqnarray*}
  \psi_0 &amp;=&amp; 1 \\
  \psi_1-\phi_1\psi_0 &amp;=&amp; \theta_1 \\
  \psi_2-\phi_1\psi_1-\phi_2\psi_0 &amp;=&amp; \theta_2 \\
  \psi_3-\phi_1\psi_2-\phi_2\psi_1-\phi_3\psi_0 &amp;=&amp; \theta_3 \\
   &amp;\vdots &amp;
\end{eqnarray*}\]</span>
<p>donde podemos tomar <span class="math inline">\(\phi_j=0\)</span> para <span class="math inline">\(j&gt;p\)</span> y <span class="math inline">\(\theta_j=0\)</span> para <span class="math inline">\(j&gt;q\)</span>.</p>
<p>Los <span class="math inline">\(\psi\)</span>-pesos satisfacen la ecuación en diferencias homogénea dada por</p>
<span class="math display" id="eq:eq-diferencias-homogeneas-pesos">\[\begin{equation}
  \psi_j-\sum_{k=1}^{p}\phi_k\psi_{j-k}=0\text{, con }j\geq\max(p,q+1)
\tag{6.28}
\end{equation}\]</span>
<p>con condiciones iniciales</p>
<span class="math display" id="eq:eq-condicion-inicial-pesos">\[\begin{equation}
    \psi_j-\sum_{k=1}^{j}\phi_k\psi_{j-k}=\theta_j\text{, con }0\leq j&lt;\max(p,q+1).
\tag{6.29}
\end{equation}\]</span>
<p>La solución general depende de las raíces del polinomio AR <span class="math inline">\(\phi(z)=1-\phi_1z-\cdots-\phi_pz^p\)</span> como se ve de <a href="modelos-arma.html#eq:eq-diferencias-homogeneas-pesos">(6.28)</a>. La solución particular de hecho dependerá de las condiciones iniciales.</p>
<p>Considere el proceso ARMA dado en <a href="modelos-arma.html#eq:eq-ejemplo-ARMA11">(6.19)</a> <span class="math inline">\(x_t=0.9x_{t-1}+0.5w_{t-1}+w_t\)</span>.</p>
<p>Dado que <span class="math inline">\(\max(p,q+1)=2\)</span>, usando <a href="modelos-arma.html#eq:eq-condicion-inicial-pesos">(6.29)</a>, tenemos que <span class="math inline">\(\psi_0=1\)</span> y <span class="math inline">\(\psi_1=0.9+0.5=1.4\)</span>. De <a href="modelos-arma.html#eq:eq-diferencias-homogeneas-pesos">(6.28)</a> para <span class="math inline">\(j=2,3,\ldots,\)</span> los <span class="math inline">\(\psi\)</span>-pesos satisfacen <span class="math inline">\(\psi_j-0.9\psi_{j-1}=0\)</span>. La solución general es <span class="math inline">\(\psi_j=c0.9^j\)</span>.</p>
<p>Para hallar la solución particular, usamos la condición inicial <span class="math inline">\(\psi=1.4\)</span>, de modo que <span class="math inline">\(1.4=c0.9\)</span> ó <span class="math inline">\(c=1.4/0.9\)</span>. Finalmente <span class="math inline">\(\psi_j=1.4(0.9)^{j-1}\)</span> para <span class="math inline">\(j\geq1\)</span> como vimos en el ejemplo <a href="modelos-arma.html#exm:ejem-redundancia-causalidad-invertibilidad">6.3</a>.</p>
Para ver los primeros 50 <span class="math inline">\(\psi\)</span>-pesos, usamos las siguientes instrucciones en R:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ARMAtoMA</span>(<span class="dt">ar=</span><span class="fl">0.9</span>,<span class="dt">ma=</span><span class="fl">0.5</span>, <span class="dv">50</span>)</code></pre></div>
<pre><code>##  [1] 1.400000 1.260000 1.134000 1.020600 0.918540
##  [6] 0.826686 0.744017 0.669616 0.602654 0.542389
## [11] 0.488150 0.439335 0.395401 0.355861 0.320275
## [16] 0.288248 0.259423 0.233481 0.210132 0.189119
## [21] 0.170207 0.153187 0.137868 0.124081 0.111673
## [26] 0.100506 0.090455 0.081410 0.073269 0.065942
## [31] 0.059348 0.053413 0.048072 0.043264 0.038938
## [36] 0.035044 0.031540 0.028386 0.025547 0.022992
## [41] 0.020693 0.018624 0.016762 0.015085 0.013577
## [46] 0.012219 0.010997 0.009898 0.008908 0.008017</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">ARMAtoMA</span>(<span class="dt">ar=</span><span class="fl">0.9</span>,<span class="dt">ma=</span><span class="fl">0.5</span>,<span class="dv">50</span>))</code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-54"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/unnamed-chunk-54-1.svg" alt="psi-pesos para el modelo ARMA,  x_t=0.9x_{t-1}+0.5w_{t-1}+w_t"  />
<p class="caption">
Figura 6.2: psi-pesos para el modelo ARMA, x_t=0.9x_{t-1}+0.5w_{t-1}+w_t
</p>
</div>
<hr />
<h3 id="función-de-autocorrelación-acf-para-modelos-arma"><span class="header-section-number">6.2</span> Función de Autocorrelación (ACF) para modelos ARMA</h3>
<p>Iniciemos mostrando la ACF de un proceso MA(q) <span class="math inline">\(x_t=\theta(B)w_t\)</span>, donde <span class="math inline">\(\theta(B)=1+\theta_1B+\cdots+\theta_qB^q\)</span>. Dado que <span class="math inline">\(x_t\)</span> es una combinación lineal de términos de ruido blanco, el proceso es estacionario con media</p>
<p><span class="math display">\[\mathbb{E}(x_t)=\sum_{j=0}^{q}\theta_j\mathbb{E}(w_{t-j})=0,\]</span></p>
<p>donde podemos escribir <span class="math inline">\(\theta_0=1\)</span>, y la función de autocovarianza es</p>
<span class="math display" id="eq:eq-autocovarianza-MA-q">\[\begin{eqnarray}
  \gamma(h)=\text{cov}(x_{t+h},x_t) &amp;=&amp; \mathbb{E}\left[\left(\sum_{j=0}^{q}\theta_jw_{t+h-j}\right)\left(\sum_{k=0}^{q}\theta_kw_{t-k}\right)\right] \nonumber \\
   &amp;=&amp; \begin{cases}\sigma_w^2\sum_{j=0}^{q-h}\theta_j\theta_{j+h},&amp;\text{ si }0\leq h\leq q\\
                    0,&amp;\text{ si }h&gt;q\end{cases}\tag{6.30}
\end{eqnarray}\]</span>
<p>Recuerde que <span class="math inline">\(\gamma(h)=\gamma(-h)\)</span>, por eso solo mostramos <span class="math inline">\(\gamma(h)\)</span> para <span class="math inline">\(h\geq0\)</span>.</p>
<p>El corte de <span class="math inline">\(\gamma(h)\)</span> después de <span class="math inline">\(q\)</span> saltos es la firma del modelo MA(q). Dividiendo () por <span class="math inline">\(\gamma(0)\)</span> conseguimos la ACF de un MA(q):</p>
<span class="math display" id="eq:eq-ACF-MA-q">\[\begin{equation}
  \rho(h)=\begin{cases}\frac{\sum_{j=0}^{q-h}\theta_j\theta_{j+h}}{1+\theta_1^2+\cdots+\theta_q^2},&amp;\text{ si }1\leq h\leq q\\
                0,\text{ si }h&gt;q\end{cases}
\tag{6.31}
\end{equation}\]</span>
<p>Para un modelo ARMA(p,q) causal <span class="math inline">\(\phi(B)x_t=\theta(B)w_t\)</span>, donde los ceros de <span class="math inline">\(\phi(z)\)</span> están fuera del círculo unitario, podemos escribir</p>
<p><span class="math display">\[x_t=\sum_{j=0}^{\infty}\psi_jw_{t-j}\]</span></p>
<p>Se sigue inmediatamente que <span class="math inline">\(\mathbb{E}(x_t)=0\)</span>. También, la función de autocovarianza de <span class="math inline">\(x_t\)</span> se puede escribir como</p>
<span class="math display" id="eq:eq-autocovarianza-ARMA">\[\begin{equation}
  \gamma(h)=\text{cov}(x_{t-h},x_t)=\sigma_w^2\sum_{j=0}^{\infty}\psi_j\psi_{j+h}\text{, }h\geq0
\tag{6.32}
\end{equation}\]</span>
<p>Podemos entonces usar <a href="modelos-arma.html#eq:eq-diferencias-homogeneas-pesos">(6.28)</a> y <a href="modelos-arma.html#eq:eq-condicion-inicial-pesos">(6.29)</a> para resolver los <span class="math inline">\(\psi\)</span>-pesos. A su vez, podemos resolver para <span class="math inline">\(\gamma(h)\)</span> y la ACF <span class="math inline">\(\rho(h)=\gamma(h)/\gamma(0)\)</span>. Como en el ejemplo <a href="modelos-arma.html#exm:ejem-ACF-AR2">6.4</a>, también es posible obtener una ecuación en diferencias homogénea directamente en términos de <span class="math inline">\(\gamma(h)\)</span>. Primero, escribimos</p>
<span class="math display" id="eq:eq-autocovarianza-ARMA-2">\[\begin{eqnarray}
  \gamma(h) &amp;=&amp; \text{cov}(x_{t+h},x_t)=\mathbb{E}\left[\left(\sum_{j=1}^{p}\phi_jx_{t+h-j}+\sum_{j=0}^{q}\theta_jw_{t+h-j}\right)x_t\right]\nonumber \\
    &amp;=&amp; \sum_{j=1}^{p}\phi_j\gamma(h-j)+\sigma_w^2\sum_{j=h}^{q}\theta_j\psi_{j-h}\text{, }h\geq0 \tag{6.33}
\end{eqnarray}\]</span>
<p>donde hemos usado el hecho de que <span class="math inline">\(x_t=\sum_{k=0}^{\infty}\psi_jw_{t-k}\)</span> y para <span class="math inline">\(h\geq0\)</span>,</p>
<p><span class="math display">\[\mathbb{E}(w_{t+h-j}x_t)=\mathbb{E}\left[w_{t+h-j}\left(\sum_{k=0}^{\infty}\psi_kw_{t-k}\right)\right]=\psi_{j-h}\sigma_w^2.\]</span></p>
<p>De <a href="modelos-arma.html#eq:eq-autocovarianza-ARMA-2">(6.33)</a> podemos escribir una ecuación general homogénea para la ACF de un proceso ARMA causal:</p>
<span class="math display" id="eq:eq-ACF-ARMA-causal">\[\begin{equation}
  \gamma(h)-\phi_1\gamma(h-1)-\cdots-\phi_p\gamma(h-p)=0\text{, }h\geq\max(p,q+1)
\tag{6.34}
\end{equation}\]</span>
<p>con condiciones iniciales</p>
<span class="math display" id="eq:eq-condicion-inicial-ACF-ARMA-causal">\[\begin{equation}
  \gamma(h)-\sum_{j=1}^{p}\phi_j\gamma(h-j)=\sigma_w^2\sum_{j=h}^{q}\theta_j\psi_{j-h}\text{, }0\leq h&lt;\max(p,q+1).
\tag{6.35}
\end{equation}\]</span>
<p>Dividiendo <a href="modelos-arma.html#eq:eq-ACF-ARMA-causal">(6.34)</a> y <a href="modelos-arma.html#eq:eq-condicion-inicial-ACF-ARMA-causal">(6.35)</a> por <span class="math inline">\(\gamma(0)\)</span> nos permite resolver la ACF <span class="math inline">\(\rho(h)=\gamma(h)/\gamma(0)\)</span>.</p>

<div class="example">
<p><span id="exm:ejem-ACF-ARMA11" class="example"><strong>Ejemplo 6.7  (La ACF de un proceso ARMA(1,1))  </strong></span> Consideremos el proceso ARMA(1,1) causal <span class="math inline">\(x_t=\phi x_{t-1}+\theta w_{t-1}+w_t\)</span> donde <span class="math inline">\(|\phi|&lt;1\)</span>. Basándonos en <a href="modelos-arma.html#eq:eq-ACF-ARMA-causal">(6.34)</a> la función de autocovarianza satisface</p>
<p><span class="math display">\[\gamma(h)-\phi\gamma(h-1)=0\text{, }h=2,3,\ldots,\]</span></p>
<p>así, la solución general es <span class="math inline">\(\gamma(h)=c\phi^h\)</span> para <span class="math inline">\(h=1,2,\ldots\)</span>. Para obtener las condiciones iniciales, usamos <a href="modelos-arma.html#eq:eq-condicion-inicial-ACF-ARMA-causal">(6.35)</a>:</p>
<span class="math display">\[\begin{eqnarray*}
  \gamma(0) &amp;=&amp; \phi\gamma(1)+\sigma_w^2[1+\theta\phi+\theta^2] \\
  \gamma(1) &amp;=&amp; \phi\gamma(0)+\sigma_w^2\theta
\end{eqnarray*}\]</span>
<p>Resolviendo para <span class="math inline">\(\gamma(0)\)</span> y <span class="math inline">\(\gamma(1)\)</span>, obtenemos</p>
<span class="math display">\[\begin{eqnarray*}
  \gamma(0) &amp;=&amp; \sigma_w^2\frac{1+2\theta\phi+\theta^2}{1-\phi^2} \\
  \gamma(1) &amp;=&amp; \sigma_w^2\frac{(1+\theta\phi)(\phi+\theta)}{1-\phi^2}
\end{eqnarray*}\]</span>
<p>Para resolver <span class="math inline">\(c\)</span>, note que <span class="math inline">\(\gamma(1)=c\phi\)</span>, en cuyo caso <span class="math inline">\(c=\gamma(1)/\phi\)</span>. Por consiguiente, la solución particular es</p>
<p><span class="math display">\[\gamma(h)=\sigma_w^2\frac{(1+\theta\phi)(\phi+\theta)}{1-\phi^2}\phi^{h-1}\]</span></p>
<p>Finalmente, dividiendo por <span class="math inline">\(\gamma(0)\)</span> nos da la ACF</p>
<span class="math display" id="eq:eq-ACF-ARMA11">\[\begin{equation}
  \rho(h)=\frac{(1+\theta\phi)(\phi+\theta)}{1+2\theta\phi+\theta^2}\phi^{h-1}\text{, }h\geq1
\tag{6.36}
\end{equation}\]</span>
</div>

<hr />
<h2 id="pronósticos"><span class="header-section-number">6.2</span> Pronósticos</h2>
<p>El objetivo en el pronóstico, es predecir los valores futuros de una serie de tiempo <span class="math inline">\(x_{n+m}, m=1,2,\ldots\)</span> basado en los valores de la serie observados hasta el tiempo actual <span class="math inline">\(\mathbf{x}=\{x_n,x_{n-1},\ldots,x_1\}\)</span>. En esta sección asumiremos que <span class="math inline">\(x_t\)</span> es estacionario y que los parámetros del modelo son conocidos. El problema de hacer pronósticos cuando los parámetros del modelo son desconocidos se analizará en la siguiente sección. En los capítulos para modelos AR y modelos MA, vimos como realizar los pronósticos o predicciones para los mismos. A continuación daremos métodos más generales de predicción y que se pueden utilizar para los modelos AR, MA y ARMA.</p>
<p>El mínimo del error cuadrático medio del predictor <span class="math inline">\(x_{n+m}\)</span> es <span class="math display">\[x_{n+m}^n=\mathbb{E}(x_{n+m}|x_n,x_{n-1},\ldots,x_1)\]</span> porque el valor esperado condicional minimiza el error cuadrático medio</p>
<span class="math display" id="eq:eq-esperanza-error-cuadratico-medio">\[\begin{equation}
  \mathbb{E}[x_{n+m}-g(\mathbf{x})]^2
\tag{6.37}
\end{equation}\]</span>
<p>donde <span class="math inline">\(g(\mathbf{x})\)</span> es una función de las observaciones <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Primero, nos restringiremos a los predictores que son función lineal de los datos, esto es, predictores de la forma</p>
<span class="math display" id="eq:eq-predictores">\[\begin{equation}
  x_{n+m}^n=\alpha_0+\sum_{k=1}^{n}\alpha_kx_k
\tag{6.38}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\alpha_0,\alpha_1,\ldots,\alpha_n\)</span> son números reales. Los predictores lineales de la forma () que minimizan el error cuadrático medio del predictor <a href="modelos-arma.html#eq:eq-esperanza-error-cuadratico-medio">(6.37)</a> son llamados el <strong>mejor predictor lineal (BLP’s)</strong>. Como demostraremos luego, los predictores lineales dependen solo del segundo momento del proceso, lo cual es fácil de estimar a partir de los datos.</p>
<p>A continuación daremos algunas propiedades y ejemplos.</p>

<div class="proposition">
<p><span id="prp:propiedad-mejor-predictor-lineal" class="proposition"><strong>Proposición 6.3  (Mejor Predictor Lineal para Procesos Estacionarios)  </strong></span> Dada las observaciones <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span>, el mejor predictor lineal <span class="math inline">\(x_{n+m}^n=\alpha_0+\sum_{k=1}^{n}\alpha_kx_k\)</span>, de <span class="math inline">\(x_{n+m}\)</span> para <span class="math inline">\(m\geq1\)</span>, se halla resolviendo</p>
<span class="math display" id="eq:eq-mejor-predictor-lineal">\[\begin{equation}
  \mathbb{E}\left[(x_{n+m}-x_{n+m}^n)x_k\right]=0\text{, para } k=0,1,2,\ldots
\tag{6.39}
\end{equation}\]</span>
donde <span class="math inline">\(x_0=1\)</span>.
</div>

<hr />
<p>Las ecuaciones especificadas en <a href="modelos-arma.html#eq:eq-mejor-predictor-lineal">(6.39)</a> son llamadas ecuaciones de predicción, y son usadas para resolver los coeficientes <span class="math inline">\(\{\alpha_0,\alpha_1,\ldots,\alpha_n\}\)</span>. Si <span class="math inline">\(\mathbb{E}(x_t)=\mu\)</span>, la primera ecuación (<span class="math inline">\(k=0\)</span>) de <a href="modelos-arma.html#eq:eq-mejor-predictor-lineal">(6.39)</a> implica</p>
<p><span class="math display">\[\mathbb{E}(x_{n+m}^n)=\mathbb{E}(x_{m+n})=\mu.\]</span></p>
<p>Entonces, tomando valor esperado en <a href="modelos-arma.html#eq:eq-predictores">(6.38)</a>, tenemos</p>
<p><span class="math display">\[\mu=\alpha_0+\sum_{k=1}^{n}\alpha_k\mu\text{  o   }\alpha_0=\mu\left(1-\sum_{k=1}^{n}\alpha_k\right).\]</span></p>
<p>Por lo tanto, la forma del BLP es <span class="math display">\[x_{n+m}^n=\mu+\sum_{k=1}^{n}\alpha_k(x_k-\mu).\]</span></p>
<p>Sin perdida de generalidad, podemos considerar el caso <span class="math inline">\(\mu=0\)</span> en cuyo caso, <span class="math inline">\(\alpha_0=0\)</span></p>
<p>Consideremos primero la predicción de un paso. Esto es, dado <span class="math inline">\(\{x_1,x_2,\ldots,x_n\}\)</span>, queremos predecir el valor la serie temporal en tiempo <span class="math inline">\(t=n+1\)</span>, o sea <span class="math inline">\(x_{n+1}\)</span>. El BLP de <span class="math inline">\(x_{n+1}\)</span> es</p>
<span class="math display" id="eq:eq-BLP-1-paso">\[\begin{equation}
  x_{n+1}^n=\phi_{n1}x_n+\phi_{n2}x_{n-1}+\cdots+\phi_{nn}x_1
\tag{6.40}
\end{equation}\]</span>
<p>donde, <span class="math inline">\(\alpha_k\)</span> en <a href="modelos-arma.html#eq:eq-predictores">(6.38)</a> lo escribiremos como <span class="math inline">\(\phi_{n,n+1-k}\)</span> en <a href="modelos-arma.html#eq:eq-BLP-1-paso">(6.40)</a>, para <span class="math inline">\(k=1,2,\ldots,n\)</span>. Usando la proposición <a href="modelos-arma.html#prp:propiedad-mejor-predictor-lineal">6.3</a>, los coeficientes <span class="math inline">\(\{\phi_{n1},\phi_{n2},\ldots,\phi_{nn}\}\)</span> satisfacen</p>
<p><span class="math display">\[\mathbb{E}\left[\left(x_{n+1}-\sum_{j=1}^{n}\phi_{nj}x_{n+1-j}\right)x_{n+1-k}\right]=0\text{, para }k=1,2,\ldots,n\]</span></p>
<p>o</p>
<span class="math display" id="eq:eq-coeficientes-BLP">\[\begin{equation}
  \sum_{j=1}^{n}\phi_{nj}\gamma(k-j)=\gamma(k)\text{, }k=1,2,\ldots,n
\tag{6.41}
\end{equation}\]</span>
<p>Las ecuaciones de predicción <a href="modelos-arma.html#eq:eq-coeficientes-BLP">(6.41)</a> se pueden escribir en forma matricial como</p>
<span class="math display" id="eq:eq-prediccion-BLP-matricial">\[\begin{equation}
  \Gamma_n\vec{\phi}_n=\vec{\gamma}_n
\tag{6.42}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\Gamma_n=\{\gamma(k-j)\}_{j,k=1}^n\)</span> es una matriz <span class="math inline">\(n\times n\)</span>, <span class="math inline">\(\vec{\phi}_n=(\phi_{n1},\phi_{n2},\ldots,\phi_{nn})^t\)</span> es un vector <span class="math inline">\(n\times1\)</span> y <span class="math inline">\(\vec{\gamma}_n=(\gamma(1),\gamma(2),\ldots,\gamma(n))^t\)</span> es un vector <span class="math inline">\(n\times1\)</span>.</p>
<p>La matriz <span class="math inline">\(\Gamma_n\)</span> es no-negativa definida. Si <span class="math inline">\(\Gamma_n\)</span> es singular, existen muchas soluciones de <a href="modelos-arma.html#eq:eq-prediccion-BLP-matricial">(6.42)</a>, pero por el Teorema de Proyección (véase Cramer &amp; Leadbetter (1967)) <span class="math inline">\(x_{n+1}^n\)</span> es único. Si <span class="math inline">\(\Gamma_n\)</span> es no singular, los elementos de <span class="math inline">\(\vec{\phi}_n\)</span> son únicos, y están dados por</p>
<span class="math display" id="eq:eq-elementos-matriz-phi-n">\[\begin{equation}
  \vec{\phi}_n=\Gamma_n^{-1}\vec{\gamma}_n.
\tag{6.43}
\end{equation}\]</span>
<p>Para un modelo ARMA, el hecho de que <span class="math inline">\(\sigma_w^2&gt;0\)</span> y <span class="math inline">\(\gamma(h)\to0\)</span> cuando <span class="math inline">\(h\to\infty\)</span> es suficiente para asegurar que <span class="math inline">\(\Gamma_n\)</span> es positiva definida.</p>
<p>A veces es conveniente escribir el pronóstico de un paso en forma vectorial</p>
<span class="math display" id="eq:eq-BLP-1-paso-matricial">\[\begin{equation} 
  x_{n+1}^n=\vec{\phi}^t_n\mathbf{x}
\tag{6.44}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\mathbf{x}=(x_1,x_2,\ldots,x_n)^t\)</span>. El error cuadrático medio de la predicción de un paso es</p>
<span class="math display" id="eq:eq-ecm-prediccion-1-paso">\[\begin{equation}
  P_{n+1}^n=\mathbb{E}(x_{n+1}-x_{n+1}^n)^2=\gamma(0)-\vec{\gamma}^t_n\Gamma_n^{-1}\vec{\gamma}_n.
\tag{6.45}
\end{equation}\]</span>
<p>Para verificar <a href="modelos-arma.html#eq:eq-ecm-prediccion-1-paso">(6.45)</a>, usemos <a href="modelos-arma.html#eq:eq-elementos-matriz-phi-n">(6.43)</a> y <a href="modelos-arma.html#eq:eq-BLP-1-paso-matricial">(6.44)</a></p>
<span class="math display">\[\begin{eqnarray*}
  \mathbb{E}(x_{n+1}-x_{n+1}^n)^2 &amp;=&amp; \mathbb{E}(x_{n+1}-\vec{\phi}^t_n\mathbf{x})^2=\mathbb{E}(x_{n+1}-\vec{\gamma}^t_n\Gamma_n^{-1}\mathbf{x})^2 \\
         &amp;=&amp; \mathbb{E}(x_{n+1}^2-2\vec{\gamma}^t_n\Gamma_n^{-1}\mathbf{x}x_{n+1}+\vec{\gamma}^t_n\Gamma_n^{-1}\mathbf{xx^t}\Gamma_n^{-1}\vec{\gamma}_n) \\
         &amp;=&amp; \gamma(0)-2\vec{\gamma}^t_n\Gamma_n^{-1}\vec{\gamma}_n+\vec{\gamma}^t_n\Gamma_n^{-1}\Gamma_n\Gamma_n^{-1}\vec{\gamma}_n \\
         &amp;=&amp; \gamma(0)-\vec{\gamma}^t_n\Gamma_n^{-1}\vec{\gamma}^t_n.
\end{eqnarray*}\]</span>

<div class="example">
<p><span id="exm:ejem-prediccion-AR2" class="example"><strong>Ejemplo 6.8  (Predicción para un AR(2))  </strong></span> Suponga que tenemos un proceso AR(2) causal <span class="math inline">\(x_t=\phi_1x_{t-1}+\phi_2x_{t-2}+w_t\)</span>, y una observación <span class="math inline">\(x_1\)</span>. Entonces, usando la ecuación <a href="modelos-arma.html#eq:eq-elementos-matriz-phi-n">(6.43)</a>, la predicción de <span class="math inline">\(x_2\)</span> basada en <span class="math inline">\(x_1\)</span> es <span class="math display">\[x_2^1=\phi_{11}x_1=\frac{\gamma(1)}{\gamma(0)}x_1=\rho(1)x_1\]</span> Ahora, supóngase que deseamos la predicción de <span class="math inline">\(x_3\)</span> basado en dos observaciones <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span>. Podemos usar <a href="modelos-arma.html#eq:eq-elementos-matriz-phi-n">(6.43)</a> de nuevo y resolver</p>
<p><span class="math display">\[x_3^2=\phi_{21}x_2+\phi_{22}x_1=(\gamma(1),\gamma(2))\left(
                                                         \begin{array}{cc}
                                                           \gamma(0) &amp; \gamma(1) \\
                                                           \gamma(1) &amp; \gamma(0) \\
                                                         \end{array}
                                                       \right)^{-1}\left(
                                                                     \begin{array}{c}
                                                                       x_2 \\
                                                                       x_1 \\
                                                                     \end{array}
                                                                   \right)
\]</span></p>
<p>pero, debe quedar claro a partir del modelo que <span class="math inline">\(x_3^2=\phi_1x_2+\phi_2x_1\)</span>.</p>
<p>Dado que <span class="math inline">\(\phi_1x_2+\phi_2x_1\)</span> satisface las ecuaciones de predicción <a href="modelos-arma.html#eq:eq-mejor-predictor-lineal">(6.39)</a></p>
<span class="math display">\[\begin{eqnarray*}
  \mathbb{E}\{[x_3-(\phi_1x_2+\phi_2x_1)]x_1\} &amp;=&amp; \mathbb{E}(w_3x_1)=0 \\
  \mathbb{E}\{[x_3-(\phi_1x_2+\phi_2x_1)]x_2\} &amp;=&amp; \mathbb{E}(w_3x_2)=0
\end{eqnarray*}\]</span>
<p>De ello se deduce que, de hecho <span class="math inline">\(x_3^2=\phi_1x_2+\phi_2x_1\)</span>, y por la unicidad de los coeficientes en este caso, que <span class="math inline">\(\phi_{21}=\phi_1\)</span> y <span class="math inline">\(\phi_{22}=\phi_2\)</span>. Continuando de esta misma manera, es fácil verificar que para <span class="math inline">\(n\geq2\)</span></p>
<p><span class="math display">\[x_{n+1}^n=\phi_1x_n+\phi_2x_{n-1}\]</span></p>
Esto es, <span class="math inline">\(\phi_{n1}=\phi_1\)</span>, <span class="math inline">\(\phi_{n2}=\phi_2\)</span> y <span class="math inline">\(\phi_{nj}=0\)</span> para <span class="math inline">\(j=3,4,\ldots,n\)</span>
</div>

<hr />
<p>Del ejemplo <a href="modelos-arma.html#exm:ejem-prediccion-AR2">6.8</a>, es claro que si la serie de tiempo es un proceso AR(p) causal, entonces para <span class="math inline">\(n\geq p\)</span></p>
<span class="math display" id="eq:eq-predictor-ARp">\[\begin{equation}
  x_{n+1}^n=\phi_1x_n+\phi_2x_{n-1}+\cdots+\phi_px_{n-p+1}.
\tag{6.46}
\end{equation}\]</span>
<p>Para modelos ARMA en general, las ecuaciones de predicción no serán tan simple como en el caso AR puro. Además, para <span class="math inline">\(n\)</span> grande, el uso de <a href="modelos-arma.html#eq:eq-elementos-matriz-phi-n">(6.43)</a> es prohibitivo, ya que requiere la inversión de una matriz grande. Sin embargo, existen soluciones iterativas que no requieren ninguna inversión de matriz. En particular, utilizaremos la solución recursiva de Levinson (1947) y Durbin (1960).</p>

<div class="proposition">
<p><span id="prp:propiedad-algoritmo-durbin-levinson" class="proposition"><strong>Proposición 6.4  (Algoritmo de Durbin-Levinson)  </strong></span> Las ecuaciones <a href="modelos-arma.html#eq:eq-elementos-matriz-phi-n">(6.43)</a> y <a href="modelos-arma.html#eq:eq-ecm-prediccion-1-paso">(6.45)</a> se pueden resolver iterativamente como sigue:</p>
<span class="math display" id="eq:eq-phi00-P10">\[\begin{equation}
  \phi_{00}=0\text{, } P_1^0=\gamma(0)
\tag{6.47}
\end{equation}\]</span>
<p>Para <span class="math inline">\(n\geq1\)</span></p>
<span class="math display" id="eq:eq-phi-nn-P-n">\[\begin{equation}
  \phi_{nn}=\frac{\rho(n)-\sum_{k=1}^{n-1}\phi_{n-1,k}\rho(n-k)}{1-\sum_{k=1}^{n-1}\phi_{n-1,k}\rho(k)}\text{, con }P_{n+1}^n=P_n^{n-1}(1-\phi_{nn}^2)
\tag{6.48}
\end{equation}\]</span>
<p>donde, para <span class="math inline">\(n\geq2\)</span></p>
<span class="math display" id="eq:eq-coeficientes-phi-durbin-levinson">\[\begin{equation}
  \phi_{nk}=\phi_{n-1,k}-\phi_{nn}\phi_{n-1,k-1}\text{, para }k=1,2,\ldots,n-1
\tag{6.49}
\end{equation}\]</span>
</div>

<hr />

<div class="example">
<p><span id="exm:ejem-algoritmo-durbin-levinson" class="example"><strong>Ejemplo 6.9  (Uso del Algoritmo Durbin-Levinson)  </strong></span> Para usar el algoritmo, iniciemos con <span class="math inline">\(\phi_{00}=0, P_1^0=\gamma(0)\)</span>. Entonces, para <span class="math inline">\(n=1\)</span>,</p>
<p><span class="math display">\[\phi_{11}=\rho(1)\text{ y }P_2^1=\gamma(0)[1-\phi_{11}^2].\]</span></p>
<p>Para <span class="math inline">\(n=2\)</span></p>
<span class="math display">\[\begin{eqnarray*}
  \phi_{22} &amp;=&amp; \frac{\rho(2)-\phi_{11}\rho(1)}{1-\phi_{11}\rho(1)}=\frac{\rho(2)-\rho(1)^2}{1-\rho(1)^2} \\
  \phi_{21} &amp;=&amp; \phi_{11}-\phi_{22}\phi_{11}=\rho(1)[1-\phi_{22}] \\
  P_3^2 &amp;=&amp; \gamma(0)[1-\phi_{11}^2][1-\phi_{22}^2]
\end{eqnarray*}\]</span>
<p>Para <span class="math inline">\(n=3\)</span></p>
<p><span class="math display">\[\phi_{33}=\frac{\rho(3)-\phi_{21}\rho(2)-\phi_{22}\rho(1)}{1-\phi_{21}\rho(1)-\phi_{22}\rho(2)}\]</span></p>
y así sucesivamente.
</div>

<hr />
<p>Una consecuencia importante del algoritmo de Durbin-Levinson es la siguiente propiedad.</p>

<div class="proposition">
<span id="prp:propiedad-solucion-iterativa-PACF" class="proposition"><strong>Proposición 6.5  (Solución Iterativa para la PACF)  </strong></span> La PACF de un proceso estacionario <span class="math inline">\(x_t\)</span>, se puede obtener via iteración de <span class="math display">\[\phi_{nn}=\frac{\rho(n)-\sum_{k=1}^{n-1}\phi_{n-1,k}\rho(n-k)}{1-\sum_{k=1}^{n-1}\phi_{n-1,k}\rho(k)}\text{, con }P_{n+1}^n=P_n^{n-1}(1-\phi_{nn}^2)\]</span> como <span class="math inline">\(\phi_{nn}\)</span>, para <span class="math inline">\(n=1,2,\ldots\)</span>
</div>

<hr />

<div class="example">
<p><span id="exm:ejem-PACF-AR2" class="example"><strong>Ejemplo 6.10  (La PACF de un AR(2))  </strong></span> Del ejemplo <a href="modelos-ar.html#exm:ejem-PACF-ARp-causal">4.4</a>, sabemos que para un AR(2), <span class="math inline">\(\phi_{hh}=0\)</span> para <span class="math inline">\(h&gt;2\)</span>, pero usaremos los resultados del ejemplo <a href="modelos-arma.html#exm:ejem-prediccion-AR2">6.8</a> y la proposición <a href="modelos-arma.html#prp:propiedad-solucion-iterativa-PACF">6.5</a> para calcular los primeros tres valores de la PACF. Recuerde (Ejemplo <a href="modelos-arma.html#exm:ejem-ACF-AR2">6.4</a>) que para un AR(2), <span class="math inline">\(\rho(1)=\phi_1/(1-\phi_2)\)</span> y en general <span class="math inline">\(\rho(h)-\phi_1\rho(h-1)-\phi_2\rho(h-2)=0\)</span> para <span class="math inline">\(h\geq2\)</span>. Entonces</p>
<span class="math display">\[\begin{eqnarray*}
  \phi_{11} &amp;=&amp; \rho(1)=\frac{\phi_1}{1-\phi_2} \\
  \phi_{22} &amp;=&amp; \frac{\rho(2)-\rho(1)^2}{1-\rho(1)^2}=\frac{\left[\phi_1\left(\frac{\phi_1}{1-\phi_2}\right)+\phi_2\right]-\left(\frac{\phi_1}{1-\phi_2}\right)^2}{1-\left(\frac{\phi_1}{1-\phi_2}\right)^2}=\phi_2 \\
  \phi_{21} &amp;=&amp; \phi_1 \\
  \phi_{33} &amp;=&amp; \frac{\rho(3)-\phi_1\rho(2)-\phi_2\rho(1)}{1-\phi_1\rho(1)-\phi_2\rho(2)}=0.
\end{eqnarray*}\]</span>
</div>

<hr />
<p>Hasta ahora nos hemos concentrado en la predicción de un paso, pero la proposición <a href="modelos-arma.html#prp:propiedad-mejor-predictor-lineal">6.3</a> nos permite calcular el BLP de <span class="math inline">\(x_{n+m}\)</span> para cada <span class="math inline">\(m\geq1\)</span>. Dado los datos <span class="math inline">\(\{x_1,x_2,\ldots,x_n\}\)</span> el predictor de <span class="math inline">\(m\)</span> pasos es</p>
<span class="math display" id="eq:eq-predictor-m-pasos">\[\begin{equation}
  x_{n+m}^n=\phi_{n1}^{(m)}x_n+\phi_{n2}^{(m)}x_{n-1}+\cdots+\phi_{nn}^{(m)}x_1
\tag{6.50}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\{\phi_{n1}^{(m)},\phi_{n2}^{(m)},\ldots,\phi_{nn}^{(m)}\}\)</span> satisfacen las ecuaciones de predicción</p>
<p><span class="math display">\[\sum_{j=1}^{n}\phi_{nj}^{(m)}\mathbb{E}(x_{n+1-j}x_{n+1-k})=\mathbb{E}(x_{n+m}x_{n+1-k})\text{, }k=1,2,\ldots,n\]</span></p>
<p>o</p>
<span class="math display" id="eq:eq-prediccion-m-pasos">\[\begin{equation}
  \sum_{j=1}^{n}\phi_{nj}^{(m)}\gamma(k-j)=\gamma(m+k-1)\text{, }k=1,2,\ldots,n
\tag{6.51}
\end{equation}\]</span>
<p>Las ecuaciones de predicción se pueden escribir nuevamente en forma matricial como</p>
<span class="math display" id="eq:eq-prediccion-m-pasos-matricial">\[\begin{equation}
  \Gamma_n\vec{\phi}_n^{(m)}=\vec{\gamma}_n^{(m)}
\tag{6.52}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\vec{\gamma}_n^{(m)}=(\gamma(m),\ldots,\gamma(m+n-1))^t\)</span> y <span class="math inline">\(\vec{\phi}_n^{(m)}=(\phi_{n1}^{(m)},\phi_{n2}^{(m)},\ldots,\phi_{nn}^{(m)})^t\)</span> son vectores <span class="math inline">\(n\times1\)</span>.</p>
<p>El error cuadrático medio del predictor de <span class="math inline">\(m\)</span> pasos es</p>
<span class="math display" id="eq:eq-ecm-prediccion-m-pasos">\[\begin{equation}
  P_{n+m}^n=\mathbb{E}(x_{n+m}-x_{n+m}^n)^2=\gamma(0)-(\vec{\gamma}_n^{(m)})^t\Gamma_n^{-1}\vec{\gamma}_n^{(m)}
\tag{6.53}
\end{equation}\]</span>
<p>Otro algoritmo útil para calcular pronósticos es dado por Brockwell &amp; Davis (1996)[]. Este algoritmo se obtiene por aplicación directa del Teorema de Proyección a <span class="math inline">\(x_t-x_t^{t-1}\)</span> para <span class="math inline">\(t=1,2,\ldots,n\)</span> usando el hecho de que <span class="math inline">\(x_t-x_t^{t-1}\)</span> y <span class="math inline">\(x_s-x_s^{s-1}\)</span> son no-correlacionados para <span class="math inline">\(s\neq t\)</span>. Presentamos el caso en el cual <span class="math inline">\(x_t\)</span> es una serie de tiempo estacionaria de media cero.</p>

<div class="proposition">
<p><span id="prp:propiedad-algoritmo-innovaciones" class="proposition"><strong>Proposición 6.6  (Algoritmo de Innovaciones)  </strong></span> Los predictores <span class="math inline">\(x_{t+1}^t\)</span> y sus errores cuadráticos medios <span class="math inline">\(P_{t+1}^t\)</span> se pueden calcular iterativamente como</p>
<p><span class="math display">\[x_1^0=0\text{, }P_1^0=\gamma(0)\]</span></p>
<span class="math display" id="eq:eq-interactiva-predictor-x">\[\begin{equation}\label{}
  x_{t+1}^t=\sum_{j=1}^{t}\theta_{tj}(x_{t+1-j}-x_{t+1-j}^{t-j})\text{, } t=1,2,\ldots
\tag{6.54}
\end{equation}\]</span>
<span class="math display" id="eq:eq-interactiva-ecm-x">\[\begin{equation}
    P_{t+1}^t=\gamma(0)-\sum_{j=0}^{t-1}\theta_{t,t-j}^2P_{j+1}^j\text{, }t=1,2,\ldots
\tag{6.55}
\end{equation}\]</span>
<p>donde, para <span class="math inline">\(j=0,1,\ldots,t-1\)</span>,</p>
<span class="math display" id="eq:eq-coeficiente-innovacion">\[\begin{equation}
  \theta_{t,t-j}=\left(\gamma(t-j)-\sum_{k=0}^{j-1}\theta_{j,j-k}\theta_{t,t-k}P_{k+1}^k\right)\left(P_{j+1}^j\right)^{-1}
\tag{6.56}
\end{equation}\]</span>
</div>

<hr />
<p>Dado los datos <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> el algoritmo de innovación se puede calcular sucesivamente para <span class="math inline">\(t=1\)</span>, luego <span class="math inline">\(t=2\)</span> y así hasta <span class="math inline">\(t=n\)</span>, en cuyo caso obtenemos el predictor <span class="math inline">\(x_{n+1}^n\)</span> y el error cuadrático medio <span class="math inline">\(P_{n+1}^n\)</span>. El predictor de <span class="math inline">\(m\)</span> pasos y el error cuadrático medio basado en el algoritmo de innovación son dados por</p>
<span class="math display" id="eq:eq-interactiva-ecm-x-m-pasos" id="eq:eq-interactiva-predictor-x-m-pasos">\[\begin{eqnarray}
  x_{n+m}^n &amp;=&amp; \sum_{j=m}^{n+m-1}\theta_{n+m-1,j}(x_{n+m-j}-x_{n+m-j}^{n+m-j-1})\tag{6.57} \\
  P_{n+m}^n &amp;=&amp; \gamma(0)-\sum_{j=m}^{n+m-1}\theta_{n+m-1,j}^2P_{n+m-j}^{n+m-j-1}\tag{6.58}
\end{eqnarray}\]</span>
<p>donde los <span class="math inline">\(\theta_{n+m-1,j}\)</span> se obtienen por iteración continua de <a href="modelos-arma.html#eq:eq-coeficiente-innovacion">(6.56)</a>.</p>

<div class="example">
<p><span id="exm:ejem-prediccion-MA1" class="example"><strong>Ejemplo 6.11  (Predicción de un MA(1))  </strong></span> El algoritmo de innovación nos da un buen predictor para un proceso de promedio móvil. Considere un modelo MA(1), <span class="math inline">\(x_t=w_t+\theta x_{t-1}\)</span>. Recuerde que <span class="math inline">\(\gamma(0)=(1+\theta^2)\sigma_w^2, \gamma(1)=\theta\sigma_w^2\)</span> y <span class="math inline">\(\gamma(h)=0\)</span> para <span class="math inline">\(h&gt;1\)</span>. Entonces, usando la proposición <a href="modelos-arma.html#prp:propiedad-algoritmo-innovaciones">6.6</a>, tenemos,</p>
<span class="math display">\[\begin{eqnarray*}
  \theta_{n1} &amp;=&amp; \theta\sigma_w^2/P_n^{n-1} \\
  \theta_{nj} &amp;=&amp; 0\text{, para }j=2,3,\ldots,n \\
  P_1^0 &amp;=&amp; (1+\theta^2)\sigma_w^2 \\
  P_{n+1}^n &amp;=&amp; (1+\theta^2-\theta\theta_{n1})\sigma_w^2
\end{eqnarray*}\]</span>
<p>Finalmente, de <a href="modelos-arma.html#eq:eq-interactiva-predictor-x">(6.54)</a> el predictor de un paso es</p>
<span class="math display">\[x_{n+1}^n=\theta(x_n-x_n^{n-1})\sigma_w^2/P_n^{n-1}\]</span>
</div>

<h3 id="pronósticos-para-procesos-arma"><span class="header-section-number">6.2</span> Pronósticos para procesos ARMA</h3>
<p>Las ecuaciones de predicción general <a href="modelos-arma.html#eq:eq-mejor-predictor-lineal">(6.39)</a> nos dan una pequeña intuición en el pronóstico de los modelos ARMA en general. Hay diferentes maneras de expresar estos pronósticos, y cada uno ayuda a entender la estructura especial de la predicción ARMA. A través de toda esta sección asumiremos que <span class="math inline">\(x_t\)</span> es un proceso ARMA(p,q) causal e invertible <span class="math inline">\(\phi(B)x_t=\theta(B)w_t\)</span> donde <span class="math inline">\(w_t\sim\text{iid}N(0,\sigma_w^2)\)</span>. En el caso de media distinto de cero, <span class="math inline">\(\mathbb{E}(x_t)=\mu\)</span>, reemplazamos <span class="math inline">\(x_t\)</span> por <span class="math inline">\(x_t-\mu\)</span> en el modelo.</p>
<p>Primero consideraremos dos tipos de pronósticos. Escribiremos el mínimo del error cuadrático medio del predictor <span class="math inline">\(x_{n+m}\)</span> como <span class="math inline">\(x_{n+m}^n\)</span> basado en los datos <span class="math inline">\(\{x_n,x_{n-1},\ldots,x_1\}\)</span>, esto es</p>
<p><span class="math display">\[x_{n+m}^n=\mathbb{E}(x_{n+m}|x_n,x_{n-1},\ldots,x_1).\]</span></p>
<p>Para un modelo ARMA, es fácil calcular el predictor de <span class="math inline">\(x_{n+m}\)</span> asumiendo que tenemos el historial completo del proceso <span class="math inline">\(\{x_n,x_{n-1},\ldots\}\)</span>. Denotaremos el predictor de <span class="math inline">\(x_{n+m}\)</span> basado en <em>infinitos valores pasados</em> como</p>
<p><span class="math display">\[\tilde{x}_{n+m}=\mathbb{E}(x_{n+m}|x_n,x_{n-1},\ldots).\]</span></p>
<p>La idea aquí, es que para muestra grandes <span class="math inline">\(\tilde{x}_{n+m}\)</span> proveerá una buena aproximación de <span class="math inline">\(x_{n+m}^n\)</span>.</p>
<p>Ahora, escribamos <span class="math inline">\(x_{n+m}\)</span> en sus formas causal e invertible</p>
<span class="math display" id="eq:eq-predictor-forma-invertible" id="eq:eq-predictor-forma-causal">\[\begin{eqnarray}
  x_{n+m} &amp;=&amp; \sum_{j=0}^{\infty}\psi_jw_{n+m-j}\text{, }\psi_0=1 \tag{6.59} \\
  w_{n+m} &amp;=&amp; \sum_{j=0}^{\infty}\pi_jx_{n+m-j}\text{, }\pi_0=1 \tag{6.60}
\end{eqnarray}\]</span>
<p>Entonces, tomando esperanza condicional en <a href="modelos-arma.html#eq:eq-predictor-forma-causal">(6.59)</a>, tenemos</p>
<span class="math display" id="eq:eq-esperanza-predictor-causal">\[\begin{equation}
  \tilde{x}_{n+m}=\sum_{j=0}^{\infty}\psi_j\tilde{w}_{n+m-j}=\sum_{j=m}^{\infty}\psi_jw_{n+m-j}
\tag{6.61}
\end{equation}\]</span>
<p>ya que por <a href="modelos-arma.html#eq:eq-predictor-forma-invertible">(6.60)</a></p>
<p><span class="math display">\[\tilde{w}_t\equiv\mathbb{E}(w_t|x_n,x_{n-1},\ldots)=\begin{cases}
                    0,&amp; t&gt;n\\
                    w_t,&amp; t\leq n
                \end{cases}\]</span></p>
<p>Similarmente, tomando esperanza condicional en <a href="modelos-arma.html#eq:eq-predictor-forma-invertible">(6.60)</a>, se tiene</p>
<p><span class="math display">\[0=\tilde{x}_{n+m}+\sum_{j=1}^{\infty}\pi_j\tilde{x}_{n+m-j}\]</span></p>
<p>o</p>
<span class="math display" id="eq:eq-esperanza-predictor-invertible">\[\begin{equation}
  \tilde{x}_{n+m}=-\sum_{j=1}^{m-1}\pi_j\tilde{x}_{n+m-j}-\sum_{j=m}^{\infty}\pi_jx_{n+m-j}
\tag{6.62}
\end{equation}\]</span>
<p>usando el hecho de que <span class="math inline">\(\mathbb{E}(x_t|x_n,x_{n-1},\ldots)=x_t\)</span> para <span class="math inline">\(t\leq n\)</span>.</p>
<p>La predicción se consigue recursivamente usando <a href="modelos-arma.html#eq:eq-esperanza-predictor-invertible">(6.62)</a> iniciando con un predictor de un paso <span class="math inline">\(m=1\)</span> y continuando para <span class="math inline">\(m=2,3,\ldots\)</span>. Usando <a href="modelos-arma.html#eq:eq-esperanza-predictor-invertible">(6.62)</a> podemos escribir</p>
<p><span class="math display">\[x_{n+m}-\tilde{x}_{n+m}=\sum_{j=0}^{m-1}\psi_jw_{n+m-j}\]</span></p>
<p>de modo que el error cuadrático medio de predicción se puede escribir como</p>
<span class="math display" id="eq:eq-ecm-prediccion">\[\begin{equation}
  P_{n+m}^n=\mathbb{E}(x_{n+m}-\tilde{x}_{n+m})^2=\sigma_w^2\sum_{j=0}^{m-1}\psi_j^2
\tag{6.63}
\end{equation}\]</span>
<p>También, observe que para una muestra fija de tamaño <span class="math inline">\(n\)</span> los errores de predicción están correlacionados. Esto es, para <span class="math inline">\(k\geq1\)</span>,</p>
<span class="math display" id="eq:eq-correlacion-ecm-prediccion">\[\begin{equation}
  \mathbb{E}[(x_{n+m}-\tilde{x}_{n+m})(x_{n+m+k}-\tilde{x}_{n+m+k})]=\sigma_w^2\sum_{j=0}^{m-1}\psi_j\psi_{j+k}
\tag{6.64}
\end{equation}\]</span>

<div class="example">
<p><span id="exm:ejem-pronostico-largo-plazo" class="example"><strong>Ejemplo 6.12  (Pronóstico a largo plazo)  </strong></span> Consideremos el pronóstico para un proceso ARMA de media <span class="math inline">\(\mu\)</span>. Del caso de media cero en <a href="modelos-arma.html#eq:eq-esperanza-predictor-causal">(6.61)</a> podemos deducir que el pronóstico de <span class="math inline">\(m\)</span> pasos se puede escribir como</p>
<span class="math display" id="eq:eq-pronostico-m-pasos">\[\begin{equation}
    \tilde{x}_{n+m}=\mu+\sum_{j=m}^{\infty}\psi_jw_{n+m-j}
\tag{6.65}
\end{equation}\]</span>
<p>Note que los <span class="math inline">\(\psi\)</span> pesos decrece a cero de forma exponencial, es claro entonces que</p>
<p><span class="math display">\[\tilde{x}_{n+m}\to\mu\]</span></p>
<p>exponencialmente (en el sentido de media cuadrado) cuando <span class="math inline">\(m\to\infty\)</span>. Más aún, por <a href="modelos-arma.html#eq:eq-ecm-prediccion">(6.63)</a> el error cuadrático medio de predicción</p>
<span class="math display" id="eq:eq-convergencia-ecm-prediccion">\[\begin{equation}
  P_{n+m}^n\to\sigma_w^2\sum_{j=0}^{\infty}\psi_j^2,
\tag{6.66}
\end{equation}\]</span>
<p>exponencialmente cuando <span class="math inline">\(m\to\infty\)</span>.</p>
Es claro de <a href="modelos-arma.html#eq:eq-pronostico-m-pasos">(6.65)</a> y <a href="modelos-arma.html#eq:eq-convergencia-ecm-prediccion">(6.66)</a> que el pronóstico de un proceso ARMA rápidamente se estabiliza a la media con un error de predicción constante a medida que el periodo de pronóstico <span class="math inline">\(m\)</span> crece.
</div>

<hr />
<p>Cuando <span class="math inline">\(n\)</span> es pequeño, las ecuaciones generales de predicción <a href="modelos-arma.html#eq:eq-mejor-predictor-lineal">(6.39)</a> se puede usar fácilmente. Cuando <span class="math inline">\(n\)</span> es grande, usaremos <a href="modelos-arma.html#eq:eq-esperanza-predictor-causal">(6.61)</a> por truncamiento, porque solo tenemos disponibles las observaciones <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span>. En este caso truncamos <a href="modelos-arma.html#eq:eq-esperanza-predictor-causal">(6.61)</a> haciendo</p>
<p><span class="math display">\[\sum_{j=n+m}^{\infty}\pi_jx_{n+m-j}=0\]</span></p>
<p>El predictor truncado se escribe entonces como</p>
<span class="math display" id="eq:eq-predictor-truncado">\[\begin{equation}
  \tilde{x}_{n+m}^n=-\sum_{j=1}^{m-1}\pi_j\tilde{x}_{n+m-j}^n-\sum_{j=m}^{n+m-1}\pi_jx_{n+m-j}
\tag{6.67}
\end{equation}\]</span>
<p>el cual es también calculado recursivamente para <span class="math inline">\(m=1,2,\ldots\)</span>. El error cuadrático medio de predicción, en este caso, se aproxima por <a href="modelos-arma.html#eq:eq-ecm-prediccion">(6.63)</a>.</p>
<p>Para un modelo AR(p) y cuando <span class="math inline">\(n&gt;p\)</span> la ecuación <a href="modelos-arma.html#eq:eq-predictor-ARp">(6.46)</a> nos da el predictor exacto <span class="math inline">\(x_{n+m}^n\)</span> de <span class="math inline">\(x_{n+m}\)</span> y no hay necesidad de aproximación. Esto es, para <span class="math inline">\(n&gt;p\)</span>, <span class="math inline">\(\tilde{x}_{n+m}^n=\tilde{x}_{n+m}=x_{n+m}^n\)</span>.</p>
<p>También, en este caso, el error de predicción de un paso es <span class="math inline">\(\mathbb{E}(x_{n+1}-x_{n+1}^n)^2=\sigma_w^2\)</span>. Para un modelo ARMA(p,q) en general, los predictores truncados para <span class="math inline">\(m=1,2,\ldots\)</span>, son</p>
<span class="math display" id="eq:eq-predictor-truncado-ARMApq">\[\begin{equation}
  \tilde{x}_{n+m}^n=\phi_1\tilde{x}_{n+m}^n+\cdots+\phi_p\tilde{x}_{n+m-p}^n+\theta_1\tilde{w}_{n+m-1}^n+\cdots+\theta_q\tilde{w}_{n+m-q}^n
\tag{6.68}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\tilde{x}_t^n=x_t\)</span> para <span class="math inline">\(1\leq t\leq n\)</span> y <span class="math inline">\(\tilde{x}_t^n=0\)</span> para <span class="math inline">\(t\leq0\)</span>. Los errores de predicción truncados están dados por:</p>
<span class="math display">\[\begin{equation*}
  \begin{cases}
  \tilde{w}_t^n=0&amp;\text{ para }t\leq0\text{ ó }t&gt;n\\
  \tilde{w}_t^n=\phi(B)\tilde{x}_t^n-\theta_1\tilde{w}_{t-1}^n-\cdots-\theta_q\tilde{w}_{t-q}^n&amp;\text{ para }1\leq t\leq n.
  \end{cases}
\end{equation*}\]</span>

<div class="example">
<p><span id="exm:ejem-pronostico-ARMA11" class="example"><strong>Ejemplo 6.13  (Pronóstico para una serie ARMA(1,1))  </strong></span> Dado los datos <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> para propósito de pronósticos, escribiremos el modelo como</p>
<p><span class="math display">\[x_{n+1}=\phi x_n+w_{n+1}+\theta w_n.\]</span></p>
<p>Entonces, basado en <a href="modelos-arma.html#eq:eq-predictor-truncado-ARMApq">(6.68)</a>, el pronóstico truncado de un paso es</p>
<p><span class="math display">\[\tilde{x}_{n+1}^n=\phi x_n+0+\theta\tilde{w}_n^n.\]</span></p>
<p>Para <span class="math inline">\(m\geq2\)</span>, tenemos</p>
<p><span class="math display">\[\tilde{x}_{n+m}^n=\phi\tilde{x}_{n+m-1}^n,\]</span></p>
<p>el cual puede ser calculado recursivamente para <span class="math inline">\(m=1,2,\ldots\)</span>.</p>
<p>Para calcular <span class="math inline">\(\tilde{w}_n^n\)</span>, que se necesitará para iniciar los pronósticos sucesivos, podemos escribir el modelo como <span class="math inline">\(w_t=x_t-\phi x_{t-1}-\theta w_{t-1}\)</span> para <span class="math inline">\(t=1,2,\ldots,n\)</span>. Para el pronóstico truncado, usando <a href="modelos-arma.html#eq:eq-predictor-truncado-ARMApq">(6.68)</a> hacemos <span class="math inline">\(\tilde{w}_0^n=0, \tilde{w}_1^n=x_1\)</span> y entonces iteramos el error</p>
<p><span class="math display">\[\tilde{w}_t^n=x_t-\phi x_{t-1}-\theta\tilde{w}_{t-1}^n\text{, }t=2,3,\ldots,n.\]</span></p>
<p>La varianza aproximada del pronóstico se calcula de <a href="modelos-arma.html#eq:eq-ecm-prediccion">(6.63)</a> usando los <span class="math inline">\(\psi\)</span> pesos determinados como en el ejemplo <a href="modelos-arma.html#exm:ejem-pesos-ARMA-causal">6.6</a>. En particular, los <span class="math inline">\(\psi\)</span> pesos satisfacen <span class="math inline">\(\psi_j=(\phi+\theta)\phi^{j-1}\)</span> para <span class="math inline">\(j\geq1\)</span>. Este resultado nos da</p>
<span class="math display">\[\begin{eqnarray*}
  P_{n+m}^n &amp;=&amp; \sigma_w^2\left[1+(\phi+\theta)^2\sum_{j=1}^{m-1}\phi^{2(j-1)}\right] \\
            &amp;=&amp; \sigma_w^2\left[1+\frac{(\phi+\theta)^2(1-\phi^{2(m-1)})}{(1-\phi^2)}\right]
\end{eqnarray*}\]</span>
</div>

<hr />
<p>Para evaluar la precisión de los pronósticos, se calculan los intervalos de predicción junto con el pronóstico. En general, los <span class="math inline">\((1-\alpha)\)</span> intervalos de predicción son de la forma</p>
<span class="math display" id="eq:eq-intervalos-prediccion">\[\begin{equation}
  x_{n+m}^n\pm c_{\frac{\alpha}{2}}\sqrt{P_{n+m}^n}
\tag{6.69}
\end{equation}\]</span>
<p>donde <span class="math inline">\(c_{\alpha/2}\)</span> se elige de manera de obtener el grado deseado de confidencia. Por ejemplo, si el proceso es gaussiano, entonces elegimos <span class="math inline">\(c_{\alpha/2}=2\)</span> los cual nos da un intervalo de predicción de aproximadamente 95% para <span class="math inline">\(x_{n+m}\)</span>. Si estamos interesados en establecer un intervalos de predicción sobre más de un periodo de tiempo, entonces <span class="math inline">\(c_{\alpha/2}\)</span> se ajustará apropiadamente, por ejemplo, usando la desigualdad de Bonferroni. (véase Shumway (2006), Capítulo 4)</p>

<div class="example">
<p><span id="exm:ejem-pronostico-serie-reclutamiento" class="example"><strong>Ejemplo 6.14  (Pronóstico para la serie de nuevos peces)  </strong></span> Usando los parámetros estimados como los valores actuales de los parámetros, la figura <a href="modelos-arma.html#fig:grafico-pronostico-serie-reclutamiento">6.3</a> muestra los resultados de la serie de nuevos peces dada en el ejemplo <a href="modelos-ar.html#exm:ejem-serie-nuevos-peces-AR2">4.5</a>, sobre un periodo de 24 meses <span class="math inline">\(m=1,2,\ldots,24\)</span>.</p>
<p>Los pronósticos actuales se calculan como</p>
<p><span class="math display">\[x_{n+m}^n=6.74+1.35x_{n+m-1}^n-0.46x_{n+m-2}^n\]</span></p>
<p>para <span class="math inline">\(n=453\)</span> y <span class="math inline">\(m=1,2,\ldots,12\)</span>. Recuerde que <span class="math inline">\(x_t^s=x_t\)</span> cuando <span class="math inline">\(t\leq s\)</span>. Los errores de pronóstico <span class="math inline">\(P_{n+m}^n\)</span> se calculan usando <a href="modelos-arma.html#eq:eq-ecm-prediccion">(6.63)</a>. Recuerde que <span class="math inline">\(\hat{\sigma}_w^2=90.31\)</span>, y usando <a href="modelos-arma.html#eq:eq-diferencias-homogeneas-pesos">(6.28)</a> del ejemplo <a href="modelos-ar.html#exm:ejem-serie-nuevos-peces-AR2">4.5</a> tenemos <span class="math inline">\(\psi_j=1.35\psi_{j-1}-0.46\psi_{j-2}\)</span> para <span class="math inline">\(j\geq2\)</span>, donde <span class="math inline">\(\psi_0=1\)</span> y <span class="math inline">\(\psi_1=1.35\)</span>. Entonces para <span class="math inline">\(n=453\)</span></p>
<span class="math display">\[\begin{eqnarray*}
  P_{n+1}^n &amp;=&amp; 90.31 \\
  P_{n+2}^n &amp;=&amp; 90.31(1+1.35^2) \\
  P_{n+3}^n &amp;=&amp; 90.31(1+1.35^2+[1.35^2-0.46^2])
\end{eqnarray*}\]</span>
<p>y así sucesivamente.</p>
<p>Note como el pronóstico se nivela rápidamente y los intervalos de predicción son amplios, aún cuando en este caso los límites están basados en un solo error estándar; esto es, <span class="math inline">\(x_{n+m}^n\pm\sqrt{P_{n+m}^n}\)</span>.</p>
</div>

<div class="figure" style="text-align: center"><span id="fig:grafico-pronostico-serie-reclutamiento"></span>
<img src="images/Estimacion-Yule-Walker-serie-reclutamiento.png" alt="24 meses de pronósticos para la serie de reclutamientos (nuevos peces)" width="1310" />
<p class="caption">
Figura 6.3: 24 meses de pronósticos para la serie de reclutamientos (nuevos peces)
</p>
</div>
<hr />
<p>Completaremos está sección con una breve discusión de retroproyección. En retroproyección deseamos predecir <span class="math inline">\(x_{1-m}\)</span>, <span class="math inline">\(m=1,2,\ldots\)</span>, basado en los datos <span class="math inline">\(\{x_1,x_2,\ldots,x_n\}\)</span>.</p>
<p>Escribamos la retroproyección como</p>
<span class="math display" id="eq:eq-retroproyeccion">\[\begin{equation}
  x_{1-m}^n=\sum_{j=1}^{n}\alpha_jx_j
\tag{6.70}
\end{equation}\]</span>
<p>Análogamente a <a href="modelos-arma.html#eq:eq-prediccion-m-pasos">(6.51)</a>, las ecuaciones de predicción (asumiendo <span class="math inline">\(\mu=0\)</span>) son</p>
<span class="math display" id="eq:eq-retroproyeccion-m-pasos-2" id="eq:eq-retroproyeccion-m-pasos-1">\[\begin{eqnarray}
  \sum_{j=1}^{n}\alpha_j\mathbb{E}(x_jx_k) &amp;=&amp; \mathbb{E}(x_{1-m}x_k)\text{, }k=1,\ldots,n\text{ ó } \tag{6.71} \\
  \sum_{j=1}^{n}\alpha_j\gamma(k-j) &amp;=&amp; \gamma(m+k-1)\text{, }k=1,\ldots,n \tag{6.72}
\end{eqnarray}\]</span>
<p>Estas ecuaciones son precisamente las ecuaciones de predicción para predicción a futuro. Estos es, <span class="math inline">\(\alpha_j\equiv\phi_{nj}^{(m)}\)</span> para <span class="math inline">\(j=1,\ldots,n\)</span> donde los <span class="math inline">\(\phi_{nj}^{(m)}\)</span> están dados por <a href="modelos-arma.html#eq:eq-prediccion-m-pasos-matricial">(6.52)</a>. Finalmente las retroproyecciones están dadas por</p>
<span class="math display" id="eq:eq-retroproyeccion-final">\[\begin{equation}
  x_{1-m}^n=\phi_{nj}^{(m)}x_1+\ldots+\phi_{nn}^{(m)}x_n\text{, con }m=1,2,\ldots
\tag{6.73}
\end{equation}\]</span>

<div class="example">
<p><span id="exm:ejem-retroproyeccion-ARMA11" class="example"><strong>Ejemplo 6.15  (Retroproyección de un proceso ARMA(1,1))  </strong></span> Considere un proceso ARMA(1,1) causal e invertible <span class="math inline">\(x_t=\phi x_{t-1}+\theta w_{t-1}+w_t\)</span>, llamaremos a este, modelo hacia adelante. Hemos visto que el mejor predictor lineal hacia atrás en el tiempo es el mismo predictor lineal hacia adelante en el tiempo para procesos estacionarios. Dado que estamos suponiendo que el modelo ARMA es gaussiano, tenemos que el mínimo error cuadrático medio de predicción hacia atrás es el mismo que hacia adelante para modelos ARMA. Entonces, el proceso se puede generar equivalentemente por un modelo hacia atrás <span class="math inline">\(x_t=\phi x_{t+1}+\theta v_{t+1}+v_t\)</span> donde <span class="math inline">\(\{v_t\}\)</span> es un ruido blanco gaussiano con varianza <span class="math inline">\(\sigma_w^2\)</span>. [^nota8]</p>
<p>[^nota8:] En el caso estacionario gaussiano (a) la distribución de <span class="math inline">\(\{x_{n+1},x_n,\ldots,x_1\}\)</span> es la misma que (b) la distribución de <span class="math inline">\(\{x_0,x_1,\ldots,x_n\}\)</span>. En pronóstico usamos (a) para obtener <span class="math inline">\(\mathbb{E}(x_{n+1}|x_n,\ldots,x_1)\)</span>; en retroproyección usamos (b) para obtener <span class="math inline">\(\mathbb{E}(x_0|x_1,\ldots,x_n)\)</span>. Dado que (a) y (b) son iguales, los dos problemas son equivalentes.</p>
<p>Escribiremos <span class="math inline">\(x_t=\sum_{j=0}^{\infty}\psi_jv_{t+j}\)</span> donde <span class="math inline">\(\psi_0=1\)</span>; esto significa que <span class="math inline">\(x_t\)</span> es no-correlacionado con <span class="math inline">\(\{v_{t-1},v_{t-2},\ldots\}\)</span>, en analogía con el modelo a futuro.</p>
<p>Dado los datos <span class="math inline">\(\{x_1,x_2,\ldots,x_n\}\)</span>, truncamos <span class="math inline">\(v_t=\mathbb{E}(v_n|x_1,\ldots,x_n)\)</span> a cero. Esto es, hacemos <span class="math inline">\(\tilde{v}_n^n=0\)</span>, como aproximación inicial, y entonces generamos los errores</p>
<p><span class="math display">\[\tilde{v}_t^n=x_t-\phi x_{t+1}+\theta\tilde{v}_{t+1}^n\text{, con }t=(n-1),(n-2),\ldots,1.\]</span></p>
<p>Entonces</p>
<p><span class="math display">\[\tilde{x}_0^n=\phi x_1+\theta\tilde{v}_1^n+\tilde{v}_0^n=\phi x_1+\theta\tilde{v}_1^n.\]</span></p>
<p>porque <span class="math inline">\(\tilde{v}_t^n=0\)</span> para <span class="math inline">\(t\leq0\)</span>. Continuando, las retroproyecciones truncadas general están dadas por</p>
<p><span class="math display">\[\tilde{x}_{1-m}^n=\phi\tilde{x}_{2-m}^n\text{, para }m=2,3,\ldots\]</span></p>
</div>


<h1 id="estimación-de-parámetros"><span class="header-section-number">6.2</span> Estimación de parámetros</h1>
<h2 id="estimación-1"><span class="header-section-number">6.2</span> Estimación</h2>
<p>A lo largo de esta sección, supongamos que tenemos <span class="math inline">\(n\)</span> observaciones, <span class="math inline">\(x_1,\ldots,x_n\)</span>, a partir de un proceso ARMA(p,q) gaussiano causal e invertible en el que, inicialmente, los parámetros de orden, <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span>, son conocidos. Nuestro objetivo es estimar los parámetros, <span class="math inline">\(\phi_1,\ldots,\phi_p,\theta_1,\ldots,\theta_q\)</span> y <span class="math inline">\(\sigma_w^2\)</span>. Vamos a discutir el problema de determinar <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span> más adelante en esta sección.</p>
<p>Comenzamos con el método de estimación de momentos. La idea detrás de estos estimadores es el de igualar los momentos de la población a los momentos de la muestra y luego resolver para los parámetros en términos de los momentos de la muestra. Inmediatamente vemos que, si <span class="math inline">\(\mathbb{E}(x_t)=\mu\)</span>, entonces estimador de momentos de <span class="math inline">\(\mu\)</span> es el promedio de la muestra <span class="math inline">\(\bar{x}\)</span>. Por lo tanto, mientras se discute el método de momentos, vamos a suponer <span class="math inline">\(\mu=0\)</span>. Aunque el método de momentos puede producir buenos estimadores, a veces puede conducir a estimadores subóptimos. En primer lugar, consideremos el caso en el cual el método conduce a un estimador óptimo (eficiente), esto es, un modelo AR(p).</p>
<p>Cuando el proceso es AR(p), <span class="math display">\[x_t=\phi_1x_{t-1}+\cdots+\phi_px_{t-p}+w_t,\]</span> las primeras <span class="math inline">\(p+1\)</span> ecuaciones de <a href="modelos-arma.html#eq:eq-ACF-ARMA-causal">(6.34)</a> y <a href="modelos-arma.html#eq:eq-condicion-inicial-ACF-ARMA-causal">(6.35)</a> conducen a la siguiente definición:</p>

<div class="definition">
<p><span id="def:defi-ecuacion-yule-walker" class="definition"><strong>Definición 6.5  </strong></span>Las <em>ecuaciones de Yule-Walker</em> están dadas por</p>
<span class="math display" id="eq:eq-yule-walker-sigma" id="eq:eq-yule-walker-gamma">\[\begin{eqnarray}
\gamma(h)  &amp;=&amp; \phi_1\gamma(h-1)+\cdots\phi_p\gamma(h-p),\quad h=1,2,\ldots,p \tag{6.74}\\
\sigma_w^2 &amp;=&amp; \gamma(0)-\phi_1\gamma(1)-\cdots-\phi_p\gamma(p)  \tag{6.75}
\end{eqnarray}\]</span>
</div>

<hr />
<p>En notacion matricial, las ecuaciones de Yule-Walker son:</p>
<span class="math display" id="eq:eq-yule-walker-matricial">\[\begin{equation}
  \Gamma_p\mathbf{\phi}=\mathbf{\gamma}_p, \sigma_w^2=\gamma(0)-\mathbf{\phi}^t\mathbf{\gamma}_p,
\tag{6.76}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\Gamma_p=\{\gamma(k-j)\}_{j,k=1}^p\)</span> es una matriz de orden <span class="math inline">\(p\times p\)</span>, <span class="math inline">\(\mathbf{\phi}=(\phi_1,\ldots,\phi_p)^t\)</span> es un vector <span class="math inline">\(p\times1\)</span> y <span class="math inline">\(\mathbf{\gamma}_p=(\gamma(1),\ldots,\gamma(p))^t\)</span> es un vector <span class="math inline">\(p\times1\)</span>. Usando el método de los momentos, reemplazamos <span class="math inline">\(\gamma(h)\)</span> en <a href="modelos-arma.html#eq:eq-yule-walker-matricial">(6.76)</a> por <span class="math inline">\(\hat{\gamma}(h)\)</span> y resolvemos</p>
<span class="math display" id="eq:eq-estimadores-yule-walker">\[\begin{equation}
  \hat{\mathbf{\phi}} = \hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p, \hat{\sigma}_w^2 = \hat{\gamma}(0)-\hat{\mathbf{\gamma}}_p^t\hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p.
\tag{6.77}
\end{equation}\]</span>
<p>Estos estimadores son llamados <strong>estimadores de Yule-Walker</strong>. Para propósitos de cálculo es a veces más conveniente trabajar con la ACF muestral. Factorizando <span class="math inline">\(\hat{\gamma}(0)\)</span> en <a href="modelos-arma.html#eq:eq-estimadores-yule-walker">(6.77)</a> podemos escribir los estimadores de Yule-Walker como:</p>
<span class="math display" id="eq:eq-estimadores-yule-walker-2">\[\begin{equation}
  \hat{\mathbf{\phi}} = \hat{\mathbf{R}}_p^{-1}\hat{\mathbf{\rho}}_p,  \hat{\sigma}_w^2 = \hat{\gamma}(0)\left[1-\hat{\mathbf{\rho}}_p^t\hat{\mathbf{R}}_p^{-1}\hat{\mathbf{\rho}}_p\right],
\tag{6.78}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\hat{\mathbf{R}}_p=\{\hat{\rho}(k-j)\}_{j,k=1}^p\)</span> es una matriz de orden <span class="math inline">\(p\times p\)</span> y <span class="math inline">\(\hat{\mathbf{\rho}}_p=(\hat{\rho}(1),\ldots,\hat{\rho}_p)^t\)</span> es un vector <span class="math inline">\(p\times1\)</span>.</p>
<p>Para un modelo <span class="math inline">\(AR(p)\)</span>, si el tamaño de la muestra es grande, los estimadores de Yule-Walker tienen distribución aproximadamente normal y <span class="math inline">\(\hat{\sigma}_w^2\)</span> es cercano al valor real de <span class="math inline">\(\sigma_w^2\)</span>. Establecemos este resultado en la proposición <a href="modelos-arma.html#prp:propie-estimadores-yule-walker-muestra-grande">6.7</a>.</p>

<div class="proposition">
<p><span id="prp:propie-estimadores-yule-walker-muestra-grande" class="proposition"><strong>Proposición 6.7  (Resultado de muestras de tamaño grande para los estimadores de Yule-Walker)  </strong></span> El comportamiento asintótico (<span class="math inline">\(n\to\infty\)</span>) de los estimadores de Yule-Walker en el caso de un proceso AR(p) causal es como sigue:</p>
<span class="math display" id="eq:eq-convergencia-estimadores-yule-walker">\[\begin{equation}
  \sqrt{n}(\hat{\mathbf{\phi}}-\mathbf{\phi})\stackrel{d}{\to} N(\mathbf{0},\sigma_w^2\Gamma_p^{-1}),\qquad \hat{\sigma}_w^2\stackrel{p}{\to}\sigma_w^2
\tag{6.79}
\end{equation}\]</span>
</div>

<hr />
<p>El algoritmo de Durbin-Levinson, <a href="modelos-arma.html#eq:eq-phi00-P10">(6.47)</a> a <a href="modelos-arma.html#eq:eq-coeficientes-phi-durbin-levinson">(6.49)</a>, se puede usar para calcular <span class="math inline">\(\hat{\mathbf{\phi}}\)</span> sin invertir <span class="math inline">\(\hat{\Gamma}_p\)</span> o <span class="math inline">\(\hat{\mathbf{R}}_p\)</span>, reemplazando <span class="math inline">\(\gamma(h)\)</span> por <span class="math inline">\(\hat{\gamma}(h)\)</span> en el algoritmo. En la corrida del algoritmo, iterativamente calculamos el <span class="math inline">\(h\times1\)</span> vector, <span class="math inline">\(\hat{\mathbf{\phi}}_h=(\hat{\phi}_{h1},\ldots,\hat{\phi}_{hh})^t\)</span>, para <span class="math inline">\(h=1,2,\ldots\)</span>. Por lo tanto, además de obtener el pronóstico deseado, el algoritmo de Durbin-Levinson nos da <span class="math inline">\(\hat{\phi}_{hh}\)</span>, la PACF muestral. Usando <a href="modelos-arma.html#eq:eq-convergencia-estimadores-yule-walker">(6.79)</a> se puede demostrar la siguiente propiedad.</p>

<div class="proposition">
<span id="prp:propie-distribucion-PACF-muestra-grande" class="proposition"><strong>Proposición 6.8  (Distribución de PACF para muestras grandes)  </strong></span> Para un proceso <span class="math inline">\(AR(p)\)</span> causal, asintóticamente (<span class="math inline">\(n\to\infty\)</span>)
<span class="math display" id="eq:eq-convergencia-PACF-muestral">\[\begin{equation}
  \sqrt{n}\hat{\phi}_{hh}\stackrel{d}{\to}N(0,1),\text{ para } h&gt;p.
\tag{6.80}
\end{equation}\]</span>
</div>

<hr />

<div class="example">
<p><span id="exm:ejem-estimacion-yule-walker-AR2" class="example"><strong>Ejemplo 6.16  (Estimación de Yule-Walker para un proceso AR(2))  </strong></span> Los datos mostrados en la figura <a href="modelos-arma.html#fig:grafico-AR2-simulado">6.1</a> son <span class="math inline">\(n=144\)</span> observaciones simuladas de un modelo AR(2) <span class="math display">\[x_t=1.5x_{t-1}-0.75x_{t-2}+w_t,\]</span> donde <span class="math inline">\(w_t\sim iid N(0,1)\)</span>. Para estos datos, <span class="math inline">\(\hat{\gamma}(0)=8.434, \hat{\rho}(1)=0.834\)</span>, y <span class="math inline">\(\hat{\rho}(2)0=.476\)</span>. En consecuencia,</p>
<p><span class="math display">\[\hat{\mathbf{\phi}} = \left(
                      \begin{array}{c}
                        \hat{\phi}_1 \\
                        \hat{\phi}_2 \\
                      \end{array}
                    \right) = \left[
                                \begin{array}{cc}
                                  1 &amp; 0.834 \\
                                  0.834 &amp; 1 \\
                                \end{array}
                              \right]^{-1}\left(
                                            \begin{array}{c}
                                              0.834 \\
                                              0.476 \\
                                            \end{array}
                                          \right) = \left(
                                                      \begin{array}{c}
                                                        1.439 \\
                                                        -0.725 \\
                                                      \end{array}
                                                    \right)
\]</span></p>
<p>y</p>
<p><span class="math display">\[\hat{\sigma}_w^2 = 8.434\left[1-(0.834,0.476)\left(
                                                 \begin{array}{c}
                                                   1.439 \\
                                                   -0.725 \\
                                                 \end{array}
                                               \right)\right] = 1.215.
\]</span></p>
<p>Por la proposición <a href="modelos-arma.html#prp:propie-estimadores-yule-walker-muestra-grande">6.7</a>, la matriz de varianza-covarianzas asintótica de <span class="math inline">\(\hat{\mathbf{\phi}}\)</span>,</p>
<p><span class="math display">\[\frac{1}{144}\frac{1.215}{8.434}\left[
                                    \begin{array}{cc}
                                      1 &amp; 0.834 \\
                                      0.834 &amp; 1 \\
                                    \end{array}
                                  \right]^{-1} = \left[
                                                   \begin{array}{cc}
                                                     0.057^2 &amp; -0.003 \\
                                                     -0.003 &amp; 0.057^2 \\
                                                   \end{array}
                                                 \right],
\]</span></p>
<p>se puede usar para hallar la región de confianza o hacer inferencias sobre <span class="math inline">\(\hat{\mathbf{\phi}}\)</span> y sus componentes. Por ejemplo, un intervalo de confianza aproximado del 95% para <span class="math inline">\(\phi_2\)</span> es <span class="math inline">\(-0.725\pm2(0.057)\)</span> 0 <span class="math inline">\((-0.839, -0.611)\)</span> el cual contiene el valor real de <span class="math inline">\(\phi_2=-0.75\)</span>.</p>
Para estos datos, las tres primeras correlaciones muestrales fueron <span class="math inline">\(\hat{\phi}_{11}=\hat{\rho}(1)=0.834, \hat{\phi}_{22}=\hat{\phi}_2=-0.725\)</span> y <span class="math inline">\(\hat{\phi}_{33}=-0.075\)</span>. De acuerdo a la Propiedad~, el error estándar asintótico de <span class="math inline">\(\hat{\phi}_{33}\)</span> es <span class="math inline">\(1/\sqrt{144}=0.083\)</span>, y el valor observado es <span class="math inline">\(-0.075\)</span>, que esta a menos de una desviación estándar de <span class="math inline">\(\phi_{33}=0\)</span>.
</div>

<hr />

<div class="example">
<p><span id="exm:ejem-estimacion-yule-walker-serie-reclutamiento" class="example"><strong>Ejemplo 6.17  (Estimación de Yule-Walker para la serie de nuevos peces)  </strong></span> Consideremos nuevamente la serie de nuevos peces y ajustemos un modeloa AR(2) usando la estimación de Yule-Walker. Abajo están los resultados de fijar el modelo usando R.</p>
<table>
<thead>
<tr class="header">
<th>Parámetros</th>
<th align="right">Valores</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Media estimada</td>
<td align="right">62.26278</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\phi_1\)</span> y <span class="math inline">\(\phi_2\)</span> estimados</td>
<td align="right">1.3315874; -0.4445447</td>
</tr>
<tr class="odd">
<td>Errores estándar</td>
<td align="right">0.04222637; 0.04222637</td>
</tr>
<tr class="even">
<td>Error de varianza estimada</td>
<td align="right">94.79912</td>
</tr>
</tbody>
</table>
Las instrucciones R para realizar la estimación de Yule-Walker y generar la figura <a href="modelos-arma.html#fig:grafico-pronostico-serie-reclutamiento-yw">6.4</a> son:
</div>

<hr />
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rec=<span class="kw">scan</span>(<span class="st">&quot;data/recruit.txt&quot;</span>)
rec.yw=<span class="kw">ar.yw</span>(rec, <span class="dt">order=</span><span class="dv">2</span>)
<span class="co"># -----------------------------------------</span>
rec.pr=<span class="kw">predict</span>(rec.yw, <span class="dt">n.ahead=</span><span class="dv">24</span>) 
U=rec.pr<span class="op">$</span>pred<span class="op">+</span>rec.pr<span class="op">$</span>se 
L=rec.pr<span class="op">$</span>pred<span class="op">-</span>rec.pr<span class="op">$</span>se 
meses=<span class="dv">360</span><span class="op">:</span><span class="dv">453</span> 
<span class="kw">plot</span>(meses,rec[meses], <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">360</span>,<span class="dv">480</span>),<span class="dt">ylab=</span><span class="st">&quot;Nuevos peces&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Estimación de Yule-Walker para la serie de nuevos peces&quot;</span>)
<span class="kw">lines</span>(rec.pr<span class="op">$</span>pred, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">type=</span><span class="st">&quot;o&quot;</span>) 
<span class="kw">lines</span>(U, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lty=</span><span class="st">&quot;dashed&quot;</span>)
<span class="kw">lines</span>(L, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lty=</span><span class="st">&quot;dashed&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:grafico-pronostico-serie-reclutamiento-yw"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/grafico-pronostico-serie-reclutamiento-yw-1.svg" alt="Estimación de Yule-Walker para la serie de nuevos peces"  />
<p class="caption">
Figura 6.4: Estimación de Yule-Walker para la serie de nuevos peces
</p>
</div>
<p>En el caso de los modelos AR(p), los estimadores de Yule-Walker dados en <a href="modelos-arma.html#eq:eq-estimadores-yule-walker-2">(6.78)</a> son óptimos en el sentido de que la distribución asintótica, <a href="modelos-arma.html#eq:eq-convergencia-estimadores-yule-walker">(6.79)</a>, es la mejor distribución normal asintótica. Esto se debe a que, dadas las condiciones iniciales, los modelos AR(p) son modelos lineales, y los estimadores de Yule-Walker son esencialmente estimadores de mínimos cuadrados. Si utilizamos el método de momentos para los modelos MA o ARMA, no obtendremos estimadores óptimos debido a que tales procesos no son lineales en los parámetros.</p>

<div class="example">
<p><span id="exm:ejem-estimacion-momentos-MA1" class="example"><strong>Ejemplo 6.18  (Estimación por el Método de los Momentos para un proceso MA(1))  </strong></span> Considere la serie de tiempo</p>
<p><span class="math display">\[x_t=w_t+\theta w_{t-1},\]</span></p>
<p>donde <span class="math inline">\(|\theta|&lt;1\)</span>. El modelo se puede escribir como</p>
<p><span class="math display">\[x_t=\sum_{j=1}^{\infty}(-\theta)^jx_{t-j}+w_t,\]</span></p>
<p>el cual es no lineal en <span class="math inline">\(\theta\)</span>. Las primeras dos autocovarianza poblacionales son <span class="math inline">\(\gamma(0)=\sigma_w^2(1+\theta^2)\)</span> y <span class="math inline">\(\gamma(1)=\sigma_w^2\theta\)</span>, de modo que la estimación de <span class="math inline">\(\theta\)</span> se halla resolviendo</p>
<p><span class="math display">\[\hat{\rho}(1) = \frac{\hat{\gamma}(1)}{\hat{\gamma}(0)} = \frac{\hat{\theta}}{1+\hat{\theta}^2}.\]</span></p>
<p>Existen dos soluciones, por lo que elegimos la invertible. Si <span class="math inline">\(|\hat{\rho}(1)|\leq\frac{1}{2}\)</span>, la solución es real, en cualquier otro caso, no existe solución real. Aún cuando <span class="math inline">\(|\rho(1)|&lt;\frac{1}{2}\)</span> para un modelo MA(1), puede pasar que <span class="math inline">\(|\hat{\rho}(1)|\geq\frac{1}{2}\)</span> porque este es un estimador. Cuando <span class="math inline">\(|\hat{\rho}(1)|&lt;\frac{1}{2}\)</span>, la estimación invertible es</p>
<p><span class="math display">\[\hat{\theta}=\frac{1-\sqrt{1-4\hat{\rho}(1)^2}}{2\hat{\rho}(1)}.\]</span></p>
<p>Se puede demostrar que [^nota10]</p>
<p>[^nota10:] La notación AN se lee  y se define como: Sea <span class="math inline">\(\{x_n\}\)</span> una sucesión de variables aleatorias, se dice que <span class="math inline">\(\{x_n\}\)</span> que es  con media <span class="math inline">\(\mu_n\)</span> y varianza <span class="math inline">\(\sigma_n^2\)</span>, si cuando <span class="math inline">\(n\to\infty\)</span>, <span class="math display">\[\sigma_n^{-1}(x_n-\mu_n)\stackrel{d}{\to}z,\]</span>donde <span class="math inline">\(z\)</span> tiene distribución normal estándar.</p>
<p><span class="math display">\[\hat{\theta} \sim AN\left(\theta,\frac{1+\theta^2+4\theta^4+\theta^6+\theta^8}{n(1-\theta^2)^2}\right).\]</span></p>
El estimador de máxima verosimilitud (que discutiremos en la próxima sección) de <span class="math inline">\(\theta\)</span>, en este caso, tiene una varianza asintótica de <span class="math inline">\((1-\theta^2)/n\)</span>. Cuando <span class="math inline">\(\theta=0.5\)</span>, por ejemplo, la relación de la varianza asintótica del estimador por el método de los momentos y el estimador por el método de máxima verosimilitud de <span class="math inline">\(\theta\)</span> es alrededor de 3.5. Esto es, para muestras grandes, la varianza del estimador por el método de los momentos es alrededor de 3.5 veces mayor que la varianza del estimador por el EMV de <span class="math inline">\(\theta\)</span> cuando <span class="math inline">\(\theta=0.5\)</span>.
</div>

<h2 id="sect-EMV"><span class="header-section-number">6.2</span> Estimación por Máxima Verosimilitud y Mínimos Cuadrados</h2>
<p>Para fijar ideas, primero enfoquemos en un modelo causal AR(1). Sea</p>
<p><span class="math display">\[x_t=\mu+\phi(x_{t-1}-\mu)+w_t,\]</span></p>
<p>donde <span class="math inline">\(|\phi|&lt;1\)</span> y <span class="math inline">\(w_t\sim\text{iid}N(0,\sigma_w^2)\)</span>. Dado los datos <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> buscamos la función de verosimilitud</p>
<p><span class="math display">\[L(\mu,\phi,\sigma_w^2)=f_{\mu,\phi,\sigma_w^2}(x_1,x_2,\ldots,x_n).\]</span></p>
<p>En el caso de un modelo AR(1), podemos escribir la función de verosimilitud como</p>
<p><span class="math display">\[L(\mu,\phi,\sigma_w^2)=f(x_1)f(x_2|x_1)\cdots f(x_n|x_{n-1}),\]</span></p>
<p>donde hemos eliminado los parámetros en las densidades <span class="math inline">\(f(\cdot)\)</span> para facilitar la notación.</p>
<p>Dado que <span class="math inline">\(x_t|x_{t-1}\sim N(\mu+\phi(x_{t-1}-\mu,\sigma_w^2)\)</span> tenemos</p>
<p><span class="math display">\[f(x_t|x_{t-1})=f_w[(x_t-\mu)-\phi(x_{t-1}-\mu)],\]</span></p>
<p>donde <span class="math inline">\(f_w(\cdot)\)</span> es la densidad de <span class="math inline">\(w_t\)</span>, esto es, la densidad normal con media cero y varianza <span class="math inline">\(\sigma_w^2\)</span>. Podemos escribir la función de verosimilitud como</p>
<p><span class="math display">\[L(\mu,\phi,\sigma_w^2)=f(x_1)\prod_{t=2}^{n}f_w[(x_t-\mu)-\phi(x_{t-1}-\mu)].\]</span></p>
<p>Para hallar <span class="math inline">\(f(x_1)\)</span> podemos usar la representación causal</p>
<p><span class="math display">\[x_1=\mu+\sum_{j=0}^{\infty}\phi^jw_{1-j},\]</span></p>
<p>para ver que <span class="math inline">\(x_1\)</span> es normal con media <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\sigma_w^2/(1-\phi^2)\)</span>.</p>
<p>Finalmente, para un AR(1), la verosimilitud es</p>
<span class="math display" id="eq:eq-funcion-verosimilitud-AR1">\[\begin{equation}
  L(\mu,\phi,\sigma_w^2)=(2\pi\sigma_w^2)^{-n/2}(1-\phi^2)^{1/2}\exp\left[-\frac{S(\mu,\phi)}{2\sigma_w^2}\right]
\tag{6.81}
\end{equation}\]</span>
<p>donde</p>
<span class="math display" id="eq:eq-S-AR1">\[\begin{equation}
  S(\mu,\phi)=(1-\phi^2)(x_1-\mu)^2+\sum_{t=2}^{n}[(x_t-\mu)-\phi(x_{t-1}-\mu)]^2.
\tag{6.82}
\end{equation}\]</span>
<p>Normalmente, <span class="math inline">\(S(\mu,\phi)\)</span> se llama <em>suma de cuadrados incondicional</em>. Podemos también considerar la estimación de <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\phi\)</span> usando la suma de cuadrados incondicional, esto es, minimizando <span class="math inline">\(S(\mu,\phi)\)</span>.</p>
<p>Tomando la derivada parcial del logaritmo de <a href="modelos-arma.html#eq:eq-funcion-verosimilitud-AR1">(6.81)</a> con respecto a <span class="math inline">\(\sigma_w^2\)</span> e igualando a cero, que para cada valor de <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\phi\)</span> en el espacio de parámetros, <span class="math inline">\(\sigma_w^2=n^{-1}S(\mu,\phi)\)</span> maximiza la verosimilitud. Por consiguiente, el estimador de máxima verosimilitud de <span class="math inline">\(\sigma_w^2\)</span> es</p>
<span class="math display" id="eq:eq-estimador-EMV-sigma-AR1">\[\begin{equation}
  \hat{\sigma}_w^2=n^{-1}S(\hat{\mu},\hat{\phi})
\tag{6.83}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\hat{\mu}\)</span> y <span class="math inline">\(\hat{\phi}\)</span> son los estimadores de máxima verosimilitud de <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\phi\)</span> respectivamente.</p>
<p>Si reemplazamos <span class="math inline">\(n\)</span> en <a href="modelos-arma.html#eq:eq-estimador-EMV-sigma-AR1">(6.83)</a> por <span class="math inline">\(n-2\)</span> podemos obtener el estimador de mínimo cuadrado incondicional de <span class="math inline">\(\sigma_w^2\)</span>.</p>
<p>Si en <a href="modelos-arma.html#eq:eq-funcion-verosimilitud-AR1">(6.81)</a> tomamos logaritmo, reemplazamos <span class="math inline">\(\sigma_w^2\)</span> por <span class="math inline">\(\hat{\sigma}_w^2\)</span>, e ignoramos las constantes, <span class="math inline">\(\hat{\mu}\)</span> y <span class="math inline">\(\hat{\phi}\)</span> son los valores que minimizan la función de criterio</p>
<span class="math display" id="eq:eq-funcion-criterio-AR1">\[\begin{equation}
  l(\mu,\phi)=\ln[n^{-1}S(\mu,\phi)]-n^{-1}\ln(1-\phi^2).
\tag{6.84}
\end{equation}\]</span>
<p>Esto es, <span class="math inline">\(l(\mu,\phi)\propto-2\ln L(\mu,\phi,\hat{\sigma}_w^2)\)</span>. <a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>[^nota11:] La función de criterio a veces es llamada perfil de verosimilitud.</p>
<p>Dado que <a href="modelos-arma.html#eq:eq-S-AR1">(6.82)</a> o <a href="modelos-arma.html#eq:eq-funcion-criterio-AR1">(6.84)</a> son funciones complicadas de los parámetros, la minimización de <span class="math inline">\(l(\mu,\phi)\)</span> o <span class="math inline">\(S(\mu,\phi)\)</span> se hace numéricamente. En el caso de modelos AR, tenemos la ventaja que, condicionando los valores inicial, ellos son modelos lineales. Esto es, podemos eliminar el término en la verosimilitud que causa la no-linealidad.</p>
<p>Condicionando sobre <span class="math inline">\(x_1\)</span> la verosimilitud condicional llega a ser</p>
<span class="math display" id="eq:eq-verosimilitud-condicional-AR1">\[\begin{eqnarray}
  L(\mu,\phi,\sigma_w^2|x_1) &amp;=&amp; \prod_{t=2}^{n}f_w[(x_t-\mu)-\phi(x_{t-1}-\mu)] \nonumber \\
        &amp;=&amp; (2\pi\sigma_w^2)^{-(n-1)/2}\exp\left[-\frac{S_c(\mu,\phi)}{2\sigma_w^2}\right] \tag{6.85}
\end{eqnarray}\]</span>
<p>donde la suma de cuadrados condicional es</p>
<span class="math display" id="eq:eq-suma-cuadrado-condicional-AR1">\[\begin{equation}
  S_c(\mu,\phi)=\sum_{t=2}^{n}[(x_t-\mu)-\phi(x_{t-1}-\mu)]^2.
\tag{6.86}
\end{equation}\]</span>
<p>El estimador de máxima verosimilitud condicional de <span class="math inline">\(\sigma_w^2\)</span> es</p>
<span class="math display" id="eq:eq-EMV-condicional-sigma-AR1">\[\begin{equation}
  \hat{\sigma}_w^2=S_c(\hat{\mu},\hat{\phi})/(n-1)
\tag{6.87}
\end{equation}\]</span>
<p>y <span class="math inline">\(\hat{\mu}\)</span> y <span class="math inline">\(\hat{\phi}\)</span> son los valores que minimizan la suma de cuadrados condicional <span class="math inline">\(S_c(\mu,\phi)\)</span>.</p>
<p>Haciendo <span class="math inline">\(\alpha=\mu(1-\phi)\)</span> la suma de cuadrados condicional se puede escribir como</p>
<span class="math display" id="eq:eq-suma-cuadrado-condicional-AR1-2">\[\begin{equation}
  S_c(\mu,\phi)=\sum_{t=2}^{n}[x_t-(\alpha+\phi x_{t-1})]^2.
\tag{6.88}
\end{equation}\]</span>
<p>El problema ahora es un problema de regresión lineal visto en el Tema 2. Siguiendo los resultados de la estimación de mínimos cuadrados, tenemos <span class="math inline">\(\hat{\alpha}=\bar{x}_{(2)}-\hat{\phi}\bar{x}_{(1)}\)</span> donde <span class="math inline">\(\bar{x}_{(1)}=(n-1)^{-1}\sum_{t=1}^{n-1}x_t\)</span> y <span class="math inline">\(\bar{x}_{(2)}=(n-1)^{-1}\sum_{t=2}^{n}x_t\)</span> y los estimados condicionales son entonces</p>
<span class="math display" id="eq:eq-EMV-condicional-phi" id="eq:eq-EMV-condicional-mu">\[\begin{eqnarray}
  \hat{\mu} &amp;=&amp; \frac{\bar{x}_{(2)}-\hat{\phi}\bar{x}_{(1)}}{1-\hat{\phi}} \tag{6.89} \\
    \hat{\phi} &amp;=&amp; \frac{\sum_{t=2}^{n}(x_t-\bar{x}_{(2)})(x_{t-1}-\bar{x}_{(1)})}{\sum_{t=2}^{n}(x_{t-1}-\bar{x}_{(1)})^2}. \tag{6.90}
\end{eqnarray}\]</span>
<p>De <a href="modelos-arma.html#eq:eq-EMV-condicional-mu">(6.89)</a> y <a href="modelos-arma.html#eq:eq-EMV-condicional-phi">(6.90)</a> vemos que <span class="math inline">\(\hat{\mu}\approx\bar{x}\)</span> y <span class="math inline">\(\hat{\phi}\approx\hat{\rho}(1)\)</span>. Estos es, los estimadores de Yule-Walker y los estimadores de mínimos cuadrados son aproximadamente los mismos. La única diferencia es la inclusión o exclusión de los términos que envuelven los puntos finales <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_n\)</span>. Podemos también ajustar el estimado de <span class="math inline">\(\sigma_w^2\)</span> en <a href="modelos-arma.html#eq:eq-EMV-condicional-sigma-AR1">(6.87)</a> para que sea equivalente al estimador de mínimos cuadrados, esto es, dividimos <span class="math inline">\(S_c(\hat{\mu},\hat{\phi})\)</span> por <span class="math inline">\((n-3)\)</span> en vez de <span class="math inline">\((n-1)\)</span> en <a href="modelos-arma.html#eq:eq-EMV-condicional-sigma-AR1">(6.87)</a>.</p>
<p>Para un modelo general AR(p) los estimadores máxima verosimilitud, mínimos cuadrados incondicionales y mínimos cuadrados condicionales se obtienen de manera análoga al ejemplo de AR(1).</p>
<p>Para modelos ARMA en general, es difícil escribir la función de verosimilitud como una función explícita de los parámetros. En vez de eso, es conveniente escribir la verosimilitud en término de las innovaciones o errores de predicción de un paso, <span class="math inline">\(x_t-x_t^{t-1}\)</span>.</p>
<p>Supóngase que <span class="math inline">\(x_t\)</span> es un proceso ARMA(p,q) causal con <span class="math inline">\(w_t\sim\text{idd}N(0,\sigma_w^2)\)</span>.</p>
<p>Sea <span class="math inline">\(\pmb{\beta}=(\mu,\phi_1\ldots,\phi_p,\theta_1,\ldots,\theta_q)^t\)</span> un vector de orden <span class="math inline">\((p+q+1)\times1\)</span> de los parámetros del modelo. La función de verosimilitud se puede escribir como</p>
<p><span class="math display">\[L(\pmb{\beta},\sigma_w^2)=\prod_{t=1}^{n}f(x_t|x_{t-1},\ldots,x_1)\]</span></p>
<p>La distribución condicional de <span class="math inline">\(x_t\)</span> dados <span class="math inline">\(x_{t-1},\ldots,x_1\)</span> es gaussiana con media <span class="math inline">\(x_t^{t-1}\)</span> y varianza <span class="math inline">\(P_t^{t-1}\)</span>. Además, para modelos ARMA, podemos escribir <span class="math inline">\(P_t^{t-1}=\sigma_w^2r_t^{t-1}\)</span> donde <span class="math inline">\(r_t^{t-1}\)</span> no depende de <span class="math inline">\(\sigma_w^2\)</span>.</p>
<p>La función de verosimilitud de la muestra se puede escribir entonces como</p>
<span class="math display" id="eq:eq-funcion-verosimilitud-datos">\[\begin{equation}
  L(\mathbf{\beta},\sigma_w^2)=(2\pi\sigma_w^2)^{-n/2}\left[r_1^0(\mathbf{\beta})r_2^1(\mathbf{\beta})\cdot sr_n^{n-1}(\mathbf{\beta})\right]^{1/2}\exp\left[-\frac{S(\mathbf{\beta})}{2\sigma_w^2}\right]
\tag{6.91}
\end{equation}\]</span>
<p>donde</p>
<span class="math display" id="eq:eq-S-beta">\[\begin{equation}
  S(\mathbf{\beta})=\sum_{t=1}^{n}\left[\frac{(x_t-x_t^{t-1}(\mathbf{\beta}))^2}{r_t^{t-1}(\mathbf{\beta})}\right].
\tag{6.92}
\end{equation}\]</span>
<p>Se tiene que <span class="math inline">\(x_t^{t-1}\)</span> y <span class="math inline">\(r_t^{t-1}\)</span> son funciones de <span class="math inline">\(\mathbf{\beta}\)</span> y hacemos este hecho explícito en <a href="modelos-arma.html#eq:eq-funcion-verosimilitud-datos">(6.91)</a> y <a href="modelos-arma.html#eq:eq-S-beta">(6.92)</a>.</p>
<p>Dados los valores para <span class="math inline">\(\mathbf{\beta}\)</span> y <span class="math inline">\(\sigma_w^2\)</span>, la verosimilitud se puede evaluar usando las técnicas vistas para Pronósticos. La estimación de máxima verosimilitud ahora procederá maximizando @ref)eq:eq-funcion-verosimilitud-datos) con respecto a <span class="math inline">\(\mathbf{\beta}\)</span> y <span class="math inline">\(\sigma_w^2\)</span>. Tenemos entonces</p>
<span class="math display" id="eq:eq-sigma-estimado-EMV">\[\begin{equation}
  \hat{\sigma}_w^2=n^{-1}S(\hat{\mathbf{\beta}}),
\tag{6.93}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\hat{\mathbf{\beta}}\)</span> es el valor de <span class="math inline">\(\mathbf{\beta}\)</span> que minimiza la función de criterio</p>
<span class="math display" id="eq:eq-funcion-criterio-EMV">\[\begin{equation}
  l(\mathbf{\beta})=\ln[n^{-1}S(\mathbf{\beta})]+n^{-1}\sum_{t=1}^{n}\ln r_t^{t-1}(\mathbf{\beta}).
\tag{6.94}
\end{equation}\]</span>
<p>Por ejemplo, para el modelo AR(1) discutido arriba, la función genérica <span class="math inline">\(l(\mathbf{\beta})\)</span> en <a href="modelos-arma.html#eq:eq-funcion-criterio-EMV">(6.94)</a> es <span class="math inline">\(l(\mu,\phi)\)</span> en <a href="modelos-arma.html#eq:eq-funcion-criterio-AR1">(6.84)</a> y la general <span class="math inline">\(S(\mathbf{\beta})\)</span> en <a href="modelos-arma.html#eq:eq-S-beta">(6.92)</a> es <span class="math inline">\(S(\mu,\phi)\)</span> dado en <a href="modelos-arma.html#eq:eq-S-AR1">(6.82)</a>.</p>
<p>De <a href="modelos-arma.html#eq:eq-S-AR1">(6.82)</a> y <a href="modelos-arma.html#eq:eq-funcion-criterio-AR1">(6.84)</a> se ve que <span class="math inline">\(x_1^0=\mu\)</span> y <span class="math inline">\(x_t^{t-1}=\mu+\phi(x_{t-1}-\mu)\)</span> para <span class="math inline">\(t=2,\ldots,n\)</span>. También <span class="math inline">\(r_1^0=1/(1-\phi^2)\)</span> y <span class="math inline">\(r_t^{t-1}=1\)</span> para <span class="math inline">\(t=2,\ldots,n\)</span>.</p>
<p>Los mínimos cuadrados incondicional se desarrollarán minimizando <a href="modelos-arma.html#eq:eq-S-beta">(6.92)</a> con respecto a <span class="math inline">\(\mathbf{\beta}\)</span>. La estimación de mínimos cuadrados condicional envuelve minimizar <a href="modelos-arma.html#eq:eq-S-beta">(6.92)</a> con respecto a <span class="math inline">\(\mathbf{\beta}\)</span> pero donde, para facilitar la carga computacional, las predicciones y sus errores se obtienen por condicionamiento sobre los valores iniciales de las observaciones. En general, se usan las rutinas numéricas de optimización para obtener las estimaciones y sus errores estándar.</p>

<div class="example">
<p><span id="exm:ejem-algoritmo-newton-raphson" class="example"><strong>Ejemplo 6.19  (Algoritmos de Newton-Raphson y puntuación)  </strong></span> Dos rutinas numéricas de optimización comunes para la estimación de máxima verosimilitud son el Newton-Raphson y el de puntuación. Daremos una breve descripción de las ideas matemáticas. La implementación de estos algoritmos es más complicada de lo que discutiremos en este ejemplo.</p>
<p>Sea <span class="math inline">\(l(\mathbf{\beta})\)</span> una función de criterio de <span class="math inline">\(k\)</span> parámetros <span class="math inline">\(\mathbf{\beta}=(\beta_1,\ldots,\beta_k)\)</span> la cual deseamos minimizar respecto a <span class="math inline">\(\mathbf{\beta}\)</span>. Por ejemplo, considere la función de verosimilitud dada por <a href="modelos-arma.html#eq:eq-funcion-criterio-AR1">(6.84)</a> o <a href="modelos-arma.html#eq:eq-funcion-criterio-EMV">(6.94)</a>. Suponga que <span class="math inline">\(l(\hat{\mathbf{\beta}})\)</span> es el extremo que estamos interesados en hallar, y <span class="math inline">\(\hat{\mathbf{\beta}}\)</span> se halla resolviendo <span class="math inline">\(\partial l(\mathbf{\beta})/\partial\beta_j=0\)</span> para <span class="math inline">\(j=1,\ldots,k\)</span>. Denotemos por <span class="math inline">\(l^{(1)}(\mathbf{\beta})\)</span> el vector <span class="math inline">\(k\times1\)</span> de derivadas parciales</p>
<p><span class="math display">\[l^{(1)}(\mathbf{\beta})=\left(\frac{\partial l(\mathbf{\beta})}{\partial\beta_1},\cdots,\frac{\partial l(\mathbf{\beta})}{\partial\beta_k}\right)^t\]</span></p>
<p>Note que <span class="math inline">\(l^{(1)}(\mathbf{\hat{\beta}})=\textbf{0}\)</span>.</p>
<p>Sea <span class="math inline">\(l^{(2)}(\mathbf{\beta})\)</span> una matriz <span class="math inline">\(k\times k\)</span> de las segundas derivadas parciales</p>
<p><span class="math display">\[l^{(2)}(\mathbf{\beta})=\left\{-\frac{\partial l^2(\mathbf{\beta})}{\partial\beta_i\partial\beta_j}\right\}_{i,j=1}^{k}\]</span></p>
<p>y supongamos que <span class="math inline">\(l^{(2)}(\mathbf{\beta})\)</span> es no singular. Sea <span class="math inline">\(\mathbf{\beta}_{(0)}\)</span> un estimador inicial de <span class="math inline">\(\mathbf{\beta}\)</span>. Entonces, usando el desarrollo de Taylor, tenemos la siguiente aproximación:</p>
<p><span class="math display">\[\textbf{0}=l^{(1)}(\mathbf{\hat{\beta}})\approx l^{(1)}(\mathbf{\beta}_{(0)})-l^{(2)}(\mathbf{\beta}_{(0)})\left[\mathbf{\hat{\beta}}-\mathbf{\beta}_0\right]\]</span></p>
<p>Haciendo el lado derecho cero y resolviendo para <span class="math inline">\(\mathbf{\hat{\beta}}\)</span> se tiene</p>
<p><span class="math display">\[\mathbf{\beta}_{(1)}=\mathbf{\beta}_{(0)}+\left[l^{(2)(\mathbf{\beta}_{(0)}})\right]^{-1}l^{(1)}(\mathbf{\beta}_{(0)})\]</span></p>
<p>El algoritmo de Newton-Raphson procede iterando este resultado, reemplazando <span class="math inline">\(\mathbf{\beta}_{(0)}\)</span> por <span class="math inline">\(\mathbf{\beta}_{(1)}\)</span> para obtener <span class="math inline">\(\mathbf{\beta}_{(2)}\)</span> y así sucesivamente, hasta que converja. Bajo un conjunto apropiado de condiciones, la sucesión de estimadores <span class="math inline">\(\mathbf{\beta}_{(1)},\mathbf{\beta}_{(2)},\ldots\)</span>, convergerá a <span class="math inline">\(\mathbf{\hat{\beta}}\)</span> el estimador de máxima verosimilitud para <span class="math inline">\(\mathbf{\beta}\)</span>.</p>
<p>Para la estimación de máxima verosimilitud, la función de criterio usada es <span class="math inline">\(l(\mathbf{\beta})\)</span> dada por (<a href="modelos-arma.html#eq:eq-funcion-criterio-EMV">(6.94)</a>; <span class="math inline">\(l^{(1)}(\mathbf{\beta})\)</span> es llamado el <strong>vector de puntuación</strong> y <span class="math inline">\(l^{(2)}(\mathbf{\beta})\)</span> es llamado el <strong>Hessiano</strong>. En el algoritmo de puntuaciones, reemplazamos <span class="math inline">\(l^{(2)}(\mathbf{\beta})\)</span> por <span class="math inline">\(\mathbb{E}[l^{(2)}(\mathbf{\beta})]\)</span>, la matriz de información. Bajo condiciones apropiadas, la inversa de la matriz de información es la matriz de varianza-covarianza asintótica del estimador <span class="math inline">\(\mathbf{\hat{\beta}}\)</span>. Esta es a veces aproximada por la inversa del Hessiano en <span class="math inline">\(\mathbf{\hat{\beta}}\)</span>.</p>
Si las derivadas son difíciles de obtener, es posible usar la estimación de verosimilitud cuasi-máxima donde se usan las técnicas numéricas para aproximar las derivadas.
</div>

<hr />

<div class="example">
<span id="exm:ejem-EMV-serie-reclutamiento" class="example"><strong>Ejemplo 6.20  (EMV para la serie de nuevos peces)  </strong></span> En el ejemplo <a href="modelos-arma.html#exm:ejem-estimacion-yule-walker-serie-reclutamiento">6.17</a> fijamos un modelo AR(2) para la serie de nuevos peces usando las ecuaciones de Yule-Walker. El siguiente comando en R fija el modelo AR(2) via máxima verosimilitud. Pueden comparar estos resultados con los obtenidos en el ejemplo <a href="modelos-arma.html#exm:ejem-estimacion-yule-walker-serie-reclutamiento">6.17</a>.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rec.mle=<span class="kw">ar.mle</span>(rec,<span class="dt">order=</span><span class="dv">2</span>)
rec.mle</code></pre></div>
<pre><code>## 
## Call:
## ar.mle(x = rec, order.max = 2)
## 
## Coefficients:
##      1       2  
##  1.351  -0.461  
## 
## Order selected 2  sigma^2 estimated as  89.3</code></pre>
<h2 id="estimación-de-mínimos-cuadrados-para-modelos-armapq"><span class="header-section-number">6.2</span> Estimación de mínimos cuadrados para modelos ARMA(p,q)</h2>
<p>Ahora discutiremos la estimación de mínimos cuadrados para modelos ARMA(p,q) via Gauss-Newton. Sea <span class="math inline">\(x_t\)</span> un proceso ARMA(p,q) gaussiano causal e invertible. Escribimos <span class="math inline">\(\mathbf{\beta}=(\phi_1,\ldots,\phi_p\)</span>, <span class="math inline">\(\theta_1,\ldots,\theta_q)^t\)</span>, para simplificación de la discusión, hacemos <span class="math inline">\(\mu=0\)</span>. Escribimos el modelo en términos de los errores</p>
<span class="math display" id="eq:eq-modelo-ARMA-pq-EMV">\[\begin{equation}
  w_t(\mathbf{\beta})=x_t-\sum_{j=1}^{p}\phi_jx_{t-j}-\sum_{k=1}^{q}\theta_kw_{t-k}(\mathbf{\beta})
\tag{6.95}
\end{equation}\]</span>
<p>para enfatizar la dependencia de los errores sobre los parámetros.</p>
<p>Para mínimos cuadrados condicional, aproximamos la suma residual de cuadrados condicionando por <span class="math inline">\(x_1,\ldots,x_p (p&gt;0)\)</span> y <span class="math inline">\(w_p=w_{p-1}=\cdots=w_{1-q}=0 (q&gt;0)\)</span>, en cuyo caso podemos evaluar <a href="modelos-arma.html#eq:eq-modelo-ARMA-pq-EMV">(6.95)</a> para <span class="math inline">\(t=p+1,p+2,\ldots,n\)</span>. Usando estos argumentos condicionales, el error de suma de cuadrados condicional es</p>
<p><span class="math display">\[S_c(\mathbf{\beta})=\sum_{t=p+1}^{n}w_t^2(\mathbf{\beta})\]</span></p>
<p>Minimizando <span class="math inline">\(S_c(\mathbf{\beta})\)</span> con respecto a <span class="math inline">\(\mathbf{\beta}\)</span> obtenemos los estimados de mínimos cuadrados condicional.</p>
<p>Si <span class="math inline">\(q=0\)</span>, el problema es una regresión lineal, y no se necesitan técnicas iterativas para minimizar <span class="math inline">\(S_c(\phi_1,\ldots,\phi_p)\)</span>. Si <span class="math inline">\(q&gt;0\)</span> el problema es de regresión no-lineal y tenemos que acudir a optimización numérica.</p>
<p>Cuando <span class="math inline">\(n\)</span> es grande, condicionando sobre unos pocos valores iniciales tendremos poca influencia sobre los estimados finales de los parámetros. En el caso de muestras de tamaño pequeño a moderado, podemos usar mínimos cuadrados incondicionales. El problema de mínimos cuadrados incondicional es elegir <span class="math inline">\(\mathbf{\beta}\)</span> para minimizar la suma de cuadrados incondicional, la cual denotamos por <span class="math inline">\(S(\mathbf{\beta})\)</span>.</p>
<p>La suma de cuadrados incondicional se puede escribir de varias maneras. Una de las maneras es la siguiente forma <a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
<p>[^nota12:] Para detalles, véase Box, G.E.P., Jenkins, G.M. and Reinsel, G.C. (1994). <em>Time Series Analysis, Forecasting and Control, 3rd ed.</em> Englewood Cliffs, NJ: Prentice Hall. Apéndice A7.3.</p>
<p><span class="math display">\[S(\mathbf{\beta})=\sum_{t=-\infty}^{n}\hat{w}_t^2(\mathbf{\beta})\]</span></p>
<p>donde <span class="math inline">\(\hat{w}_t^2(\mathbf{\beta})=\mathbb{E}(w_t|x_1,\ldots,x_n)\)</span>. Cuando <span class="math inline">\(t\leq0\)</span> los <span class="math inline">\(\hat{w}_t(\mathbf{\beta})\)</span> se obtienen por retroproyección. Como una forma práctica, aproximamos <span class="math inline">\(S(\mathbf{\beta})\)</span> por medio de iniciar la suma en <span class="math inline">\(t=-M+1\)</span> donde <span class="math inline">\(M\)</span> se elige suficientemente grande para garantizar que <span class="math inline">\(\sum_{t=-\infty}^{-M}\hat{w}_t^2(\mathbf{\beta})\approx0\)</span>. En el caso de estimación por mínimos cuadrados incondicional, son necesarias las técnicas de optimización numéricas aún cuando <span class="math inline">\(q=0\)</span>.</p>
<p>Para emplear Gauss-Newton, sea <span class="math inline">\(\mathbf{\beta}_{(0)}=(\phi_1^{(0)},\ldots,\phi_p^{(0)},\theta_1^{(0)},\ldots,\theta_q^{(0)})^t\)</span> un estimado inicial de <span class="math inline">\(\mathbf{\beta}\)</span>. Por ejemplo, podemos obtener <span class="math inline">\(\mathbf{\beta}_{(0)}\)</span> por el método de los momentos. El desarrollo de Taylor de primer orden de <span class="math inline">\(w_t(\mathbf{\beta})\)</span> es</p>
<span class="math display" id="eq:eq-desarrollo-Taylor-1-wt">\[\begin{equation}
  w_t(\mathbf{\beta}) \approx w_t(\mathbf{\beta}_{(0)})-\left(\mathbf{\beta}-\mathbf{\beta}_{(0)}\right)^t z_t(\mathbf{\beta}_{(0)})
\tag{6.96}
\end{equation}\]</span>
<p>donde</p>
<p><span class="math display">\[z_t(\mathbf{\beta}_{(0)})=\left(-\frac{\partial w_t(\mathbf{\beta}_{(0)})}{\partial\beta_1},\cdots,-\frac{\partial w_t(\mathbf{\beta}_{(0)})}{\partial\beta_{p+q}}\right)^t\text{, }t=1,\ldots,n\]</span></p>
<p>La aproximación lineal de <span class="math inline">\(S_c(\mathbf{\beta})\)</span> s</p>
<span class="math display" id="eq:eq-aprox-lineal-S-beta">\[\begin{equation}
  Q(\mathbf{\beta})=\sum_{t=p+1}^{n}\left[w_t(\mathbf{\beta}_{(0)})-\left(\mathbf{\beta}-\mathbf{\beta}_{(0)}\right)^tz_t(\mathbf{\beta}_{(0)})\right]^2
\tag{6.97}
\end{equation}\]</span>
<p>y esta es la cantidad que queremos minimizar. Para aproximar los mínimos cuadrados incondicional, iniciaremos la suma en <a href="modelos-arma.html#eq:eq-aprox-lineal-S-beta">(6.97)</a> en <span class="math inline">\(t=-M+1\)</span> para <span class="math inline">\(M\)</span> grande, y trabajamos con los valores de retroproyección.</p>
<p>Usando los resultados de mínimos cuadrados ordinarios, sabemos que</p>
<span class="math display" id="eq:eq-beta-estimado-mc">\[\begin{equation}
  (\widehat{\mathbf{\beta}-\mathbf{\beta}}_{(0)})=\left(n^{-1}\sum_{t=p+1}^{n}z_t(\mathbf{\beta}_{(0)})z_t^t(\mathbf{\beta}_{(0)})\right)^{-1}
\left(n^{-1}\sum_{t=p+1}^{n}z_t(\mathbf{\beta}_{(0)})w_t(\mathbf{\beta}_{(0)})\right)
\tag{6.98}
\end{equation}\]</span>
<p>minimiza <span class="math inline">\(Q(\mathbf{\beta})\)</span>. De <a href="modelos-arma.html#eq:eq-beta-estimado-mc">(6.98)</a> podemos escribir el estimado Gauss-Newton de un paso como</p>
<span class="math display" id="eq:eq-estimador-gauss-newton-1">\[\begin{equation}
  \mathbf{\beta}_{(1)}=\mathbf{\beta}_{(0)}+\Delta(\mathbf{\beta}_{(0)})
\tag{6.99}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\Delta(\mathbf{\beta}_{(0)})\)</span> denota el lado derecho de <a href="modelos-arma.html#eq:eq-beta-estimado-mc">(6.98)</a>. La estimación Gauss-Newton se logra reemplazando <span class="math inline">\(\mathbf{\beta}_{(0)}\)</span> por <span class="math inline">\(\mathbf{\beta}_{(1)}\)</span> en <a href="modelos-arma.html#eq:eq-estimador-gauss-newton-1">(6.99)</a>. Este procedimiento se repite, iterando para <span class="math inline">\(j=2,3,\ldots\)</span>, para calcular</p>
<p><span class="math display">\[\mathbf{\beta}_{(j)}=\mathbf{\beta}_{(j-1)}+\Delta(\mathbf{\beta}_{(j-1)})\]</span></p>
<p>hasta converger.</p>

<div class="example">
<p><span id="exm:ejem-gauss-newton-MA1" class="example"><strong>Ejemplo 6.21  (Gauss-Newton para un MA(1))  </strong></span> Considere un proceso MA(1) invertible, <span class="math inline">\(x_t=w_t+\theta w_{t-1}\)</span>. Escribimos el error truncado como</p>
<span class="math display" id="eq:eq-error-truncado-MA1">\[\begin{equation}
  w_t(\theta)=x_t-\theta w_{t-1}(\theta)\text{, }t=1,\ldots,n
\tag{6.100}
\end{equation}\]</span>
<p>donde condicionamos <span class="math inline">\(w_0(\theta)=0\)</span>. Derivando respecto de <span class="math inline">\(\theta\)</span></p>
<span class="math display" id="eq:eq-derivada-error-truncado-MA1">\[\begin{equation}
  -\frac{\partial w_t(\theta)}{\partial\theta}=w_{t-1}(\theta)+\theta\frac{\partial w_{t-1}(\theta)}{\partial\theta}\text{, }t=1,\ldots,n
\tag{6.101}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\partial w_0(\theta)/\partial\theta=0\)</span>. Usando la notación de <a href="modelos-arma.html#eq:eq-desarrollo-Taylor-1-wt">(6.96)</a> podemos escribir <a href="modelos-arma.html#eq:eq-derivada-error-truncado-MA1">(6.101)</a> como</p>
<span class="math display" id="eq:eq-derivada-error-truncado-MA1-2">\[\begin{equation}
  z_t(\theta)=w_{t-1}(\theta)-\theta z_{t-1}(\theta)\text{, }t=1,\ldots,n
\tag{6.102}
\end{equation}\]</span>
<p>donde <span class="math inline">\(z_0(\theta)=0\)</span>.</p>
<p>Sea <span class="math inline">\(\theta_{(0)}\)</span> una estimación inicial de <span class="math inline">\(\theta\)</span>, por ejemplo, el estimado dado en el ejemplo <a href="modelos-arma.html#exm:ejem-estimacion-momentos-MA1">6.18</a>. Entonces, el procedimiento Gauss-Newton para mínimos cuadrados condicional está dado por</p>
<span class="math display" id="eq:eq-procedimiento-gauss-newton-MA1">\[\begin{equation}
  \theta_{(j+1)}=\theta_{(j)}+\frac{\sum_{t=1}^{n}z_t(\theta_{(j)})w_t(\theta_{(j)})}{\sum_{t=1}^{n}z_t^2(\theta_{(j)})}\text{, }j=0,1,2,\ldots
\tag{6.103}
\end{equation}\]</span>
donde los valores en <a href="modelos-arma.html#eq:eq-procedimiento-gauss-newton-MA1">(6.103)</a> se calculan recursivamente usando <a href="modelos-arma.html#eq:eq-error-truncado-MA1">(6.100)</a> y <a href="modelos-arma.html#eq:eq-derivada-error-truncado-MA1">(6.101)</a>. Los cálculos se paran cuando <span class="math inline">\(|\theta_{(j+1)}-\theta_{(j)}|\)</span> ó <span class="math inline">\(|Q(\theta_{(j+1)})-Q(\theta_{(j)})|\)</span> son menor que alguna cantidad prefijada.
</div>

<hr />

<div class="example">
<p><span id="exm:ejem-ajuste-varvas-glaciares" class="example"><strong>Ejemplo 6.22  (Ajuste de la serie de varvas glaciares)  </strong></span> Consideremos la serie de espesores de varvas glaciares en Massachusetts para <span class="math inline">\(n=634\)</span> años, como analizamos en el ejemplo 3.4.3 (Tema 3) donde ajustamos a un modelo de promedio móvil de primer orden una transformación logarítmica, podemos también a esa serie ajustar una ecuación en diferencia de la transformación logarítmica, como sigue</p>
<p><span class="math display">\[\nabla[\ln(x_t)]=\ln(x_t)-\ln(x_{t-1})=\ln\left(\frac{x_t}{x_{t-1}}\right)\]</span></p>
<p>el cual se puede interpretar como la proporción del porcentaje de cambio en el espesor.</p>
<p>En la figura <a href="modelos-arma.html#fig:grafico-ACF-PACF-varvas-glaciares">6.5</a> mostramos las ACF y PACF muestral, confirmando la tendencia de <span class="math inline">\(\nabla[\ln(x_t)]\)</span> de comportarse como proceso de promedio móvil de primer orden ya que la ACF tiene un pico significativa en paso 1 y la PACF decrece exponencialmente.</p>
<p>A continuación se muestran 9 iteraciones del procedimiento de Gauss-Newton dado en <a href="modelos-arma.html#eq:eq-procedimiento-gauss-newton-MA1">(6.103)</a>, iniciando con <span class="math inline">\(\hat{\theta}_0=-0.1\)</span>, dando los valores</p>
<p><span class="math display">\[-0.442; -0.624; -0.717;-0.750;-0.763;-0.768;-0.771;-0.772;-0.772;\]</span></p>
para <span class="math inline">\(\theta_{(1)},\ldots,\theta_{(9)}\)</span>, y la varianza estimada del error es <span class="math inline">\(\hat{\sigma}_w^2=0.236\)</span>. Usando el valor final de <span class="math inline">\(\hat{\theta}=\theta_{(9)}=-0.772\)</span> y el vector <span class="math inline">\(z_t\)</span> de derivadas parciales en <a href="modelos-arma.html#eq:eq-derivada-error-truncado-MA1-2">(6.102)</a> nos da un error estándar de <span class="math inline">\(0.025\)</span> y un <span class="math inline">\(t\)</span>-valor de <span class="math inline">\(-0.772/0.025=-30.88\)</span> con <span class="math inline">\(632\)</span> grados de libertad (se pierde uno con las diferencias).
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">varva=<span class="kw">scan</span>(<span class="st">&quot;data/varve.txt&quot;</span>)
dv=<span class="kw">log</span>(varva[<span class="dv">2</span><span class="op">:</span><span class="dv">634</span>]<span class="op">/</span>varva[<span class="dv">1</span><span class="op">:</span><span class="dv">633</span>]);
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))
<span class="kw">acf</span>(dv,<span class="dv">30</span>) 
<span class="kw">pacf</span>(dv,<span class="dv">30</span>)</code></pre></div>
<div class="figure"><span id="fig:grafico-ACF-PACF-varvas-glaciares"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/grafico-ACF-PACF-varvas-glaciares-1.svg" alt="ACF y PACF de la serie varvas glaciares"  />
<p class="caption">
Figura 6.5: ACF y PACF de la serie varvas glaciares
</p>
</div>
<hr />
<p>En el caso general de un proceso ARMA(p,q) causal e invertible, las estimaciones de máxima verosimilitud, y las estimaciones de mínimos cuadrados condicional e incondicional (y las estimación de Yule-Walker en el caso de modelos AR) dan estimadores óptimos. La prueba de este resultado general se puede hallar en Brockwell y Davis (2006). Denotaremos los coeficientes del proceso ARMA por <span class="math inline">\(\mathbf{\beta}=(\phi_1,\ldots,\phi_p,\theta_1,\ldots,\theta_q)&#39;\)</span>.</p>

<div class="proposition">
<p><span id="prp:propie-distribucion-estimadores-muestras-grandes" class="proposition"><strong>Proposición 6.9  (Distribución de los estimadores para muestras grandes)  </strong></span> Bajo condiciones apropiadas, para procesos ARMA causal e invertible, los estimadores de máxima verosimilitud, mínimos cuadrados incondicional y condicional, cada uno inicializado por los estimadores dados por el método de los momentos, proveen estimadores óptimos de <span class="math inline">\(\sigma_w^2\)</span> y <span class="math inline">\(\mathbf{\beta}\)</span> en el sentido de que <span class="math inline">\(\hat{\sigma}_w^2\)</span> es consistente, y la distribución asintótica de <span class="math inline">\(\mathbf{\hat{\beta}}\)</span> es la mejor distribución normal asintótica. En particular, cuando <span class="math inline">\(n\to\infty\)</span></p>
<span class="math display" id="eq:eq-distribucion-estimadores-muestras-grandes">\[\begin{equation}
  \sqrt{n}\left(\mathbf{\hat{\beta}}-\mathbf{\beta}\right)\overset{d}{\to}N(\textbf{0},\sigma_w^2\Gamma_{p,q}^{-1})
\tag{6.104}
\end{equation}\]</span>
</div>

<hr />
<p>En <a href="modelos-arma.html#eq:eq-distribucion-estimadores-muestras-grandes">(6.104)</a> la matriz de varianza-covarianza del estimador <span class="math inline">\(\mathbf{\hat{\beta}}\)</span> es la inversa de la matriz de transformación. En este caso, la matriz <span class="math inline">\(\Gamma_{p,q}\)</span> de orden <span class="math inline">\((p+q)\times(p+q)\)</span>, que tiene la forma</p>
<span class="math display" id="eq:eq-matriz-Gamma-pq">\[\begin{equation}
  \Gamma_{p,q}=\left(
                   \begin{array}{cc}
                     \Gamma_{\phi\phi} &amp; \Gamma_{\phi\theta} \\
                     \Gamma_{\theta\phi} &amp; \Gamma_{\theta\theta} \\
                   \end{array}
                 \right)
\tag{6.105}
\end{equation}\]</span>
<p>La <span class="math inline">\(p\times p\)</span> matriz <span class="math inline">\(\Gamma_{\phi\phi}\)</span> es dada por <a href="modelos-arma.html#eq:eq-yule-walker-matricial">(6.76)</a>, esto es, el <span class="math inline">\(ij\)</span>-ésimo elemento de <span class="math inline">\(\Gamma_{\phi\phi}\)</span> para <span class="math inline">\(i,j=1,\ldots,p\)</span> es <span class="math inline">\(\gamma_x(i-j)\)</span> de un proceso AR(p) <span class="math inline">\(\phi(B)x_t=w_t\)</span>. Similarmente, <span class="math inline">\(\Gamma_{\theta\theta}\)</span> es una matriz <span class="math inline">\(q\times q\)</span> con el <span class="math inline">\(ij\)</span>-ésimo elemento para <span class="math inline">\(i,j=1,\ldots,q\)</span> igual a <span class="math inline">\(\gamma_y(i-j)\)</span> de un proceso AR(q) <span class="math inline">\(\theta(B)y_t=w_t\)</span>. La <span class="math inline">\(p\times q\)</span> matriz <span class="math inline">\(\Gamma_{\phi\theta}=\{\gamma_{xy}(i-j)\}\)</span> para <span class="math inline">\(i=1,\ldots,p; j=1,\ldots,q\)</span>; estos es, el <span class="math inline">\(ij\)</span>-ésimo elemento es la covarianza cruzada entre dos procesos AR dados por <span class="math inline">\(\phi(B)x_t=w_t\)</span> y <span class="math inline">\(\theta(B)y_t=w_t\)</span>. Finalmente, <span class="math inline">\(\Gamma_{\theta\phi}=\Gamma_{\phi\theta}&#39;\)</span> es de orden <span class="math inline">\(q\times p\)</span>.</p>

<div class="example">
<p><span id="exm:ejem-distribuciones-asintoticas-especificas" class="example"><strong>Ejemplo 6.23  (Algunas distribuciones asintóticas específicas)  </strong></span> Las siguientes distribuciones son algunos casos de la proposición <a href="modelos-arma.html#prp:propie-distribucion-estimadores-muestras-grandes">6.9</a></p>
<ol style="list-style-type: decimal">
<li><strong>AR(1):</strong> <span class="math inline">\(\gamma_x(0)=\sigma_w^2/(1-\phi^2)\)</span>, de esta manera <span class="math inline">\(\sigma_w^2\Gamma_{1,0}^{-1}=(1-\phi^2)\)</span>. Entonces
<span class="math display" id="eq:eq-distribucion-asintotica-AR1">\[\begin{equation}
    \hat{\phi}\sim AN[\phi,n^{-1}(1-\phi^2)]
\tag{6.106}
\end{equation}\]</span></li>
<li><strong>AR(2):</strong> Pueden verificar que <span class="math inline">\(\gamma_x(0)=\left(\frac{1-\phi_2}{1+\phi_2}\right)\frac{\sigma_w^2}{(1-\phi_2)^2-\phi_1^2}\)</span> y <span class="math inline">\(\gamma_x(1)=\phi_1\gamma_x(0)+\phi_2\gamma_x(1)\)</span>. De este hecho, podemos calcular <span class="math inline">\(\Gamma_{2,0}^{-1}\)</span>. En particular, tenemos
<span class="math display" id="eq:eq-distribucion-asintotica-AR2">\[\begin{equation}
\left(
  \begin{array}{c}
    \hat{\phi}_1 \\
    \hat{\phi}_2 \\
  \end{array}
\right)\sim AN\left[\left(
                      \begin{array}{c}
                        \phi_1 \\
                        \phi_2 \\
                      \end{array}
                    \right), n^{-1}\left(
                                     \begin{array}{cc}
                                       1-\phi_2^2 &amp; -\phi_1(1+\phi_2) \\
                                       \text{sym} &amp; 1-\phi_2^2 \\
                                     \end{array}
                                   \right)\right]
\tag{6.107}  
\end{equation}\]</span></li>
<li><strong>MA(1):</strong> En este caso, escribimos <span class="math inline">\(\theta(B)y_t=w_t\)</span> ó <span class="math inline">\(y_t+\theta y_{t-1}=w_t\)</span>. Entonces, análogamente al caso AR(1), <span class="math inline">\(\gamma_t(0)=\sigma_w^2/(1-\theta^2)\)</span>, de este modo <span class="math inline">\(\sigma_w^2\Gamma_{0,1}^{-1}=(1-\theta^2)\)</span>. Entonces,
<span class="math display" id="eq:eq-distribucion-asintotica-MA1">\[\begin{equation}
\hat{\theta}\sim AN[\theta,n^{-1}(1-\theta^2)]
  \tag{6.108}
  \end{equation}\]</span></li>
<li><strong>MA(2):</strong> Escribiendo <span class="math inline">\(y_t+\theta_1y_{t-1}+\theta_2y_{t-2}=w_t\)</span>, así, análogamente al caso AR(2), tenemos
<span class="math display" id="eq:eq-distribucion-asintotica-MA2">\[\begin{equation}
\left(
  \begin{array}{c}
    \hat{\theta}_1 \\
    \hat{\theta}_2 \\
  \end{array}
\right)\sim AN\left[\left(
                      \begin{array}{c}
                        \theta_1 \\
                        \theta_2 \\
                      \end{array}
                    \right), n^{-1}\left(
                                     \begin{array}{cc}
                                       1-\theta_2^2 &amp; -\theta_1(1+\theta_2) \\
                                       \text{sym} &amp; 1-\theta_2^2 \\
                                     \end{array}
                                   \right)\right]
 \tag{6.109}
 \end{equation}\]</span></li>
<li><strong>ARMA(1,1):</strong> Para calcular <span class="math inline">\(\Gamma_{\phi\theta}\)</span> debemos hallar <span class="math inline">\(\gamma_{xy}(0)\)</span>, donde <span class="math inline">\(x_t-\phi x_{t-1}=w_t\)</span> y <span class="math inline">\(y_t+\theta y_{t-1}=w_t\)</span>. Tenemos
<span class="math display">\[\begin{eqnarray*}
  \gamma_{xy}(0) &amp;=&amp; \text{cov}(x_t,y_t)=\text{cov}(\phi x_{t-1}+w_t,-\theta y_{t-1}+w_t \\
             &amp;=&amp; -\phi\theta\gamma_{xy}(0)+\sigma_w^2
\end{eqnarray*}\]</span>
Resolviendo, hallamos <span class="math inline">\(\gamma_{xy}(0)=\sigma_w^2/(1+\phi\theta)\)</span>. Entonces,
<span class="math display" id="eq:eq-distribucion-asintotica-ARMA11">\[\begin{equation}
   \left(
  \begin{array}{c}
    \hat{\phi} \\
    \hat{\theta} \\
  \end{array}
\right)\sim AN\left[\left(
                      \begin{array}{c}
                        \phi \\
                        \theta \\
                      \end{array}
                    \right), n^{-1}\left[
                                     \begin{array}{cc}
                                       (1-\phi^2)^{-1} &amp; (1+\phi\theta)^{-1} \\
                                       \text{sym} &amp; (1-\theta^2)^{-1} \\
                                     \end{array}
                                   \right]^{-1}\right]
 \tag{6.110}
\end{equation}\]</span></li>
</ol>
</div>

<hr />
<p>Puede resultar sorprendente, que las distribuciones asintóticas de <span class="math inline">\(\hat{\phi}\)</span> de un AR(1) [ecuación <a href="modelos-arma.html#eq:eq-distribucion-asintotica-AR1">(6.106)</a>] y <span class="math inline">\(\hat{\theta}\)</span> de un MA(1) [ecuación <a href="modelos-arma.html#eq:eq-distribucion-asintotica-MA1">(6.108)</a>] sean de la misma forma. Es posible explicar este resultado heurístico inesperado usando la intuición de regresión lineal. Esto es, para el modelo de regresión normal presentado en la Sección 3.3 del Tema 3 sin término de intercepción <span class="math inline">\(x_t=\beta z_t+w_t\)</span>, sabemos que <span class="math inline">\(\hat{\beta}\)</span> es normalmente distribuido con media <span class="math inline">\(\beta\)</span>, y de (3.16) (Tema 3)</p>
<p><span class="math display">\[\text{var}\left\{\sqrt{n}\left(\hat{\beta}-\beta\right)\right\}=n\sigma_w^2\left(\sum_{t=1}^{n}z_t^2\right)^{-1}=\sigma_w^2\left(n^{-1}\sum_{t=1}^{n}z_t^2\right)^{-1}\]</span></p>
<p>Para el modelo AR(1) causal dado por <span class="math inline">\(x_t=\phi x_{t-1}+w_t\)</span>, la intuición de regresión nos dice que debemos esperar que para <span class="math inline">\(n\)</span> grande</p>
<p><span class="math display">\[\sqrt{n}(\hat{\phi}-\phi)\]</span></p>
<p>es aproximadamente normal con media cero y varianza dada por</p>
<p><span class="math display">\[\sigma_w^2\left(n^{-1}\sum_{t=1}^{n}x_{t-1}^2\right)^{-1}\]</span></p>
<p>Ahora, <span class="math inline">\(n^{-1}\sum_{t=2}^{n}x_{t-1}^2\)</span> es la varianza muestral (recuerde que la media de <span class="math inline">\(x_t\)</span> es cero) de <span class="math inline">\(x_t\)</span>, de modo que cuando <span class="math inline">\(n\)</span> se hace grande podemos esperar que esta se aproxime a <span class="math inline">\(\text{var}(x_t)=\gamma(0)=\sigma_w^2/(1-\phi^2)\)</span>. Entonces, la varianza muestral grande de <span class="math inline">\(\sqrt{n}(\hat{\phi}-\phi)\)</span> es</p>
<p><span class="math display">\[\sigma_w^2\gamma_x(0)^{-1}=\sigma_w^2\left(\frac{\sigma_w^2}{1-\phi^2}\right)^{-1}=(1-\phi^2)\]</span></p>
<p>esto es, <a href="modelos-arma.html#eq:eq-distribucion-asintotica-AR1">(6.106)</a> vale.</p>
<p>En el caso de un MA(1), podemos usar la discusión del ejemplo <a href="modelos-arma.html#exm:ejem-gauss-newton-MA1">6.21</a> para escribir un modelo de regresión aproximado para el MA(1). Esto es, considere la aproximación <a href="modelos-arma.html#eq:eq-derivada-error-truncado-MA1-2">(6.102)</a> como el modelo de regresión</p>
<p><span class="math display">\[z_t(\hat{\theta})=-\theta z_{t-1}(\hat{\theta})+w_{t-1}\]</span></p>
<p>donde ahora, <span class="math inline">\(z_{t-1}(\hat{\theta})\)</span> se define como en el ejemplo <a href="modelos-arma.html#exm:ejem-gauss-newton-MA1">6.21</a>, jugando el papel de regresor.</p>
<p>Continuando con la analogía, podemos esperar que la distribución asintótica de <span class="math inline">\(\sqrt{n}(\hat{\phi}-\phi)\)</span> sea normal con media cero y varianza aproximada</p>
<p><span class="math display">\[\sigma_w^2\left(n^{-1}\sum_{t=1}^{n}z_{t-1}^2(\hat{\theta})\right)^{-1}\]</span></p>
<p>Como en el caso AR(1), <span class="math inline">\(n^{-1}\sum_{t=1}^{n}z_{t-1}^2(\hat{\theta})\)</span> es la varianza muestral de <span class="math inline">\(z_t(\hat{\theta})\)</span>, de modo que para <span class="math inline">\(n\)</span> grande, esta debería ser <span class="math inline">\(\text{var}\{z_t(\theta)\}=\gamma_z(0)\)</span>.</p>
<p>Pero, note que, como se ve de <a href="modelos-arma.html#eq:eq-derivada-error-truncado-MA1-2">(6.102)</a>, <span class="math inline">\(z_t(\theta)\)</span> es aproximadamente un proceso AR(1) con parámetro <span class="math inline">\(-\theta\)</span>. Por la tanto,</p>
<p><span class="math display">\[\sigma_w^2\gamma_X(0)^{-1}=\sigma_w^2\left(\frac{\sigma_w^2}{1-(-\theta)^2}\right)^{-1}=(1-\theta^2)\]</span></p>
<p>lo cual concuerda con <a href="modelos-arma.html#eq:eq-distribucion-asintotica-MA1">(6.108)</a>.</p>
<p>Finalmente, la distribución asintótica de los parámetros estimados de un AR y de un MA son de la misma forma, porque en el caso MA, los <em>regresores</em> son las diferencias del proceso <span class="math inline">\(z_t(\theta)\)</span> que tienen estructura AR, y es esta estructura la que determina la varianza asintótica de los estimadores.</p>
<p>En el ejemplo 3.31 el error estándar estimado de <span class="math inline">\(\hat{\theta}\)</span> fue <span class="math inline">\(0.025\)</span>. En el ejemplo, este valor se calculó como la raíz cuadrada de</p>
<p><span class="math display">\[s_w^2\left(n^{-1}\sum_{t=2}^{n}z_{t-1}^2(\hat{\theta})\right)^{-1}\]</span></p>
<p>donde <span class="math inline">\(n=633, s_w^2=0.236\)</span> y <span class="math inline">\(\hat{\theta}=-0.772\)</span>. Usando <a href="modelos-arma.html#eq:eq-distribucion-asintotica-MA1">(6.108)</a>, también pudimos haber calculado este valor usando la aproximación asintótica, como la raíz cuadrada de <span class="math inline">\((1-0.772^2)/633\)</span> lo cual también nos da <span class="math inline">\(0.025\)</span>.</p>
<p>El comportamiento asintótico de los estimadores de los parámetros nos da una información adicional sobre el problema de ajuste de los modelos ARMA a los datos. Por ejemplo, supongamos que una serie de tiempo sigue un proceso AR(1) y decidimos fijar un modelo AR(2) a los datos. ¿Habrá algún problema si hacemos esto? Más generalmente, <em>¿por qué no fijamos un modelo AR de orden grande para asegurar que capturamos toda la dinámica del proceso?</em> Después de todo, si el proceso es realmente un AR(1), los otros parámetros autoregresivos no serán significativos. La respuesta es que si sobre ajustamos el modelo, podemos perder eficiencia. Por ejemplo, si fijamos un modelo AR(1) a un proceso AR(1), para <span class="math inline">\(n\)</span> grande, <span class="math inline">\(\text{var}(\hat{\phi})\approx n^{-1}(1-\phi_1^2)\)</span>. Pero si fijamos un modelo AR(2) a un proceso AR(1), para <span class="math inline">\(n\)</span> grande, <span class="math inline">\(\text{var}(\hat{\phi})\approx n^{-1}(1-\phi_2^2)=n^{-1}\)</span> porque <span class="math inline">\(\phi_2=0\)</span>. En consecuencia, la varianza de <span class="math inline">\(\phi_1\)</span> ha sido aumentada, haciendo del estimador menos preciso. Sin embargo, diremos que el sobre ajuste lo podemos usar como una herramienta de diagnóstico. Por ejemplo,, si fijamos un modelo AR(2) a los datos y estos se satisfacen con el modelo, entonces, agregando un parámetro más y fijando un modelo AR(3) debería darnos aproximadamente el mismo modelo como en el ajuste AR(2). Discutiremos los modelos de diagnóstico con más detalle más adelante.</p>
<p>Si <span class="math inline">\(n\)</span> es pequeño o si los parámetros están cerca de los bordes o cotas, la aproximación asintótica puede ser un poco pobre. La técnica de bootstrap puede ser útil en este caso. Para una explicación ampliada de bootstrap véase Efron y Tibshirani (1994). Daremos un ejemplo simple de bootstrap para un proceso AR(1)</p>

<div class="example">
<p><span id="exm:ejem-bootstrap-AR1" class="example"><strong>Ejemplo 6.24  (Bootstrap para un AR(1))  </strong></span> Consideremos un modelo AR(1) con coeficiente de regresión cerca a la cota de causalidad y un error del proceso que es simétrico pero no normal. Específicamente, considere el modelo estacionario y causal</p>
<span class="math display" id="eq:eq-modelo-estacionario-causal">\[\begin{equation}\label{}
  x_t=\mu+\phi(x_{t-1}-\mu)+w_t
\tag{6.111}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\mu=50, \phi=0.95\)</span> y <span class="math inline">\(w_t\)</span> son iid doble exponencial con localización cero, y parámetro de escala <span class="math inline">\(\beta=2\)</span>. La densidad de <span class="math inline">\(w_t\)</span> está dada por</p>
<p><span class="math display">\[f_{w_t}(w)=\frac{1}{2\beta}\exp[-|w|/\beta]\text{ con }-\infty&lt;w&lt;\infty\]</span></p>
En este ejemplo, <span class="math inline">\(\mathbb{E}(w_t)=0\)</span> y <span class="math inline">\(\text{var}(w_t)=2\beta^2=8\)</span>. La figura <a href="modelos-arma.html#fig:grafico-modelo-estacionario-causal-n-100">6.6</a> muestra <span class="math inline">\(n=100\)</span> observaciones simuladas de este proceso.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">boot=<span class="kw">scan</span>(<span class="st">&quot;data/ar1boot.txt&quot;</span>)
<span class="kw">plot</span>(boot,<span class="dt">type=</span><span class="st">&quot;b&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Tiempo&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:grafico-modelo-estacionario-causal-n-100"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/grafico-modelo-estacionario-causal-n-100-1.svg" alt="Modelo causal estacionario, n=100"  />
<p class="caption">
Figura 6.6: Modelo causal estacionario, n=100
</p>
</div>
<p>Esta realización en particular es interesante, ya que los datos lucen como si fuesen generados de un proceso no-estacionario con tres diferentes niveles de media. De hecho, los datos fueron generados por un modelo estacionario y causal de buen comportamiento, aunque no normal. Para mostrar las ventajas del bootstrap, procederemos como si no conociéramos la distribución del error y procederemos como si este fuera normal; por supuesto, esto significa, por ejemplo, que los EMV de <span class="math inline">\(\phi\)</span> basados en una normal no serán los EMV reales porque los datos no son normales.</p>
<p>Usando los datos mostrado en la figura @ref{fig:grafico-modelo-estacionario-causal-n-100), obtenemos los estimadores de Yule-Walker <span class="math inline">\(\hat{\mu}=40.0483, \hat{\phi}=0.9572\)</span> y <span class="math inline">\(s_w^2=15.55\)</span>, donde <span class="math inline">\(s_w^2\)</span> es el estimado de <span class="math inline">\(\text{var}(w_t)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m=<span class="kw">mean</span>(boot)
m</code></pre></div>
<pre><code>## [1] 40.05</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit=<span class="kw">ar.yw</span>(boot,<span class="dt">order=</span><span class="dv">1</span>)
fit</code></pre></div>
<pre><code>## 
## Call:
## ar.yw.default(x = boot, order.max = 1)
## 
## Coefficients:
##     1  
## 0.957  
## 
## Order selected 1  sigma^2 estimated as  15.6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">phi=fit<span class="op">$</span>ar</code></pre></div>
<p>Basándonos en la proposición <a href="modelos-arma.html#prp:propie-distribucion-estimadores-muestras-grandes">6.9</a>, diremos que <span class="math inline">\(\hat{\phi}\)</span> es aproximadamente normal con media <span class="math inline">\(\phi\)</span> y varianza <span class="math inline">\((1-\phi^2)/100\)</span>, la cual es aproximada por <span class="math inline">\((1-0.957^2)/100=0.029^2\)</span>.</p>
<p>Para evaluar la distribución muestral finita de <span class="math inline">\(\hat{\phi}\)</span> cuando <span class="math inline">\(n=100\)</span>, simularemos 1000 realizaciones de este proceso AR(1) y estimaremos los parámetros vía Yule-Walker. La densidad muestral finita del estimador Yule-Walker de <span class="math inline">\(\phi\)</span> basado en 1000 simulaciones se muestra en la figura <a href="modelos-arma.html#fig:grafico-densidad-muestral-estimadores-yule-walker">6.7</a>. Claramente la distribución muestral no está cerca a la normalidad para este tamaño muestral. La media de la distribución mostrada es <span class="math inline">\(0.8638\)</span> y la varianza es <span class="math inline">\(0.122^2\)</span> estos valores son muy distintos de los valores asintóticos.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Densidad del estimador de Yule-Walker de phi</span>
phi.est=<span class="dv">0</span>
x.sim=boot[<span class="dv">1</span>]
wt=<span class="kw">rexp</span>(<span class="dv">100</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>)
  {
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="dv">100</span>)
  {x.sim[j]=<span class="dv">50</span><span class="op">+</span><span class="fl">0.95</span><span class="op">*</span>(x.sim[j<span class="op">-</span><span class="dv">1</span>]<span class="op">-</span><span class="dv">50</span>)<span class="op">+</span>wt[j]}
  fit.est=<span class="kw">ar.yw</span>(x.sim,<span class="dt">order=</span><span class="dv">1</span>)
  phi.est[i]=fit.est<span class="op">$</span>ar}

<span class="kw">plot</span>(<span class="kw">density</span>(phi.est),<span class="dt">main=</span><span class="st">&quot;Densidad muestral finita de los estimadores de Yule-Walker de phi&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:grafico-densidad-muestral-estimadores-yule-walker"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/grafico-densidad-muestral-estimadores-yule-walker-1.svg" alt="Densidad muestral finita de los estimadores de Yule-Walker de phi"  />
<p class="caption">
Figura 6.7: Densidad muestral finita de los estimadores de Yule-Walker de phi
</p>
</div>
<p>Algunos de los cuantiles de la distribución muestral son:</p>
<table>
<thead>
<tr class="header">
<th>Cuantil</th>
<th align="center">5%</th>
<th align="center">10%</th>
<th align="center">25%</th>
<th align="center">50%</th>
<th align="center">75%</th>
<th align="center">90%</th>
<th align="center">95%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Valor</td>
<td align="center">0.6747</td>
<td align="center">0.6957</td>
<td align="center">0.7587</td>
<td align="center">0.8638</td>
<td align="center">0.9689</td>
<td align="center">1.0320</td>
<td align="center">1.0530</td>
</tr>
</tbody>
</table>
<p>Antes de discutir la técnica de bootstrap, estudiemos el proceso de innovación muestral <span class="math inline">\(x_t-x_t^{t-1}\)</span> con la correspondiente varianza <span class="math inline">\(P_t^{t-1}\)</span>. Para el modelo AR(1) de este ejemplo</p>
<p><span class="math display">\[x_t^{t-1}=\mu+\phi(x_{t-1}-\mu)\text{, }t=2,\ldots,100\]</span></p>
<p>De aquí, se sigue que</p>
<p><span class="math display">\[P_t^{t-1}=\mathbb{E}(x_t-x_t^{t-1})^2=\sigma_w^2\text{, }t=2,\ldots,100\]</span></p>
<p>Cuando <span class="math inline">\(t=1\)</span>, tenemos</p>
<p><span class="math display">\[x_1^0=\mu\text{ y }P_1^0=\sigma_w^2/(1-\phi^2)\]</span></p>
<p>Entonces, las innovaciones tiene media cero pero varianzas distintas; a fin de que todas las innovaciones tengan la misma varianza <span class="math inline">\(\sigma_w^2\)</span>, las escribiremos como</p>
<span class="math display" id="eq:eq-innovaciones-bootstrap">\[\begin{eqnarray}
  \epsilon_1 &amp;=&amp; (x_1-\mu)\sqrt{(1-\phi^2)} \nonumber \\
  \epsilon_t &amp;=&amp; (x_t-\mu)-\phi(x_{t-1}-\mu)\text{, para }t=2,\ldots,100 \tag{6.112}
\end{eqnarray}\]</span>
<p>De estas ecuaciones, podemos escribir el modelo en término de las innovaciones <span class="math inline">\(\epsilon_t\)</span> como</p>
<span class="math display" id="eq:eq-modelo-innovaciones">\[\begin{eqnarray}
  x_1 &amp;=&amp; \mu+\epsilon_1/\sqrt{(1-\phi^2)} \nonumber\\
  x_t &amp;=&amp; \mu+\phi(x_{t-1}-\mu)+\epsilon_t\text{, para }t=2,\ldots,100 \tag{6.113}
\end{eqnarray}\]</span>
<p>A continuación, reemplazamos los parámetros con sus estimados en <a href="modelos-arma.html#eq:eq-innovaciones-bootstrap">(6.112)</a>, esto es, <span class="math inline">\(n=100, \hat{\mu}=40.048\)</span> y <span class="math inline">\(\hat{\phi}=0.957\)</span> y denotamos los resultados de las innovaciones muestrales como <span class="math inline">\(\{\hat{\epsilon}_1,\ldots,\hat{\epsilon}_{100}\}\)</span>. Para obtener una muestra bootstrap, primero escogemos una muestra aleatoria con reemplazo con <span class="math inline">\(n=100\)</span> del conjunto de innovaciones muestral, llamemos a esta muestra <span class="math inline">\(\{\epsilon_1^*,\ldots,\epsilon_{100}^*\}\)</span>. Ahora, generamos un conjunto de datos bootstrap secuencialmente haciendo</p>
<span class="math display" id="eq:eq-generacion-datos-bootstrap">\[\begin{eqnarray}
  x_1^* &amp;=&amp; 40.048+\epsilon_1^*/\sqrt{(1-0.957^2)} \nonumber\\
  x_t^* &amp;=&amp; 40.048+0.957(x_{t-1}^*-40.048)+\epsilon_t^*\text{, }t=2,\ldots,n \tag{6.114}
\end{eqnarray}\]</span>
<p>A continuación, estimamos los parámetros como si los datos fueran <span class="math inline">\(x_t^*\)</span>. Llamamos a estos estimados <span class="math inline">\(\hat{\mu}(1),\hat{\phi}(1)\)</span> y <span class="math inline">\(s_w^2(1)\)</span>. Repetimos este proceso un número grande <span class="math inline">\(N\)</span> de veces, generando una colección de parámetros estimados bootstrap <span class="math inline">\(\{\hat{\mu}(k),\hat{\phi}(k),s_w^2(k),k=1,\ldots,N\}\)</span>. Podemos entonces aproximar la distribución muestral finita de un estimador de los valores del parámetro obtenido con bootstrap. Por ejemplo, podemos aproximar la distribución de <span class="math inline">\(\hat{\phi}-\phi\)</span> por la distribución empírica de <span class="math inline">\(\hat{\phi}(k)-\hat{\phi}\)</span> para <span class="math inline">\(k=1,\ldots,N\)</span>.</p>
<p>La figura <a href="modelos-arma.html#fig:grafico-histograma-bootstrap">6.8</a> muestra un histograma bootstrap de 200 estimaciones de <span class="math inline">\(\phi\)</span> hechas con bootstrap usando los datos en la figura @ref(fig:grafico-modelo-estacionario-causal-n-100}. En particular, la media de la distribución de <span class="math inline">\(\hat{\phi}(k)\)</span> es <span class="math inline">\(0.8750\)</span> con varianza <span class="math inline">\(0.0556^2\)</span>. Algunos cuantiles de esta distribución son:</p>
<table>
<thead>
<tr class="header">
<th>Cuantil</th>
<th align="center">5%</th>
<th align="center">10%</th>
<th align="center">25%</th>
<th align="center">50%</th>
<th align="center">75%</th>
<th align="center">90%</th>
<th align="center">95%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Valor</td>
<td align="center">0.7833</td>
<td align="center">0.8014</td>
<td align="center">0.8412</td>
<td align="center">0.8762</td>
<td align="center">0.9135</td>
<td align="center">0.9455</td>
<td align="center">0.9672</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Booststrap</span>

nboot=<span class="dv">200</span>
resids=fit<span class="op">$</span>resid
resids=resids[<span class="dv">2</span><span class="op">:</span><span class="dv">100</span>]
boot.star=boot
phi.star=<span class="kw">matrix</span>(<span class="dv">0</span>,nboot,<span class="dv">1</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nboot){
  resid.star=<span class="kw">sample</span>(resids,<span class="dt">replace=</span><span class="ot">TRUE</span>)
  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">99</span>){
    boot.star[t<span class="op">+</span><span class="dv">1</span>]=boot<span class="op">+</span>phi<span class="op">*</span>(boot.star[t]<span class="op">-</span>boot)<span class="op">+</span>resid.star[t]
  }
  phi.star[i]=<span class="kw">ar.yw</span>(boot.star,<span class="dt">order=</span><span class="dv">1</span>)<span class="op">$</span>ar
}
<span class="co"># Histograma</span>
<span class="kw">hist</span>(phi.star,<span class="dt">breaks=</span><span class="dv">15</span>,<span class="dt">col =</span> <span class="st">&quot;lightblue&quot;</span>,
<span class="dt">main=</span><span class="st">&quot;Histograma de frecuencia para phi estimado con bootstrap&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:grafico-histograma-bootstrap"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/grafico-histograma-bootstrap-1.svg" alt="Histograma bootstrap de phi basado en 200 iteraciones."  />
<p class="caption">
Figura 6.8: Histograma bootstrap de phi basado en 200 iteraciones.
</p>
</div>
<hr />

<h1 id="modelos-arima"><span class="header-section-number">6.2</span> Modelos ARIMA</h1>
<p>En este capítulo examinaremos el problema de encontrar un modelo apropiado para un conjunto determinado de observaciones <span class="math inline">\(\{x_1,\ldots,x_n\}\)</span> que no son necesariamente generados por una serie de tiempo estacionaria. Si los datos (a) no muestran desviaciones aparentes de la estacionariedad y (b) tienen una función de autocovarianza en rápida disminución, intentamos ajustar un modelo ARMA a los datos medios corregidos utilizando las técnicas desarrolladas en el capítulo de modelos ARMA. De lo contrario, buscamos primero una transformación de los datos que genere una nueva serie con las propiedades (a) y (b). Esto puede lograrse frecuentemente mediante la diferenciación, lo que nos lleva a considerar la clase de modelos ARIMA (siglas en inglés: autoregressive integrated moving-average).</p>
<p>En muchas situaciones, las series de tiempo pueden pensarse o ver como la composición de dos componentes, una componente de tendencia no estacionaria y una componente estacionaria de media cero. Por ejemplo, consideremos el modelo</p>
<span class="math display" id="eq:eq-modelo-base-1-diferencia">\[\begin{equation}
x_t = \mu_t+y_t,
\tag{6.115}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\mu_t=\beta_0+\beta_1t\)</span> y <span class="math inline">\(y_t\)</span> es estacionario. Si diferenciamos este proceso, obtenemos un proceso estacionario, en efecto</p>
<span class="math display">\[\begin{eqnarray*}
\nabla x_t &amp;=&amp; x_t-x_{t-1} = (\mu_t+y_t)-(\mu_{t-1}+y_{t-1}) \\
    &amp;=&amp; (\beta_0+\beta_1t+y_t)-(\beta_0+\beta_1(t-1)+y_{t-1}) \\
    &amp;=&amp; \beta_1 + y_t-y_{t-1} \\
    &amp;=&amp; \beta_1+\nabla y_t.
\end{eqnarray*}\]</span>
<p>El cual claramente es estacionario.</p>
<p>Otro modelo que lleva a la primera diferenciación es el caso en el cual <span class="math inline">\(\mu_t\)</span> en la ecuación <a href="modelos-arma.html#eq:eq-modelo-base-1-diferencia">(6.115)</a>, es un proceso estocástico y que varía lentamente de acuerdo a un paseo laeatorio. Esto es,</p>
<p><span class="math display">\[\mu_t=\mu_{t-1}+v_t,\]</span></p>
<p>donde <span class="math inline">\(v_t\)</span> es estacionario. Tenemos entonces</p>
<p><span class="math display">\[x_t=\mu_{t-1}+v_t+y_t.\]</span></p>
<p>En este caso,</p>
<span class="math display">\[\begin{eqnarray*}
\nabla x_t &amp;=&amp; x_t-x_{t-1} \\
    &amp;=&amp; (\mu_{t-1}+v_t+y_t)-(\mu_{t-2}+v_{t-1}+y_{t-1}) \\
    &amp;=&amp; v_t+\nabla y_t,
\end{eqnarray*}\]</span>
<p>es estacionario. Si <span class="math inline">\(\mu_t\)</span> en <a href="modelos-arma.html#eq:eq-modelo-base-1-diferencia">(6.115)</a> es un polinomio de grado <span class="math inline">\(k\)</span>, <span class="math inline">\(\mu_t=\sum_{i=0}^k\beta_it^i\)</span>, entonces la serie diferenciada <span class="math inline">\(\nabla^ky_t\)</span> es estacinaia. Por ejemplo, sea <span class="math inline">\(\mu_t=\beta_0+\beta_1t+\beta_2t^2\)</span>, luego</p>
<p><span class="math display">\[x_t=\mu_t+y_t = \beta_0+\beta_1t+\beta_2t^2+y_t.\]</span></p>
<p>Diferenciando una vez se tiene</p>
<span class="math display">\[\begin{eqnarray*}
\nabla x_t &amp;=&amp; (\mu_t+y_t)-(\mu_{t-1}+y_{t-1}) \\
    &amp;=&amp; (\beta_0+\beta_1t+\beta_2t^2+y_t) - (\beta_0+\beta_1(t-1)+\beta_2(t-1)^2+y_{t-1}) \\
    &amp;=&amp; \beta_0+\beta_1t+\beta_2t^2+y_t-\beta_0-\beta_1t-\beta_1-\beta_2(t^2-2t+1)-y_{t-1} \\
    &amp;=&amp; \beta_1+\beta_2+2\beta_2t+\nabla y_t.
\end{eqnarray*}\]</span>
<p>Volvemos a diferenciar</p>
<span class="math display">\[\begin{eqnarray*}
\nabla(\nabla x_t) &amp;=&amp; (\beta_1+\beta_2+2\beta_2t+(y_t-y_{t-1})) - (\beta_1+\beta_2+2\beta_2(t-1)+(y_{t-1}-y_{t-2})) \\
    &amp;=&amp; \beta_1+\beta_2+2\beta_2t+y_t-y_{t-1}-\beta_1-\beta_2-2\beta_2t+2\beta_2-y_{t-1}+y_{t-2} \\
    &amp;=&amp; 2\beta_2+\nabla(\nabla y_t) \\
    &amp;=&amp; 2\beta_2+\nabla^2y_t.
\end{eqnarray*}\]</span>
<p>El cual es estacionario. Los modelos estocásticos con tendencia llevan a diferenciaciones de orden superior. Por ejemplo, supongamos</p>
<p><span class="math display">\[\mu_t=\mu_{t-1}+v_t\text{ y }v_t=v_{t-1}+e_t\]</span></p>
<p>donde <span class="math inline">\(e_t\)</span> es estacionario. Entonces <span class="math inline">\(\nabla x_t = v_t+\nabla y_t\)</span> no es estacionario, pero</p>
<p><span class="math display">\[\nabla^2x_t = e_t+\nabla^2y_t,\]</span></p>
<p>si es estacionario.</p>
<p>Los modelos ARMA integrados o modelos ARIMA, es una extensión de los modelos ARMA que incluyen diferenciación. Formalmente, tenemos la siguiente definición.</p>

<div class="definition">
<p><span id="def:defi-modelo-ARIMA" class="definition"><strong>Definición 6.6  </strong></span>Un proceso <span class="math inline">\(x_t\)</span> es un proceso ARIMA(p,d,q) si</p>
<p><span class="math display">\[\nabla^dx_t = (1-B)^dx_t\]</span></p>
<p>es un proceso ARMA(p,q). En general, escribimos el modelo como</p>
<span class="math display" id="eq:eq-modelo-ARIMA">\[\begin{equation}
  \phi(B)(1-B)^dx_t = \theta(B)w_t.
\tag{6.116}
\end{equation}\]</span>
<p>Si <span class="math inline">\(\mathbb{E}(\nabla^dx_t)=\mu\)</span>, escribimos el modelo como</p>
<p><span class="math display">\[\phi(B)(1-B)^dx_t=\delta+\theta(B)w_t,\]</span></p>
donde <span class="math inline">\(\delta=\mu(1-\phi_1-\cdots-\phi_p)\)</span>.
</div>

<hr />
<p>Debido a la no estacionaridad, debemosser cuidadosos cuando realizamos predicciones.</p>
<h2 id="construcción-de-modelos-arima"><span class="header-section-number">6.2</span> Construcción de modelos ARIMA</h2>
<p>Hay algunos pasos básicos para ajustar modelos ARIMA a series de tiempo:</p>
<ul>
<li><p>Gráfico de los datos.</p></li>
<li><p>Posible transformación de los datos (diferenciación, logaritmo, etc.).</p></li>
<li><p>Identificación del orden de dependencia del modelo.</p></li>
<li><p>Estimación del (los) parámetro(s).</p></li>
<li><p>Diagnóstico.</p></li>
<li><p>Elección del modelo.</p></li>
</ul>
<p>Primero, como en todo análisis de datos, debemos realizar un gráfico de serie de tiempo de los datos e inspeccionar el mismo para ver cualquier anomalía. Si, por ejemplo, la variabilidad crece en el tiempo, será necesario que transformemos los datos para estabilizar la varianza. En este caso, las transformaciones de potencia de la clase Box-Cox resultan útiles. Por ejemplo, supongamos un proceso que evoluciona como un cambio porcentual bastante pequeño, como una inversión. Supongamos que</p>
<p><span class="math display">\[x_t=(1+p_t)x_{t-1},\]</span></p>
<p>donde <span class="math inline">\(x_t\)</span> es el valor de la inversión en tiempo <span class="math inline">\(t\)</span> y <span class="math inline">\(p_t\)</span> es el porcentaje de cambio del periodo <span class="math inline">\(t-1\)</span> al <span class="math inline">\(t\)</span>, el cual puede ser negativo. Tomando logaritmo tenemos</p>
<p><span class="math display">\[\ln(x_t) = \ln(1+p_t)+\ln(x_{t-1})\]</span></p>
<p>o</p>
<p><span class="math display">\[\nabla\ln(x_t) = \ln(1+p_t).\]</span></p>
<p>Si el porcentaje de cambio <span class="math inline">\(p_t\)</span> se mantiene relativamente pequeño en magnitud, entonces <span class="math inline">\(\ln(1+p_t)\approx p_t\)</span> <a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> y entonces</p>
<p><span class="math display">\[\nabla\ln(x_t)\approx p_t,\]</span></p>
<p>será un proceso relativamente estable. A menudo <span class="math inline">\(\nabla\ln(x_t)\)</span> se llama el retorno o tasa de crecimiento. Después de una transformación apropiada de los datos, el siguiente paso es identificar los valores preliminares del orden autoregresivo <span class="math inline">\(p\)</span>, el orden de diferenciación <span class="math inline">\(d\)</span> y el orden de promedio móvil <span class="math inline">\(q\)</span>.</p>
<p>Ya hemos abordado en parte el problema de la elección del orden de diferenciación. El gráfico de la serie de tiempo nos ayudará a deterinar si hace falta una diferenciación. Si se requiere una diferenciación, entonces diferenciamos los datos una vez <span class="math inline">\((d=1)\)</span>, e inspeccionamos el gráfico de <span class="math inline">\(\nabla x_t\)</span>. Si hace falta otra diferenciación, entonces volvemos a diferenciar <span class="math inline">\((d=2)\)</span> e inspeccionamos nuevamente el gráfico, esta vez el de <span class="math inline">\(\nabla^2x_t\)</span>. Debemos tener cuidado de no sobre diferenciar pues esto puede introducir dependencia donde no la hay. Por ejemplo, <span class="math inline">\(x_t=w_t\)</span> es no correlacionado, pero <span class="math inline">\(\nabla x_t=w_t-w_{t-1}\)</span> es un proceso <span class="math inline">\(MA(1)\)</span>. Además del gráfico de la serie de tiempo, la ACF muestral nos puede ayudar a ver si es necesaria una diferenciación. Dado que el polinomio <span class="math inline">\(\phi(z)(1-z)^d\)</span> tiene raíz unitaria, la ACF muestral <span class="math inline">\(\hat{\rho}(h)\)</span>, no decaerá a cero tan rápido cuando <span class="math inline">\(h\)</span> crece. Entonces, un lento decaimiento en <span class="math inline">\(\hat{\rho}(h)\)</span> es un indicativo de que se necesitará una diferenciación.</p>
<p>Una vez que hemos establecido el valor preliminar de <span class="math inline">\(d\)</span>, el siguiente paso es ver la ACF y PACF muestrales de <span class="math inline">\(\nabla^dx_t\)</span> para el valor de <span class="math inline">\(d\)</span> elegido. Usando la tabla resumen para la elección de modelos ARMA (véase la tabla al final de la sección <a href="modelos-arma.html#propiedades-de-los-modelos-armapq">Propiedades de los modelos ARMA(p,q)</a>) como guía, escogemos los valores preliminares de <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span>.</p>
<p>Dado que estamos trabajando con estimaciones, dos modelos que luzcan diferentes pueden ser de hecho bastante similares, por lo tanto, nodebemos preocuparnos, de momento, en ser muy precisos al ajustar un modelo. En este punto, unos pocos valores preliminares de <span class="math inline">\(p,d\)</span> y <span class="math inline">\(q\)</span> serán suficientes y nos permitirán iniciar las estimaciones de los parámetros. A continuación daremos un ejemplo de uso de los pasos previamente descritos.</p>

<div class="example">
<span id="exm:ejem-gnp-data" class="example"><strong>Ejemplo 6.25  (Análisis de datos G.N.P.)  </strong></span>Consideremos los datos del Producto Nacional Bruto trimestral de EE.UU en miles de millónes de dólares, desde el primer trimestre de 1947 hasta el tercer trimestre de 2002. Son <span class="math inline">\(n=223\)</span> observaciones, losdatos han sido ajustados estacionalmente. El archivo de datos es “gnp96.txt”, y fueron obtenidos del <em>Federal Reserve Bank of St. Louis</em> (<a href="http://research.stlouisfed.org/" class="uri">http://research.stlouisfed.org/</a>). El gráfico <a href="modelos-arma.html#fig:fig-gnp-data">6.9</a> muestra la serie de tiempo correspondiente.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Lectura de los datos</span>
gnp=<span class="kw">read.table</span>(<span class="st">&quot;data/gnp96.txt&quot;</span>)
<span class="co"># Grafico de la serie</span>
<span class="kw">plot</span>(gnp,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Años&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;G.N.P.&quot;</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-gnp-data"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-gnp-data-1.svg" alt="Producto Nacional Bruto trimestral de EE.UU (en miles de millónes de dólares), desde el 1er trimestre de 1947 hasta el 3er trimestre de 2002."  />
<p class="caption">
Figura 6.9: Producto Nacional Bruto trimestral de EE.UU (en miles de millónes de dólares), desde el 1er trimestre de 1947 hasta el 3er trimestre de 2002.
</p>
</div>
<p>Dado que la serie presenta una fuerte tendencia creciente, no es claro si la varianza crece con el tiempo. Por lo tanto para propósito de demostrar como usar la ACF muestral, en la figura <a href="modelos-arma.html#fig:fig-acf-gnp">6.10</a> mostramos la misma. Como el decaimiento es lento, esto nos sugiere que una diferenciación es posible.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ACF muestral de GNP</span>
<span class="kw">acf</span>(gnp[,<span class="dv">2</span>], <span class="dv">50</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-acf-gnp"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-acf-gnp-1.svg" alt="ACF muestral para la serie de datos G.N.P."  />
<p class="caption">
Figura 6.10: ACF muestral para la serie de datos G.N.P.
</p>
</div>
<p>La figura <a href="modelos-arma.html#fig:fig-gnp-dif-1">6.11</a> muestra la primera diferenciación, allí podemos observar que la variabilidad en la segunda mitad de datos es mayor que en la primera mitad. Además, parece que la tendencia creciente todavía está presente, porlo tanto, tomando en cuenta los pasos descritos al inicio de esta sección primero transformamos losdatos y luego diferenciamos, así obtenemos <span class="math inline">\(y_t=\nabla\ln(x_t)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Primera diferencia</span>
gnpdif=<span class="kw">diff</span>(gnp[,<span class="dv">2</span>])
<span class="kw">plot</span>(gnp[<span class="dv">2</span><span class="op">:</span><span class="dv">223</span>,<span class="dv">1</span>],gnpdif, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Años&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;diff(G.N.P.)&quot;</span>,
     <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-gnp-dif-1"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-gnp-dif-1-1.svg" alt="Primera diferenciación de la serie de tiempo G.N.P."  />
<p class="caption">
Figura 6.11: Primera diferenciación de la serie de tiempo G.N.P.
</p>
</div>
<p>La figura <a href="modelos-arma.html#fig:fig-gnp-log-dif-1">6.12</a> muestra la serie transformada y diferenciada, podemos ver allí que el proceso parece ser estable. Más aún, podemos interpretar los valores de <span class="math inline">\(y_t\)</span> como el porcentaje de crecimiento trimestral del Producto Nacional Bruto de EE.UU.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Transformacion y primera diferencia</span>
gnpgr =<span class="st"> </span><span class="kw">diff</span>(<span class="kw">log</span>(gnp[,<span class="dv">2</span>]))
<span class="kw">plot</span>(gnp[<span class="dv">2</span><span class="op">:</span><span class="dv">223</span>,<span class="dv">1</span>],gnpgr,<span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Años&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;diff(G.N.P.)&quot;</span>,
     <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-gnp-log-dif-1"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-gnp-log-dif-1-1.svg" alt="Serie de tiempo de G.N.P. transformada (log) y diferenciada una vez"  />
<p class="caption">
Figura 6.12: Serie de tiempo de G.N.P. transformada (log) y diferenciada una vez
</p>
</div>
<p>Graficamos ahora las ACF y PACF muestral de <span class="math inline">\(y_t\)</span>. Observando las ACF y PACF parece que la ACF se corta en paso 2 y la PACF decae, lo que nos sugiere un modelo <span class="math inline">\(MA(2)\)</span> para la tasa de crecimiento del P.N.B., o un modelo ARIMA(0,1,2) para <span class="math inline">\(y_t\)</span>. Pero en lugar de enfocarnos en un solo modelo, si detallamos las ACF y PACF muestral, parece sugerir que la ACF decrece y la PACF se corta en paso 1, lo que sugiere un modelo <span class="math inline">\(AR(1)\)</span> para la tasa de cambio o un modelo ARIMA(1,1,0) para <span class="math inline">\(y_t\)</span>. Podemos decir entonces que un modelo ARIMA(1,1,2) es una primera elección de ajuste.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ACF y PACF de la transformacion</span>
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))
<span class="kw">acf</span>(gnpgr, <span class="dv">24</span>)
<span class="kw">pacf</span>(gnpgr,<span class="dv">24</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-acf-pacf-gnp"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-acf-pacf-gnp-1.svg" alt="ACF y PACF muestrales para las serie de tiempo G.N.P., transformada y diferenciada"  />
<p class="caption">
Figura 6.13: ACF y PACF muestrales para las serie de tiempo G.N.P., transformada y diferenciada
</p>
</div>
<p>A modo de entrenamiento vamos a realizar primeramente los ajustes de los modelos <span class="math inline">\(MA(2)\)</span> y <span class="math inline">\(AR(1)\)</span> por separado. Para ello nos valemos de R, usaremos la función ‘sarima’ del paquete ‘astsa’. Iniciamos con el modelo <span class="math inline">\(AR(1)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gnpgr.ar=<span class="kw">arima</span>(gnpgr, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>)) <span class="co"># AR(1)</span></code></pre></div>
<p>El modelo <span class="math inline">\(AR(1)\)</span> estimado es <span class="math inline">\(x_t=\mu(1-\phi_1)+\phi_1x_{t-1}+\hat{w}_t\)</span></p>
<span class="math display" id="eq:eq-modelo-AR1-gnp">\[\begin{equation}
x_t = 0.0083_{(0.001)}(1-0.3467)+0.3467_{(0.063)}x_{t-1}+\hat{w}_t
\tag{6.117}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\hat{\sigma}_w=0.0095\)</span> con 220 grados de libertad; note que la constante en <a href="modelos-arma.html#eq:eq-modelo-AR1-gnp">(6.117)</a> es <span class="math inline">\(0.0083(1-0.3467)=0.005\)</span>. Los valores entre parentesis son los errores estándar estimados.</p>
<p>Ahora usando EMV fijamos un modelo <span class="math inline">\(MA(2)\)</span> para la tasa de crecimiento <span class="math inline">\(x_t\)</span>, siendo el modelo estimado</p>
<span class="math display" id="eq:eq-modelo-MA2-gnp">\[\begin{equation}
x_t = 0.008_{(0.001)}+0.303_{(0.065)}\hat{w}_{t-1}+0204_{(0.064)}\hat{w}_{t-2}+\hat{w}_t
\tag{6.118}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\hat{\sigma}_w=0.0094\)</span> con 219 grados de libertad. Los valores entre parentesis corresponden a los errores estándar estimados. Aunque la constante es muy pequeña, su valor es significativo, no incluir una constante lleva a conclusiones erróneas sobre la naturaleza de la economía estadounidense. Si no incluimos una constane, asumiríamos que la tasa de crecimiento trimestral promedio es cero, mientras que en realidad la tasa de crecimiento trimestral promedio del P.N.B. de EE.UU es de alrededor del 1% (véase la gráfica <a href="modelos-arma.html#fig:fig-gnp-data">6.9</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gnpgr.ma2=<span class="kw">arima</span>(gnpgr, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>)) <span class="co"># MA(2)</span>
<span class="co"># Psi-pesos</span>
<span class="kw">ARMAtoMA</span>(<span class="dt">ar=</span>.<span class="dv">3467</span>, <span class="dt">ma=</span><span class="dv">0</span>, <span class="dv">10</span>) <span class="co"># prints psi-weights</span></code></pre></div>
<pre><code>##  [1] 3.467e-01 1.202e-01 4.167e-02 1.445e-02 5.009e-03
##  [6] 1.737e-03 6.021e-04 2.088e-04 7.237e-05 2.509e-05</code></pre>
<hr />
<p>El siguiente paso en el ajuste de modelos es el diagnóstico. Esta investigación incluye el análisis de residuales así como la comparación de modelos. De nuevo, el primer paso envuelve un gráfico de las innovaciones (o residuales) <span class="math inline">\(x_t-\hat{x}_t^{t-1}\)</span> o de las innovaciones estandarizadas</p>
<span class="math display" id="eq:eq-innovaciones-estandar">\[\begin{equation}
 e_t = (x_t-\hat{x}_t^{t-1})/\sqrt{\hat{P}_t^{t-1}}
\tag{6.119}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\hat{x}_t^{t-1}\)</span> es la predicción de un paso de <span class="math inline">\(x_t\)</span> basado en el modelo ajustado y <span class="math inline">\(\hat{P}_t^{t-1}\)</span> es varianza del error estimado de un paso. Si el modelo se ajusta bien, el residual estandarizado debe comportarse como una sucesión iid de media cero y varianza uno, así que debemos observar bien el gráfico de la serie de tiempo para ver si hay desviación evidente de esta suposición. Por ejemplo, es posible en el caso no-gaussiano tener un proceso no-correlacionado para el cual valores contiguos en tiempo sean altamente dependientes. Como ejemplo, podemos mencionar la familia de modelos GARCH que discutiremos en el capítulo siguiente.</p>
<p>Para determinar o investigar sobre la normaidad marginal nos valemos del histograma de los residuales, así visualmente podemos ver si el mismo se parece o ajusta a la curva de densidad normal. Además de esto, un gráfico de probabilidad normal o un gráfico de cuantiles (qq-plot) nos puede ayudar a identificar la desviación de la normalidad.</p>
<p>También podemos inspeccionar la autocorrelación muestral de los residuales <span class="math inline">\(\hat{\rho}_e(h)\)</span>, para ver algún patrón o valores grandes. Recordemos que, para un ruido blanco, las autocorrelaciones muestrales son aproximadamente independientes y normalmente distribuidas con media cero y varianza <span class="math inline">\(1/n\)</span>. Por consiguiente, una buena forma de inspeccionar la estructura de correlación de los residuales es graficar <span class="math inline">\(\hat{\rho}_e(h)\)</span> vs <span class="math inline">\(h\)</span> junto con las cotas de error <span class="math inline">\(\pm2/\sqrt{n}\)</span>. Tome en cuenta, sin embargo, que los residuales de un modelo ajustado, no tendrán necesariamente las propiedades de un ruido blanco y la varianza de <span class="math inline">\(\hat{\rho}_e(h)\)</span> puede ser mucho menor que <span class="math inline">\(1/n\)</span>.</p>
<p>Además de graficar <span class="math inline">\(\hat{\rho}_e(h)\)</span>, podemos realizar una prueba de hipótesis general que tome en consideración las magnitudes de <span class="math inline">\(\hat{\rho}_e(h)\)</span> como grupo. Por ejemplo, puede ser el caso que individualmente cada <span class="math inline">\(\hat{\rho}_e(h)\)</span> sea pequeño en magnitud, digamos menr que <span class="math inline">\(2/\sqrt{n}\)</span> en magnitud, es decir <span class="math inline">\(|\hat{\rho}_e(h)|&lt;2/\sqrt{n}\)</span>, pero colectivamente, los valores sean grandes. El estadístico de Ljung-Box-Pierce dado por</p>
<span class="math display" id="eq:eq-estadistico-ljung-box-pierce">\[\begin{equation}
 Q = n(n+2)\sum_{h=1}^H\frac{\hat{\rho}_e^2(h)}{n-h}
 \tag{6.120}
\end{equation}\]</span>
<p>es útil para realizar esta prueba de hipótesis. El valor <span class="math inline">\(H\)</span> en <a href="modelos-arma.html#eq:eq-estadistico-ljung-box-pierce">(6.120)</a> se elige de manera arbitraria, en general se usa <span class="math inline">\(H=20\)</span>. Bajo la hipótesis nula de que el modelo es adecuado, asintóticamente, (cuando <span class="math inline">\(n\to\infty\)</span>), <span class="math inline">\(Q\)</span> se distribuye como una chi-cuadrado con <span class="math inline">\(H-p-q\)</span> grados de libertad, esto es <span class="math inline">\(Q\sim\chi_{H-p-q}^2\)</span>. Entonces, rechazamos la hipótesis nula a nivel <span class="math inline">\(\alpha\)</span> si el valor de <span class="math inline">\(Q\)</span> es mayor que la <span class="math inline">\(\chi_{H-p-q}^2(1-\alpha)\)</span>.</p>

<div class="example">
<span id="exm:ejem-diagnostico-tasa-crecimiento-gnp" class="example"><strong>Ejemplo 6.26  (Diagnóstico para la tasa de crecimiento del P.N.B)  </strong></span>Enfoquémonos en el modelo <span class="math inline">\(MA(2)\)</span> ajustado del ejemplo <a href="modelos-arma.html#exm:ejem-gnp-data">6.25</a>, el análisis de los residuales de <span class="math inline">\(AR(1)\)</span> es similar. La figura <a href="#fig:fig-modelo-arima002-gnp"><strong>??</strong></a> muestra el gráfico de los residuales estandarizados (parte superior), la ACF de los residuales (parte media izquierda) (Note que R incluye la correlación en paso cero que siempre es uno) y los valores del estadístico <span class="math inline">\(Q\)</span>, dado en <a href="modelos-arma.html#eq:eq-estadistico-ljung-box-pierce">(6.120)</a> desde paso <span class="math inline">\(H=1\)</span> hasta <span class="math inline">\(H=20\)</span> (parte inferior).
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tsdiag</span>(gnpgr.ma2,<span class="dt">gof.lag=</span><span class="dv">20</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-diagnostico-residual-MA2-gnp"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-diagnostico-residual-MA2-gnp-1.svg" alt="Diagnóstico de los residuales con el modelo MA(2) de la serie P.N.B."  />
<p class="caption">
Figura 6.14: Diagnóstico de los residuales con el modelo MA(2) de la serie P.N.B.
</p>
</div>
<p>Observando el gráfico de los residuales estandarizados en <a href="modelos-arma.html#fig:fig-diagnostico-residual-MA2-gnp">6.14</a>, no muestran un patrón obvio. Note que no hay valores atípicos pero si algunos pocos valores mayores que 3 desviaciones estándar. La ACF de los residuales no muestra una aparente desviación de la suposición del modelo, y el estadístico <span class="math inline">\(Q\)</span> no es significativo para los primeros 20 pasos calculados.</p>
<p>Finalmente la figura <a href="modelos-arma.html#fig:fig-histograma-qq-plot-residual-gnp">6.15</a> muestra un histograma de los residuales (parte superior) y un gráfico qq-plot de los residuales (parte inferior). En la misma podemos observar que los residuales están cercanos a la normalidad excepto para algunos valores extremos en la cola.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))
<span class="kw">hist</span>(gnpgr.ma2<span class="op">$</span>resid,<span class="dt">br=</span><span class="dv">12</span>)
<span class="kw">qqnorm</span>(gnpgr.ma2<span class="op">$</span>resid)</code></pre></div>
<div class="figure"><span id="fig:fig-histograma-qq-plot-residual-gnp"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-histograma-qq-plot-residual-gnp-1.svg" alt="Histograma de los residuales para P.N.B. (parte superior), y qq-plot de los residuales (parte inferior), para el modelo MA(2)"  />
<p class="caption">
Figura 6.15: Histograma de los residuales para P.N.B. (parte superior), y qq-plot de los residuales (parte inferior), para el modelo MA(2)
</p>
</div>
<p>Para concluir, realizamos la prueba de Shapiro-Wilk (referencia), la cual nos da un <span class="math inline">\(p\)</span>-valor de 0.003, lo que indica que los residuales no son normal. Por lo tanto, el modelo parece ajustarse bien salvo que debemos usar una distribución con una cola más pesada que la distribución normal. El comando en R para la prueba de Shapiro-Wilk es</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(gnpgr.ma2<span class="op">$</span>resid)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  gnpgr.ma2$resid
## W = 0.98, p-value = 0.003</code></pre>
<hr />
<p>Podemos hacer lo mismo ahora para el modelo ARIMA(1,1,2)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gnpgr.arima=<span class="kw">arima</span>(gnpgr,<span class="dt">order=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">tsdiag</span>(gnpgr.arima,<span class="dt">gof.lag=</span><span class="dv">20</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-diagnostico-gnp-arima"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-diagnostico-gnp-arima-1.svg" alt="Diagnóstico para el modelo ARIMA de B.N.P"  />
<p class="caption">
Figura 6.16: Diagnóstico para el modelo ARIMA de B.N.P
</p>
</div>
<p>Nuevamente, observando el gráfico de los residuales estandarizados, estos no muestran un patrón obvio. Note que tampoco hay valores atípicos pero si algunos pocos valores mayores que 3 desviaciones estándar. La ACF de los residuales no muestra una aparente desviación de la suposición del modelo, y el estadístico <span class="math inline">\(Q\)</span> no es significativo para los primeros 20 pasos calculados.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))
<span class="kw">hist</span>(gnpgr.arima<span class="op">$</span>resid)
<span class="kw">qqnorm</span>(gnpgr.arima<span class="op">$</span>resid)</code></pre></div>
<div class="figure"><span id="fig:fig-histograma-qq-plot-residual-gnp-arima"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-histograma-qq-plot-residual-gnp-arima-1.svg" alt="Histograma de los residuales para P.N.B. (parte superior), y qq-plot de los residuales (parte inferior), para el modelo ARIMA(1,1,2)"  />
<p class="caption">
Figura 6.17: Histograma de los residuales para P.N.B. (parte superior), y qq-plot de los residuales (parte inferior), para el modelo ARIMA(1,1,2)
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(gnpgr.arima<span class="op">$</span>resid)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  gnpgr.arima$resid
## W = 0.98, p-value = 0.001</code></pre>
<p>La figura <a href="modelos-arma.html#fig:fig-histograma-qq-plot-residual-gnp-arima">6.17</a> muestra un histograma de los residuales (parte superior) y un gráfico qq-plot de los residuales (parte inferior). En la misma, de manera similar al modelo <span class="math inline">\(MA(2)\)</span> podemos observar que los residuales están cercanos a la normalidad excepto para algunos valores extremos en la cola. Por último, realizamos la prueba de Shapiro-Wilk, la cual nos da un <span class="math inline">\(p\)</span>-valor de 0.001, lo que indica que los residuales no son normales. Por lo tanto, de nuevo, este modelo parece también ajustarse bien salvo que debemos usar una distribución con una cola más pesada que la distribución normal.</p>
<hr />
<p>Como explicamos previamente, debemos tener cuidado con sobreajustar un modelo; no siempre es el caso que más es mejor. Sobreajustar nos lleva a estimadores menos preciso, y agregar más parámetros puede ajustar mejor los datos pero puede llevar a malas predicciones. Este resultado se ilustra en el ejemplo siguiente.</p>

<div class="example">
<p><span id="exm:ejem-problema-sobreajuste" class="example"><strong>Ejemplo 6.27  (Un problema de sobreajuste)  </strong></span>La figura <a href="#fig:fig-sobreajuste-poblacion-usa"><strong>??</strong></a>, muestra la población de los EE.UU., según el censo oficial cada 10 años de 1910 hasta 1990 (puntos azules). Si usamos estos nueve puntos para predecir la población a futuro de los EE.UU. podemos usar un polinomio de grado 8 para ajustar las 9 observaciones; lo cual como se observa en la gráfica es perfecta. El modelo en este caso es</p>
<p><span class="math display">\[x_t=\beta_0+\beta_1t+\beta_2t^2+\cdots+\beta_8t^8+w_t\]</span></p>
El modelo fijado, el cual es graficado hasta el año 2010, (linea continua roja), pasa a través de los 9 puntos. El modelo predice que la población de los EE.UU. estará cercana a cero en el año 2000, y cruzará el cero en algún mes del año 2002, lo cual es falso.
</div>

<p><code>{rfig.cap=&quot;Población de los EE.UU (puntos azules) y modelos ajustado (linea roja continua) de 1910 hasta 2010&quot;,fig-sobreajuste-poblacion-usa} uspop=read.table(&quot;data/USPOP2.txt&quot;, header = TRUE) fit.usp=lm(Pob~t+I(t^2)+I(t^3)+I(t^4)+I(t^5)+I(t^6)+I(t^7)+I(t^8),            data=uspop) plot(uspop,type=&quot;p&quot;,lty=19,col=&quot;blue&quot;,xlim=c(1910,2010)) lines(x = uspop$t, y=predict(fit.usp), col = &quot;red&quot;, lwd = 2)</code> —-</p>
<p>El paso final en el ajuste de modelos es la elección del modelo. Esto es, debemos decidir que modelo mantendremos para la predicción. La técnica más popular es calcular los índices AIC, AICc y SIC (BIC), descritos en las definiciones <a href="modelos-ar.html#def:defi-AIC-2">4.3</a>, <a href="modelos-ar.html#def:defi-AICc">4.4</a> y <a href="modelos-ar.html#def:defi-SIC">4.5</a>.</p>

<div class="example">
<span id="exm:ejem-eleccion-modelo-gnp" class="example"><strong>Ejemplo 6.28  (Elección del modelo para la serie P.N.B. de EE.UU.)  </strong></span>Volviendo al análisis del P.N.B. de EE.UU., visto en los ejemplos <a href="modelos-arma.html#exm:ejem-gnp-data">6.25</a> y <a href="modelos-arma.html#exm:ejem-diagnostico-tasa-crecimiento-gnp">6.26</a>, recordemos que los modelos son AR(1), MA(2) y ARIMA(1,1,2). Para escoger el modelo final, comparemos los valores del AIC, AICc y SIC para los 3 modelos
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n=<span class="kw">length</span>(gnpgr)
<span class="co"># Modelos AR</span>
kar=<span class="kw">length</span>(gnpgr.ar<span class="op">$</span>coef)
sar=gnpgr.ar<span class="op">$</span>sigma2
<span class="co"># Modelos MA</span>
kma=<span class="kw">length</span>(gnpgr.ma2<span class="op">$</span>coef)
sma=gnpgr.ma2<span class="op">$</span>sigma2
<span class="co"># Modelo ARIMA</span>
karima=<span class="kw">length</span>(gnpgr.arima<span class="op">$</span>coef)
sarima=gnpgr.arima<span class="op">$</span>sigma2
<span class="co"># AIC</span>
<span class="kw">log</span>(sar)<span class="op">+</span>(n<span class="op">+</span><span class="dv">2</span><span class="op">*</span>kar)<span class="op">/</span>n</code></pre></div>
<pre><code>## [1] -8.294</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">log</span>(sma)<span class="op">+</span>(n<span class="op">+</span><span class="dv">2</span><span class="op">*</span>kma)<span class="op">/</span>n</code></pre></div>
<pre><code>## [1] -8.298</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">log</span>(sarima)<span class="op">+</span>(n<span class="op">+</span><span class="dv">2</span><span class="op">*</span>karima)<span class="op">/</span>n</code></pre></div>
<pre><code>## [1] -8.285</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># AICc</span>
<span class="kw">log</span>(sar)<span class="op">+</span>(n<span class="op">+</span>kar)<span class="op">/</span>(n<span class="op">-</span>kar<span class="op">-</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] -8.285</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">log</span>(sma)<span class="op">+</span>(n<span class="op">+</span>kma)<span class="op">/</span>(n<span class="op">-</span>kma<span class="op">-</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] -8.288</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">log</span>(sarima)<span class="op">+</span>(n<span class="op">+</span>karima)<span class="op">/</span>(n<span class="op">-</span>karima<span class="op">-</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] -8.275</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># BIC (SIC)</span>
<span class="kw">log</span>(sar)<span class="op">+</span>kar<span class="op">*</span><span class="kw">log</span>(n)<span class="op">/</span>n</code></pre></div>
<pre><code>## [1] -9.264</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">log</span>(sma)<span class="op">+</span>kma<span class="op">*</span><span class="kw">log</span>(n)<span class="op">/</span>n</code></pre></div>
<pre><code>## [1] -9.252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">log</span>(sarima)<span class="op">+</span>karima<span class="op">*</span><span class="kw">log</span>(n)<span class="op">/</span>n</code></pre></div>
<pre><code>## [1] -9.239</code></pre>
<hr />
<h2 id="modelos-sarima"><span class="header-section-number">6.2</span> Modelos SARIMA</h2>
<p>En esta sección vamos a introducir diversas modificaciones a los modelos ARIMA para que se ajusten a comportamiento estacional y no-estacionario. A menudo, la dependencia del pasado tiende a ocurrir más fuertemente en múltiplos de algún paso estacinal <span class="math inline">\(s\)</span> oculto. Por ejemplo, con datos económicos mensuales, existe una fuerte componente anual con pasos que son múltiplos de <span class="math inline">\(s=12\)</span>, debido a la fuerte conexión de todas las actividades al calendario anual. Los datos tomados trimestralmente exhibirán un período repetitivo para <span class="math inline">\(s=4\)</span>. Los fenómenos naturales tales como temperatura, lluvia, etc., también presentan una fuerte componente correspondiente a la estación del año. Por consiguiente, la variabilidad natural de muchos fenómenos físicos, biológicos y procesos económicos tienden a comportarse según las fluctuaciones estacionales. Debido a esto, es apropiado introducir polinomios autorregresivo de promedio móvil que se identifiquen con los rezagos estacionales.</p>

<div class="definition">
<p><span id="def:defi-modelo-sarma" class="definition"><strong>Definición 6.7  </strong></span>El <strong>modelo autorregresivo de promedio móvil estacional puro</strong> denotado <span class="math inline">\(ARMA(P,Q)_s\)</span> tiene la forma</p>
<span class="math display" id="eq:eq-modelo-sarma">\[\begin{equation}
\Phi_P(B^s)x_t=\Theta_Q(B^s)w_t,
\tag{6.121}
\end{equation}\]</span>
<p>donde los operadores</p>
<span class="math display" id="eq:eq-operador-AR-estacional">\[\begin{equation}
\Phi_P(B^s) = 1-\Phi_1B^s-\Phi_2B^{2s}-\cdots-\Phi_PB^{Ps}
\tag{6.122}
\end{equation}\]</span>
<p>y</p>
<span class="math display" id="eq:eq-operador-MA-estacional">\[\begin{equation}
\Theta_Q(B^s) = 1+\Theta_1B^s+\Theta_2B^{2s}+\cdots+\Theta_QB^{Qs},
\tag{6.123}
\end{equation}\]</span>
<p>son los <em>operadores autorreggresivo estacional</em> y de <em>promedio móvil estacional</em> de ordenes <span class="math inline">\(P\)</span> y <span class="math inline">\(Q\)</span> respectivamente, con período estacional <span class="math inline">\(s\)</span>.</p>
Análogo a las propiedades de los modelos <span class="math inline">\(ARMA\)</span> no-estacionales, el modelo <span class="math inline">\(ARMA(P,Q)_s\)</span> puro es <em>causal</em> sólo cuando las raíces de <span class="math inline">\(\Phi_P(z^s)\)</span> están fuera del círculo unitario y es <em>invertible</em> cuando las raíces de <span class="math inline">\(\Theta_Q(z^s)\)</span> están fuera del círculo unitario.
</div>

<hr />

<div class="example">
<p><span id="exm:ejem-serie-AR-estacional" class="example"><strong>Ejemplo 6.29  (Una serie AR estacional)  </strong></span>Una serie autorregresiva estacional de primer orden que podría durar meses la podemos escribir como</p>
<p><span class="math display">\[(1-\Phi B^{12})x_t=w_t\]</span></p>
<p>o</p>
<p><span class="math display">\[x_t=\phi x_{t-12}+w_t.\]</span></p>
<p>Este modelo exhibe la serie <span class="math inline">\(x_t\)</span> en términos de saltos o rezagos múltiplos del periodo estacional anual <span class="math inline">\(s=12\)</span> meses.</p>
<p>De la forma anterior se desprende cláramente que la estimación y el pronóstico para tal proceso sólo implica modificaciones directas del cso de rezago unitario que ya tratamos. En particular, la condición causal requiere <span class="math inline">\(|\Phi|&lt;1\)</span>.</p>
Simulamos 3 años de datos de este modelo con <span class="math inline">\(\Phi=0.9\)</span> y mostramos las ACF y PACF teóricas del modelo. Véase la figura <a href="modelos-arma.html#fig:fig-modelo-AR1-estacional">6.18</a>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">666</span>)
phi=<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">11</span>),<span class="fl">0.9</span>)
sAR=<span class="kw">arima.sim</span>(<span class="kw">list</span>(<span class="dt">order=</span><span class="kw">c</span>(<span class="dv">12</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="dt">ar=</span>phi),<span class="dt">n=</span><span class="dv">37</span>)
sAR =<span class="st"> </span><span class="kw">ts</span>(sAR, <span class="dt">freq=</span><span class="dv">12</span>)
<span class="kw">layout</span>(<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>, <span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">3</span>), <span class="dt">nc=</span><span class="dv">2</span>))
<span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>), <span class="dt">mgp=</span><span class="kw">c</span>(<span class="fl">1.6</span>,.<span class="dv">6</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(sAR, <span class="dt">axes=</span><span class="ot">FALSE</span>, <span class="dt">main=</span><span class="st">&#39;Serie AR(1) estacional&#39;</span>, <span class="dt">xlab=</span><span class="st">&quot;años&quot;</span>, <span class="dt">type=</span><span class="st">&#39;c&#39;</span>)
Months =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;E&quot;</span>,<span class="st">&quot;F&quot;</span>,<span class="st">&quot;M&quot;</span>,<span class="st">&quot;A&quot;</span>,<span class="st">&quot;M&quot;</span>,<span class="st">&quot;J&quot;</span>,<span class="st">&quot;J&quot;</span>,<span class="st">&quot;A&quot;</span>,<span class="st">&quot;S&quot;</span>,<span class="st">&quot;O&quot;</span>,<span class="st">&quot;N&quot;</span>,<span class="st">&quot;D&quot;</span>)
<span class="kw">points</span>(sAR, <span class="dt">pch=</span>Months, <span class="dt">cex=</span><span class="fl">1.25</span>, <span class="dt">font=</span><span class="dv">4</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)
<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>); <span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="kw">gray</span>(.<span class="dv">7</span>))
<span class="kw">axis</span>(<span class="dv">2</span>); <span class="kw">box</span>()
ACF =<span class="st"> </span><span class="kw">ARMAacf</span>(<span class="dt">ar=</span>phi, <span class="dt">ma=</span><span class="dv">0</span>, <span class="dv">100</span>)
PACF =<span class="st"> </span><span class="kw">ARMAacf</span>(<span class="dt">ar=</span>phi, <span class="dt">ma=</span><span class="dv">0</span>, <span class="dv">100</span>, <span class="dt">pacf=</span><span class="ot">TRUE</span>)
<span class="kw">plot</span>(ACF,<span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Rezago&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>.<span class="dv">1</span>,<span class="dv">1</span>)); <span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>)
<span class="kw">plot</span>(PACF, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Rezago&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>.<span class="dv">1</span>,<span class="dv">1</span>)); <span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-modelo-AR1-estacional"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-modelo-AR1-estacional-1.svg" alt="Datos generados de un modelo estaconal AR(1), con s=12 y las funciones ACF y PACF del modelo x_t=0.9x_{t-12}+w_t"  />
<p class="caption">
Figura 6.18: Datos generados de un modelo estaconal AR(1), con s=12 y las funciones ACF y PACF del modelo x_t=0.9x_{t-12}+w_t
</p>
</div>
<hr />
<p>Para un modelo <span class="math inline">\(MA(1)\)</span> estacional con <span class="math inline">\(s=12\)</span>, <span class="math inline">\(x_t=w_t+\Theta w_{t-12}\)</span>, es fácil verificar que</p>
<span class="math display">\[\begin{eqnarray*}
  \gamma(0) &amp;=&amp; (1+\Theta^2)\sigma^2 \\
  \gamma(\pm12) &amp;=&amp; \Theta\sigma^2 \\
  \gamma(h) &amp;=&amp; 0 \text{, cualquier otro caso.}
\end{eqnarray*}\]</span>
<p>Entonces, la única correlación no cero, aparte del paso 0 es</p>
<p><span class="math display">\[\rho(\pm12) = \Theta/(1+\Theta^2).\]</span></p>
<p>Para un modelo <span class="math inline">\(AR(1)\)</span> estacional con <span class="math inline">\(s=12\)</span>, usando las técnicas para el modelo <span class="math inline">\(AR(1)\)</span> no-estacional, tenemos</p>
<span class="math display">\[\begin{eqnarray*}
  \gamma(0) &amp;=&amp; \sigma^2/(1-\Phi^2) \\
  \gamma(\pm12) &amp;=&amp; \sigma^2\Phi^k/(1-\Phi^2), k=1,2,\ldots \\
  \gamma(h) &amp;=&amp; 0 \text{, cualquier otro caso.}
\end{eqnarray*}\]</span>
<p>En este caso, las únicas correlaciones no cero son</p>
<p><span class="math display">\[\rho(\pm12) = \Phi^k, k=0,1,2,\ldots.\]</span> Estos resultados se pueden verificar usando el resultado general</p>
<p><span class="math display">\[\gamma(h) = \Phi\gamma(h-12)\text{, para }h\geq1.\]</span> Por ejemplo, cuando <span class="math inline">\(h=1\)</span>, <span class="math inline">\(\gamma(1)=\Phi\gamma(11)\)</span>, pero para <span class="math inline">\(h=11\)</span>, se tiene que <span class="math inline">\(\gamma(11)=\Phi\gamma(1)\)</span>, lo que implica que <span class="math inline">\(\gamma(1)=\gamma(11)=0\)</span>. Adicional a estos resultados, la PACF tiene extensión análoga del modelo no-estacional al estacional. Estos resultados se observan en la figura <a href="modelos-arma.html#fig:fig-modelo-AR1-estacional">6.18</a>.</p>
<p>Como un criterio de diagnóstico inicial, podemos usar las propiedades de una serie autorregresiva de promedio móvil estacional puro que se muestran en la tabla siguiente. Estas propiedades las podemos considerar como una generalización de las propiedades para modelos no estacionales que presentamos en la sección (<em>referencia</em>)</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">AR(P)s</th>
<th align="center">MA(Q)s</th>
<th align="center">ARMA(P,Q)s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ACF*</td>
<td align="center">Cola fuera en</td>
<td align="center">Corte después</td>
<td align="center">Cola fuera en</td>
</tr>
<tr class="even">
<td></td>
<td align="center">paso k, k=1,2,…</td>
<td align="center">de paso Q</td>
<td align="center">paso k</td>
</tr>
<tr class="odd">
<td>PACF*</td>
<td align="center">Corte después</td>
<td align="center">Cola fuera en</td>
<td align="center">Cola fuera en</td>
</tr>
<tr class="even">
<td></td>
<td align="center">de paso P</td>
<td align="center">paso k, k=1,2,…</td>
<td align="center">paso k</td>
</tr>
</tbody>
</table>
<p>*Los valores en paso no-estacional <span class="math inline">\(h\neq k\)</span>, para <span class="math inline">\(k=1,2,\ldots\)</span> son cero.</p>
<p>En general, podemos combinar los operadores no estacionales y estacionales en un solo modelo.</p>

<div class="definition">
<p><span id="def:defi-modelo-sarma-multiplicativo" class="definition"><strong>Definición 6.8  </strong></span>Un modelo <strong>multiplicativo autorregresivo de promedio móvil estacional</strong> denotado por <span class="math inline">\(ARMA(p,q)\times(P,Q)_s\)</span> tiene la forma</p>
<span class="math display" id="eq:eq-sarma-multiplicativo">\[\begin{equation}
\Phi_P(B^s)\phi(B)x_t = \Theta_Q(B^s)\theta(B)w_t
\tag{6.124}
\end{equation}\]</span>
</div>

<hr />
<p>Aunque las propiedades de diagnóstico en la tabla anterior no son estrictamente ciertas para el modelo general mixto, el comportamiento de las ACF y PACF tienden a mostrar patrones aproximados de la forma indicada. De hecho, para modelos mixtos, podemos ver una mezcla de las propiedades listadas en las tablas mencionadas.</p>
<p>Al ajustar tales modelos, nos centraremos primero en los componentes estacionales autorregresivo de promedio móvil estacional, lo que en general nos conduce a resultados más satisfactorios.</p>

<div class="example">
<p><span id="exm:ejem-modelo-estacional-mixto" class="example"><strong>Ejemplo 6.30  (Un modelo estacional mixto)  </strong></span>Consideremos un modelo <span class="math inline">\(ARMA(0,1)\times(1,0)_{12}\)</span></p>
<p><span class="math display">\[x_t = \Phi x_{t-12}+w_t+\theta w_{t-1},\]</span></p>
<p>donde <span class="math inline">\(|\Phi|&lt;1\)</span> y <span class="math inline">\(|\theta|&lt;1\)</span>. Entonces, dado que <span class="math inline">\(x_{t-12}, w_t\)</span> y <span class="math inline">\(w_{t-1}\)</span> son no-correlacionados, y <span class="math inline">\(x_t\)</span> es estacionario <span class="math inline">\(\gamma(0)=\Phi^2\gamma(0)+\sigma_w^2+\theta^2\\sigma_w^2\)</span> o</p>
<p><span class="math display">\[\gamma(0) = \frac{1+\theta^2}{1-\Phi^2}\sigma_w^2.\]</span></p>
<p>Además, multiplicando el modelo por <span class="math inline">\(x_{t-h}, h&gt;0\)</span>, y tomando valor esperado, tenemos <span class="math inline">\(\gamma(1)=\Phi\gamma(11)+\theta\sigma_w^2\)</span>, y <span class="math inline">\(\gamma(h)=\Phi\gamma(h-12)\)</span> para <span class="math inline">\(h\geq2\)</span>. Entonces, la ACF para este modelo es</p>
<span class="math display">\[\begin{eqnarray*}
  \rho(12h) &amp;=&amp; \Phi^h, h=1,2,\ldots\\
  \rho(12h-1) &amp;=&amp; \rho(12h+1) = \frac{\theta}{1+\theta^2}\Phi^h, h=0,1,2,\ldots \\
  \rho(h) &amp;=&amp; 0 \text{, en otro caso.}
\end{eqnarray*}\]</span>
Las ACF y PACF para este modelo, con <span class="math inline">\(\Phi=0.8\)</span> y <span class="math inline">\(\theta=-0.5\)</span> se muestran en la Figura <a href="modelos-arma.html#fig:fig-modelo-sarma-multiplicativo">6.19</a>. Los comandos en <span class="math inline">\(R\)</span> para reproducir la Figura <a href="modelos-arma.html#fig:fig-modelo-sarma-multiplicativo">6.19</a> son los siguientes.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">phi =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">11</span>),.<span class="dv">8</span>)
ACF =<span class="st"> </span><span class="kw">ARMAacf</span>(<span class="dt">ar=</span>phi, <span class="dt">ma=</span><span class="op">-</span>.<span class="dv">5</span>, <span class="dv">50</span>)[<span class="op">-</span><span class="dv">1</span>] <span class="co"># [-1] remueve el rezago 0</span>
PACF =<span class="st"> </span><span class="kw">ARMAacf</span>(<span class="dt">ar=</span>phi, <span class="dt">ma=</span><span class="op">-</span>.<span class="dv">5</span>, <span class="dv">50</span>, <span class="dt">pacf=</span><span class="ot">TRUE</span>)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(ACF, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Rezago&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>.<span class="dv">4</span>,.<span class="dv">8</span>)); <span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>)
<span class="kw">plot</span>(PACF, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Rezago&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>.<span class="dv">4</span>,.<span class="dv">8</span>)); <span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-modelo-sarma-multiplicativo"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-modelo-sarma-multiplicativo-1.svg" alt="ACF y PACF de un modelo ARMA estacional mixto"  />
<p class="caption">
Figura 6.19: ACF y PACF de un modelo ARMA estacional mixto
</p>
</div>
<hr />
<p>La persistencia estacional ocurre cuando el proceso es casi periódico en la temporada. Por ejemplo, con promedios de temperaturas mensuales sobre los años, cada enero será aproximadamente igual, cada febrero será aproximadamente el mismo, y así sucesivamente. En este caso, podemos pensar que la temperatura promedio mensual <span class="math inline">\(x_t\)</span> es modelada como</p>
<p><span class="math display">\[x_t = S_t+w+t,\]</span></p>
<p>donde <span class="math inline">\(S_t\)</span> es una componente estacional que varia poco de un año a otro de acuerdo a un paseo aleatorio</p>
<p><span class="math display">\[S_t = S_{t-12}+v_t.\]</span></p>
<p>En este modelo, <span class="math inline">\(w_t\)</span> y <span class="math inline">\(v_t\)</span> son ruidos blancos no-correlacionados. La tendencia de los datos que sigue este tipo de modelos se exhibe en la ACF muestral que es grande y decae muy lentamente en los rezagos <span class="math inline">\(h=12k\)</span> para <span class="math inline">\(k=1,2,\ldots\)</span>. Si sustraemos el efecto de años sucesivos el uno del otro, encontramos que</p>
<p><span class="math display">\[(1-B^{12})x_t = x_t-x_{t-12} = v_t+w_t-w_{t-12}.\]</span></p>
<p>Este modelo es un modelo <span class="math inline">\(MA(1)_{12}\)</span> estacionario y su ACF tendrá un pico solo en paso 12. En general, la diferenciación estacional puede ser indicada cuando la ACF decae lentamente en múltiplos de algún período estacional <span class="math inline">\(s\)</span>, pero es despreciable entre los períodos.</p>

<div class="definition">
<p><span id="def:defi-diferencia-estacional-D" class="definition"><strong>Definición 6.9  </strong></span>La <strong>diferencia estacional de orden <span class="math inline">\(D\)</span></strong> se define como</p>
<span class="math display" id="eq:eq-diferencia-estacional-D">\[\begin{equation}
\nabla_s^Dx_t=(1-B^2)^Dx_t
\tag{6.125}
\end{equation}\]</span>
</div>

<hr />
<p>Normalmente, <span class="math inline">\(D=1\)</span>, es suficiente para obtener estacionaridad estacional. incorporando estas ideas al modelo general nos lleva a la siguiente definición.</p>

<div class="definition">
<p><span id="def:defi-modelo-sarima" class="definition"><strong>Definición 6.10  </strong></span>Un modelo <strong>autorregresivo integrado de promedio móvil estacional multiplicativo</strong> o modelo <span class="math inline">\(SARIMA\)</span> está dado por</p>
<span class="math display" id="eq:eq-modelo-sarima">\[\begin{equation}
\Phi_P(B^s)\phi(B)\nabla_s^D\nabla^dx_t = \delta+\Theta_Q(B^s)\theta(B)w_t,
\tag{6.126}
\end{equation}\]</span>
donde <span class="math inline">\(w_t\)</span> es un ruido blanco gaussiano. El modelo general es denotado como <span class="math inline">\(ARIMA(p,d,q)\times(P,D,Q)_s\)</span>. Las componentes autorregresiva y de promedio móvil ordinarias son representadas por <span class="math inline">\(\phi(B)\)</span> y <span class="math inline">\(\theta(B)\)</span> de órdenes <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span> respectivamente, y las componentes autorregresivas y de promedio móvil estacionales por <span class="math inline">\(\Phi_P(B^s)\)</span> y <span class="math inline">\(\Theta_Q(B^s)\)</span> de órdenes <span class="math inline">\(P\)</span> y <span class="math inline">\(Q\)</span>, y las componentes de diferencias ordinarias y estacionales <span class="math inline">\(\nabla^d=(1-B)^d\)</span> y <span class="math inline">\(\nabla_s^D=(1-B^s)^D\)</span>.
</div>

<hr />

<div class="example">
<p><span id="exm:ejem-modelo-sarima" class="example"><strong>Ejemplo 6.31  (Un modelo SARIMA)  </strong></span>Consideremos el siguiente modelo, el cual a menudo provee una representación razonable para seires econométricas estacionales y no estacionarias. Mostramos la ecuación para el modelo, denotado por <span class="math inline">\(ARIMA(0,1,1)\times(0,1,1)_{12}\)</span> en la notación de la definición anterior (Definición <a href="modelos-arma.html#def:defi-modelo-sarima">6.10</a>), donde las fluctuaciones estacionales ocurren cada 12 meses. Entonces con <span class="math inline">\(\delta=0\)</span>, el modelo <a href="modelos-arma.html#eq:eq-modelo-sarima">(6.126)</a> llega a ser</p>
<p><span class="math display">\[\nabla_{12}\nabla x_t=\Theta(B^{12})\theta(B)w_t,\]</span></p>
<p>o</p>
<span class="math display" id="eq:eq-modelo-sarima-p12">\[\begin{equation}
(1-B^{12})(1-B)x_t = (1+\Theta B^{12})(1+\theta B)w_t.
\tag{6.127}
\end{equation}\]</span>
<p>Expandiendo ambos lados de <a href="modelos-arma.html#eq:eq-modelo-sarima-p12">(6.127)</a>, obtenemos la representación</p>
<p><span class="math display">\[(1-B-B^{12}+B^{13})x_t = (1+\theta B+\Theta B^{12}+\Theta\theta B^{13})w_t,\]</span></p>
<p>o en la forma de ecuaciones en diferencias</p>
<p><span class="math display">\[x_t = x_{t-1}+x_{t-12}-x_{t-13}+w_t+\theta w_{t-1}+\Theta w_{t-12}+\Theta\theta w_{t-13}.\]</span></p>
Note que la naturaleza multiplicativa del modelo implica que el coeficiente de <span class="math inline">\(w_{t-13}\)</span> es el producto de los coeficientes de <span class="math inline">\(w_t\)</span> y <span class="math inline">\(w_{t-12}\)</span>, en lugar de un parámetro libre. El supuesto del modelo multiplicativo parece funcionar bien con muchos conjuntos de datos de series de tiempo estacionales a la vez que reduce el número de parámetros que debemos estimar.
</div>

<hr />
<p>Seleccionar el modelo apropiado para un conjunto de datos dado entre todos los posibles modelos representados por la ecuación <a href="modelos-arma.html#eq:eq-modelo-sarima">(6.126)</a> es una tarea desalentadora, y generalmente pensamos primero en términos de encontrar operadores de diferencia que producen una serie más o menos estacionaria y luego en términos de encontrar un modelo autorregresivo de promedio móvil simple o un modelo ARMA multiplicativo estacional para adaptarlo a la serie de residuales resultante.</p>
<p>Primero aplicamos operaciones de diferenciación y luego construimos los residuos a partir de una serie de tamaño reducido. A continuación, evaluamos las ACF y PACF de estos residuos. Los picos que aparecen en estas funciones a menudo pueden eliminarse fjando o ajustando una componente autorregresiva o una componente de promedio móvil de acuerdo con las propiedades de las Tablas para las funciones ACF y PACF. Al considerar si el modelo es satisfactorio podemos aplicar las técnicas de diagnóstico discutidas en la Sección <a href="modelos-arma.html#construcción-de-modelos-arima">Construcción de modelos ARIMA</a>.</p>

<div class="example">
<span id="exm:ejem-pasajeros-aereos" class="example"><strong>Ejemplo 6.32  (Pasajeros aéreos)  </strong></span>Consideremos el conjunto de datos de <span class="math inline">\(R\)</span> “AirPassengers”, que son los totales mensuales de pasajeros de lineas aereas internacionales de 1949 a 1960. En la Figura <a href="modelos-arma.html#fig:fig-serie-pasajeros-transformados">6.20</a> mostramos la serie de datos así como 3 transformaciones de los mismos. Primero una transformación logarítmica, luego una diferenciación de un paso sobre esta, y finalmente una diferenciación adicional de orden 12. Las instrucciones en <span class="math inline">\(R\)</span> son:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x =<span class="st"> </span>AirPassengers
lx =<span class="st"> </span><span class="kw">log</span>(x); dlx =<span class="st"> </span><span class="kw">diff</span>(lx); ddlx =<span class="st"> </span><span class="kw">diff</span>(dlx, <span class="dv">12</span>)
<span class="kw">plot.ts</span>(<span class="kw">cbind</span>(x,lx,dlx,ddlx), <span class="dt">main=</span><span class="st">&quot;&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-serie-pasajeros-transformados"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-serie-pasajeros-transformados-1.svg" alt="Serie de tiempo AirPassengers, x, (parte superior), el cual es los totales mensuales de pasajeros de lineas aéreas internacionales de 1949 a 1960; y los datos transformados lx=log x_t (segundo cuadro); dlx=diff(log x_t) (tercer cuadro) y ddlx=diff_12 diff(log x_t) (cuadro inferior)"  />
<p class="caption">
Figura 6.20: Serie de tiempo AirPassengers, x, (parte superior), el cual es los totales mensuales de pasajeros de lineas aéreas internacionales de 1949 a 1960; y los datos transformados lx=log x_t (segundo cuadro); dlx=diff(log x_t) (tercer cuadro) y ddlx=diff_12 diff(log x_t) (cuadro inferior)
</p>
</div>
<p>Observe que la serie original <span class="math inline">\(x\)</span> muestra tendencia y varianza crecientes; en <span class="math inline">\(lx\)</span> están los datos transformados logarítmicamente y en estso la varianza se estabiliza. Luego diferenciamos la serie transformada para eliminar la tendencia, y la guardamos en <span class="math inline">\(dlx\)</span>. Se observa claramente la persistencia estacional (i.e., <span class="math inline">\(dlx_t\approx dlx_{t-12}\)</span>) de modo que aplicamos una diferenciación de orden 12. Los datos transformados parecen ser estacionarios, así que estamos listos para fijar un modelo inicial a los mismos.</p>
<p>Las ACF y PACF muestrales de <span class="math inline">\(ddlx (\nabla_{12}\nabla\log x_t)\)</span> los mostramos en la Figura <a href="modelos-arma.html#fig:fig-acf-pacf-pasajeros">6.21</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">acf2</span>(ddlx,<span class="dv">50</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-acf-pacf-pasajeros"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-acf-pacf-pasajeros-1.svg" alt="ACF y PACF muestrales de ddlx"  />
<p class="caption">
Figura 6.21: ACF y PACF muestrales de ddlx
</p>
</div>
<pre><code>##       [,1]  [,2]  [,3]  [,4] [,5] [,6]  [,7]  [,8]
## ACF  -0.34  0.11 -0.20  0.02 0.06 0.03 -0.06  0.00
## PACF -0.34 -0.01 -0.19 -0.13 0.03 0.03 -0.06 -0.02
##      [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16]
## ACF  0.18 -0.08  0.06 -0.39  0.15 -0.06  0.15 -0.14
## PACF 0.23  0.04  0.05 -0.34 -0.11 -0.08 -0.02 -0.14
##      [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]
## ACF   0.07  0.02 -0.01 -0.12  0.04 -0.09  0.22 -0.02
## PACF  0.03  0.11 -0.01 -0.17  0.13 -0.07  0.14 -0.07
##      [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32]
## ACF   -0.1  0.05 -0.03  0.05 -0.02 -0.05 -0.05  0.20
## PACF  -0.1 -0.01  0.04 -0.09  0.05  0.00 -0.10 -0.02
##      [,33] [,34] [,35] [,36] [,37] [,38] [,39] [,40]
## ACF  -0.12  0.08 -0.15 -0.01  0.05  0.03 -0.02 -0.03
## PACF  0.01 -0.02  0.02 -0.16 -0.03  0.01  0.05 -0.08
##      [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48]
## ACF  -0.07  0.10 -0.09  0.03 -0.04 -0.04  0.11 -0.05
## PACF -0.17  0.07 -0.10 -0.06 -0.03 -0.12 -0.01 -0.05
##      [,49] [,50]
## ACF   0.11 -0.02
## PACF  0.09  0.13</code></pre>
<p><em>Componente estacional</em>: Parece que en la estacionalidad, la ACF se corta en paso <span class="math inline">\(1s\)</span> (<span class="math inline">\(s=12\)</span>), mientras que la PACF se rezaga en pasos <span class="math inline">\(1s,2s,3s,4s,\ldots\)</span>. Estos resultados implican un <span class="math inline">\(SMA(1), P=0,Q=1\)</span>, en la componente estacional (s=12).</p>
<p><em>Componente no-estacional</em>: Inspeccionando las ACf y PACF muestrales en los primeros pasos, parece que ambas colas decaen. Esto sugiere un modelo <span class="math inline">\(ARMA(1,1)\)</span>, dentor de las estaciones, <span class="math inline">\(p=q=1\)</span>.</p>
<p>Entonces podemos empear con el modelo <span class="math inline">\(ARIMA(1,1,1)\times(0,1,1)_{12}\)</span> sobre la serie <span class="math inline">\(lx\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sarima</span>(lx, <span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>, <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">12</span>)</code></pre></div>
<pre><code>## initial  value -3.085211 
## iter   2 value -3.225399
## iter   3 value -3.276697
## iter   4 value -3.276902
## iter   5 value -3.282134
## iter   6 value -3.282524
## iter   7 value -3.282990
## iter   8 value -3.286319
## iter   9 value -3.286413
## iter  10 value -3.288141
## iter  11 value -3.288262
## iter  12 value -3.288394
## iter  13 value -3.288768
## iter  14 value -3.288969
## iter  15 value -3.289089
## iter  16 value -3.289094
## iter  17 value -3.289100
## iter  17 value -3.289100
## iter  17 value -3.289100
## final  value -3.289100 
## converged
## initial  value -3.288388 
## iter   2 value -3.288459
## iter   3 value -3.288530
## iter   4 value -3.288649
## iter   5 value -3.288753
## iter   6 value -3.288781
## iter   7 value -3.288784
## iter   7 value -3.288784
## iter   7 value -3.288784
## final  value -3.288784 
## converged</code></pre>
<p><img src="Serie-de-Tiempo-en-R_files/figure-html/unnamed-chunk-60-1.svg" /><!-- --></p>
<pre><code>## $fit
## 
## Call:
## arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), 
##     include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, 
##         REPORT = 1, reltol = tol))
## 
## Coefficients:
##         ar1     ma1    sma1
##       0.196  -0.578  -0.564
## s.e.  0.247   0.213   0.075
## 
## sigma^2 estimated as 0.00134:  log likelihood = 244.9,  aic = -481.9
## 
## $degrees_of_freedom
## [1] 128
## 
## $ttable
##      Estimate     SE t.value p.value
## ar1    0.1960 0.2475  0.7921  0.4298
## ma1   -0.5784 0.2132 -2.7127  0.0076
## sma1  -0.5643 0.0747 -7.5544  0.0000
## 
## $AIC
## [1] -3.679
## 
## $AICc
## [1] -3.677
## 
## $BIC
## [1] -3.591</code></pre>
<p>Sin embargo, el parámetro <span class="math inline">\(AR\)</span> no es significativo, así que intentamos eliminando un parámetro de la parte dentro de las estaciones. En este caso probaremos con los modelos <span class="math inline">\(ARIMA(0,1,1)\times(0,1,1)_{12}\)</span> y <span class="math inline">\(ARIMA(1,1,0)\times(0,1,1)_{12}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sarima</span>(lx, <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>, <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">12</span>)</code></pre></div>
<pre><code>## initial  value -3.086228 
## iter   2 value -3.267980
## iter   3 value -3.279950
## iter   4 value -3.285996
## iter   5 value -3.289332
## iter   6 value -3.289665
## iter   7 value -3.289672
## iter   8 value -3.289676
## iter   8 value -3.289676
## iter   8 value -3.289676
## final  value -3.289676 
## converged
## initial  value -3.286464 
## iter   2 value -3.286855
## iter   3 value -3.286872
## iter   4 value -3.286874
## iter   4 value -3.286874
## iter   4 value -3.286874
## final  value -3.286874 
## converged</code></pre>
<div class="figure"><span id="fig:fig-residuales-pasajeros"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-residuales-pasajeros-1.svg" alt="Análisis de residuales para el modelo ARIMA(0,1,1)x(0,1,1)_12 ajustado a la serie lx de pasajeros aéreos"  />
<p class="caption">
Figura 6.22: Análisis de residuales para el modelo ARIMA(0,1,1)x(0,1,1)_12 ajustado a la serie lx de pasajeros aéreos
</p>
</div>
<pre><code>## $fit
## 
## Call:
## arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), 
##     include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, 
##         REPORT = 1, reltol = tol))
## 
## Coefficients:
##          ma1    sma1
##       -0.402  -0.557
## s.e.   0.090   0.073
## 
## sigma^2 estimated as 0.00135:  log likelihood = 244.7,  aic = -483.4
## 
## $degrees_of_freedom
## [1] 129
## 
## $ttable
##      Estimate     SE t.value p.value
## ma1   -0.4018 0.0896  -4.482       0
## sma1  -0.5569 0.0731  -7.619       0
## 
## $AIC
## [1] -3.69
## 
## $AICc
## [1] -3.689
## 
## $BIC
## [1] -3.624</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sarima</span>(lx, <span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>, <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">12</span>)</code></pre></div>
<pre><code>## initial  value -3.085211 
## iter   2 value -3.259459
## iter   3 value -3.262637
## iter   4 value -3.275171
## iter   5 value -3.277007
## iter   6 value -3.277205
## iter   7 value -3.277208
## iter   8 value -3.277209
## iter   8 value -3.277209
## iter   8 value -3.277209
## final  value -3.277209 
## converged
## initial  value -3.279535 
## iter   2 value -3.279580
## iter   3 value -3.279586
## iter   3 value -3.279586
## iter   3 value -3.279586
## final  value -3.279586 
## converged</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-61"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/unnamed-chunk-61-1.svg" alt="Análisis de residuales para el modelo ARIMA(1,1,0)x(0,1,1)_12 ajustado a la serie lx de pasajeros aéreos"  />
<p class="caption">
Figura 6.23: Análisis de residuales para el modelo ARIMA(1,1,0)x(0,1,1)_12 ajustado a la serie lx de pasajeros aéreos
</p>
</div>
<pre><code>## $fit
## 
## Call:
## arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), 
##     include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, 
##         REPORT = 1, reltol = tol))
## 
## Coefficients:
##          ar1    sma1
##       -0.340  -0.562
## s.e.   0.082   0.075
## 
## sigma^2 estimated as 0.00137:  log likelihood = 243.7,  aic = -481.5
## 
## $degrees_of_freedom
## [1] 129
## 
## $ttable
##      Estimate     SE t.value p.value
## ar1   -0.3395 0.0822  -4.130   1e-04
## sma1  -0.5619 0.0748  -7.511   0e+00
## 
## $AIC
## [1] -3.675
## 
## $AICc
## [1] -3.675
## 
## $BIC
## [1] -3.61</code></pre>
<p>Todos los criterios de información prefieren el modelo <span class="math inline">\(ARIMA(0,1,1)\times(0,1,1)_{12}\)</span>. En la Figura <a href="modelos-arma.html#fig:fig-residuales-pasajeros">6.22</a> mostramos los diagnósticos para los residuales y excepto para uno o dos datos atípicos, el modelo parece ajustarse bien.</p>
<p>Finalmente, el pronóstico para 12 meses de los datos con la transformación logarítmica lo mostramo en la Figura <a href="modelos-arma.html#fig:fig-pronostico-pasajeros">6.24</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sarima.for</span>(lx, <span class="dv">12</span>, <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>, <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">12</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-pronostico-pasajeros"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-pronostico-pasajeros-1.svg" alt="Pronóstico de 12 meses usando el modelo ARIMA(0,1,1)x(0,1,1)_12 de los datos transformados de pasajeros aéreos"  />
<p class="caption">
Figura 6.24: Pronóstico de 12 meses usando el modelo ARIMA(0,1,1)x(0,1,1)_12 de los datos transformados de pasajeros aéreos
</p>
</div>
<pre><code>## $pred
##        Jan   Feb   Mar   Apr   May   Jun   Jul   Aug
## 1961 6.110 6.054 6.172 6.199 6.233 6.369 6.507 6.503
##        Sep   Oct   Nov   Dec
## 1961 6.325 6.209 6.063 6.168
## 
## $se
##          Jan     Feb     Mar     Apr     May     Jun
## 1961 0.03672 0.04278 0.04809 0.05287 0.05725 0.06132
##          Jul     Aug     Sep     Oct     Nov     Dec
## 1961 0.06513 0.06873 0.07216 0.07543 0.07856 0.08157</code></pre>
<hr />

<h1 id="modelos-arch-y-garch"><span class="header-section-number">6.2</span> Modelos ARCH y GARCH</h1>
<p>Antes de comenzar, es necesario precisar el concepto de volatilidad en el contexto del análisis financiero. Se denomina volatilidad a la tasa relativa a la que un activo experimenta una drástica disminución o aumento de su precio dentro de un período predeterminado de tiempo. La volatilidad se determina mediante el cálculo de la desviación estándar anualizada de la variación diaria del precio. Si el precio de la acción aumenta y disminuye rápidamente durante cortos períodos de tiempo, entonces se dice que tiene una volatilidad alta. Si el precio se mantiene casi siempre en el mismo valor entonces se dice que tiene volatilidad baja. Los inversores evalúan la volatilidad de las acciones antes de tomar una decisión en, la compra de una oferta de acciones nuevas, la adquisición de acciones adicionales de un activo ya presente en una cartera, o en la venta de acciones que actualmente est án en poder del inversionista. La idea detrás de la comprensión del comportamiento de la volatilidad de los activos es organizar las inversiones para obtener el máximo rendimiento con el mínimo de oportunidades de pérdida.</p>
<p>En esta sección se discutirán algunos de los modelos estadísticos y econométricos mas importantes para la modelización de la volatilidad de series de tiempo de rentabilidades de activos. A diferencia del análisis de series de tiempo tradicional, el cual se enfoca principalmente en la modelización del momento condicional de primer orden, los denominados modelos de heterocedasticidad condicional buscan captar la dependencia dentro del momento condicional de segundo orden, en otras palabras, el objetivo ahora es modelizar la volatilidad. La incertidumbre o riesgo constituye uno de los temas de investigación principales en el análisis financiero. Como se mencion´o, la volatilidad es un factor importante en las finanzas puesto que proporciona un método simple para calcular el valor en riesgo de una situación financiera en la gestión de riesgos. Por otra parte, la modelización de la volatilidad de una serie de tiempo puede mejorar la eficiencia en la estimación de parámetros y la exactitud en los intervalos de predicción. En esta sección se discutirán los modelos univariados de la volatilidad entre los que se incluyen el modelo autorregresivo de heterocedasticidad condicional (<em>ARCH</em>) de Engle (1982), el modelo generalizado ARCH (<em>GARCH</em>) de Bollerslev (1986), entre otros.</p>
<p>La volatilidad tiene la particularidad de que no es posible su observación directa. Aún cuando esto no es posible, la volatilidad tiene algunas características que pueden ser observadas en las series de rentabilidad de activos entre los que se pueden destacar,</p>
<ul>
<li><p>Agrupamiento de la volatilidad (cluster). En otras palabras, períodos de volatilidades altas y períodos de volatilidades bajas.</p></li>
<li><p>Evolución continua de la volatilidad en el tiempo.</p></li>
<li><p>Las variaciones de la volatilidad se presentan en un rango fijo, es decir, no diverge al infinito. En términos estadísticos, se puede decir que la volatilidad es a menudo estacionaria.</p></li>
<li><p>La volatilidad parece reaccionar de manera diferente a un incremento elevado de los precios o una disminución sustancial de los precios. Este efecto es conocido con el nombre de apalancamiento o efecto palanca.</p></li>
</ul>
<p>Tales propiedades descritas anteriormente juegan un papel importante en el desarrollo de los modelos usados para caracterizar la volatilidad.</p>
<h2 id="estructura-de-los-modelos"><span class="header-section-number">6.2</span> Estructura de los Modelos</h2>
<p>Como se analizó en secciones previas, más precisamente en la sección de modelos lineales, una serie de tiempo <span class="math inline">\(x_t\)</span> se puede escribir como la suma de dos componentes,</p>
<span class="math display" id="eq:eq-modelos-xt-2-componentes">\[\begin{equation}
x_t=\mu_t+w_t = \mathbb{E}(x_t|\mathcal{F}_{t-1})
\tag{6.128}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\mathcl{F}_{t-1}\)</span> representa la información disponible hasta el tiempo <span class="math inline">\(t-1\)</span>. Usualmente, <span class="math inline">\(\mathcal{F}_{t-1}\)</span> consiste de todas la funciones lineales del pasado de <span class="math inline">\(x_t\)</span>. El objetivo de los proceso descritos por <a href="modelos-arma.html#eq:eq-modelos-xt-2-componentes">(6.128)</a> es la modelización de <span class="math inline">\(\mu_t=\mathbb{E}(x_t|\mathcal{F}_{t-1})\)</span>, con la suposición de que <span class="math inline">\(w_t\)</span> sea un ruido blanco condicionalmente homocedástico, es decir,</p>
<span class="math display" id="eq:eq-ruido-blanco-homocedastico">\[\begin{equation}
\mathbb{E}(w_t^2) = \mathbb{E}(w_t^2|\mathcal{F}_{t-1}) = \sigma_w^2.
\tag{6.129}
\end{equation}\]</span>
<p>Los modelos de heterocedasticidad condicional suponen que el segundo momento condicional depende del tiempo, es decir,</p>
<span class="math display" id="eq:eq-varianza-heterocedastica">\[\begin{equation}
\sigma_t^2=Var(x_t|\mathcal{F}_{t-1})=\mathbb{E}((x_t-\mu_t)^2|\mathcal{F}_{t-1})=\mathbb{E}(w_t^2|\mathcal{F}_{t-1})=h_t,
\tag{6.130}
\end{equation}\]</span>
<p>siendo <span class="math inline">\(h_t\)</span> una función no negtiva, <span class="math inline">\(h_t=h_t(\mathcal{F}_{t-1})\)</span>. A través de este capítulo discutiremos algunas de las posibles funciones para <span class="math inline">\(h_t\)</span>.</p>
<p>La forma en que <span class="math inline">\(h_t\)</span> evoluciona respecto del tiempo distinguirá una forma de otra. Ya que nuestro objetivo esel estudio de modelos que nos permitan caracterizar series de tiempo financieras, consideraremos de forma general que <span class="math inline">\(x_t\)</span> representa la serie de rentabilidades de activos. Así mismo, haremos referencia de <span class="math inline">\(w_t\)</span> como la <em>rentabilidad corregida en media</em> o <em>impulso</em> del activo.</p>
<p>Los modelos de heterocedasticidad condicional los podemos clasificar en dos categorías generales</p>
<ol style="list-style-type: decimal">
<li><p>La primera categoría, agrupa los modelos que usan una función exacta que rige la evolución de <span class="math inline">\(\sigma_t^2=h_t\)</span>.</p></li>
<li><p>La segunda categoría, agrupa los modelos que usan una ecuación estocástica para describir <span class="math inline">\(\sigma_t^2=h_t\)</span>.</p></li>
</ol>
<p>Los modelos GARCH pertenecen a la primera categoría, mientras que los modelos de volatilidad estocástica están en la segunda categoría.</p>
<h2 id="modelos-arch"><span class="header-section-number">6.2</span> Modelos ARCH</h2>
<p>El primer modelo que proporciona un enfoque sistemático para el modelado de la volatilidad es el modelo <em>Autorregresivo de Heterocedasticidad Condicional</em> denotado por sus sigla en inglés <span class="math inline">\(ARCH\)</span> (Autoregressive Conditional Heteroscedasticity), introducido por Engle (1982). un modelo <span class="math inline">\(ARCH(p)\)</span> tiene la forma</p>
<span class="math display" id="eq:eq-modelo-ARCHp">\[\begin{eqnarray}
w_t^2 &amp;=&amp; \nu_t\sqrt{h_t} \nonumber \\
h_t   &amp;=&amp; \alpha_0+\alpha_1w_{t-1}^2+\cdots+\alpha_pw_{t-p}^2.
\tag{6.131}
\end{eqnarray}\]</span>
<p>Donde <span class="math inline">\(\{\nu_t\}\)</span> es una sucesión de variables aleatorias iid con media 0 y varianza 1, <span class="math inline">\(\alpha_0&gt;0,\alpha_p&gt;0\)</span> y <span class="math inline">\(\alpha_i\geq0, i=1,\ldots,p-1\)</span>. La condición de no negatividad sobre los coeficinetes <span class="math inline">\(\alpha_i\)</span> garantizan que la varianza condicional <span class="math inline">\(h_t\)</span> sea positiva.</p>

<div class="remark">
<p> <span class="remark"><em>Nota. </em></span> Algunos autores usan <span class="math inline">\(\sigma_t^2\)</span> para denotar la varianza condicional en la ecuación <a href="#eq:eq-modelos-ARCHp">(<strong>??</strong>)</a> en lugar de <span class="math inline">\(h_t\)</span> tal como lo denotamos. Así pues, el modelo <span class="math inline">\(ARCH(p)\)</span> también lo podemos escirbir de la siguiente manera:</p>
<span class="math display">\[\begin{eqnarray*}
w_t &amp;=&amp; \nu_t\sigma_t \\
\sigma_t^2 &amp;=&amp; \alpha_0+\alpha_1w_{t-1}^2+\cdots+\alpha_pw_{t-p}^2.
\end{eqnarray*}\]</span>
Sin embargo, en lo que sigue y por razones prácticas, usareos la primera notación que describimos en la ecuación <a href="#eq:eq-modelos-ARCHp">(<strong>??</strong>)</a>.
</div>

<p>El modelo <span class="math inline">\(ARCH(P)\)</span> lo podemos escribir como un modelo <span class="math inline">\(AR(p)\)</span> para <span class="math inline">\(w_t^2\)</span>. En efecto,</p>
<span class="math display" id="eq:eq-modelo-AR-wt2">\[\begin{equation}
w_t^2=\alpha_0+\alpha_1w_{t-1}^2+\cdots+\alpha_pw_{t-p}^2+\eta_t,
\tag{6.132}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\eta_t=w_t^2-h_t\)</span>. Recordando la teoría de los modelos <span class="math inline">\(AR\)</span>, si las raíces de la ecuación característica del proceso <span class="math inline">\(AR\)</span> están fuera del círculo unitario, entonces el proceso es estacionario y además podemos calcular la varianza incondicional de <span class="math inline">\(w_t\)</span>, como</p>
<p><span class="math display">\[Var(w_t^2) = \sigma_w^2 = \mathbb{E}(w_t^2) = \frac{\alpha_0}{1-\alpha_1-\cdots-\alpha_p}\]</span></p>
<p>siempre y cuando <span class="math inline">\(1-\alpha_1-\cdots-\alpha_p&gt;0\)</span>. Tomando en cuenta la ecuación <a href="modelos-arma.html#eq:eq-modelo-ARCHp">(6.131)</a>, podemos ver la razón por la cual los modelos <span class="math inline">\(ARCH\)</span> pueden describir el agrupamiento de la volatilidad. El mmodelo establece que la varianza condicional <span class="math inline">\(h_t\)</span> es una función creciente de <span class="math inline">\(w_{t-1}^2\)</span> para <span class="math inline">\(i=1,\ldots,p\)</span>. Por lo tanto, valores grandes de <span class="math inline">\(w_{t-1}\)</span> (en módulo) dan lugar a valores grandes de <span class="math inline">\(h_t\)</span>. Por consiguiente, <span class="math inline">\(w_t\)</span> también tiende a asumirvalores grandes (en módulo).</p>
<p>Además de capturar el agrupamiento de la volatilidad, los modelos <span class="math inline">\(ARCH\)</span> tambie’n reflejan el exceso de kurtosis estándar de las series de rentabilidad. Para estudiar esta y otras propiedades, consideraremos por simplicidad el modelo <span class="math inline">\(ARCH(1)\)</span>, que asume la forma siguiente:</p>
<span class="math display" id="eq:eq-varianza-proceso-periodico" id="eq:eq-funcion-covarianza-proceso-periodico" id="eq:eq-proceso-periodico-general" id="eq:eq-proceso-periodico-2" id="eq:eq-proceso-periodico" id="eq:eq-periodo-serie-tiempo" id="eq:eq-modelo-ARCH1">\[\begin{eqnarray*}
w_t &amp;=&amp; \nu_t\sqrt{h_t} \nonumber\\
h_t &amp;=&amp; \alpha_0+\alpha_1w_{t-1}^2. 
\tag{6.133}
\end{equation}

Entonces, tenemos que 

$$\mathbb{E}(w_t) = \mathbb{E}[\mathbb{E}(w_t|\mathcal{F}_{t-1})] = \mathbb{E}(\sqrt{h_t}\mathbb{E}(\nu_t)) = 0.$$

Por otro lado, suponiendo estacionaridad de la serie, la varianza incondicional de $w_t$ es 

$$\sigma_w^2=\mathbb{E}(w_t^2) = \frac{\alpha_0}{1-\alpha_1},$$

con $0\leq\alpha_1&lt;1$. Suponiendo normalidad en $\nu_t$, tenemos

$$\mathbb{E}(w_t^4|\mathcal{F}_{t-1}) = 3(\alpha_0+\alpha_1w_{t-1}^2)^2,$$

y por lo tanto

$$\mathbb{E}(w_t^4) = \mathbb{E}(\mathbb{E}(w_t^4|\mathcal{F}_{t-1})) = 3\mathbb{E}(\alpha_0^2+2\alpha_0\alpha_1w_{t-1}^2+\alpha_1^2w_{t-1}^4).$$

Entonces si $w_t$ es estacionario de cuarto orden con $\mu_4=\mathbb{E}(w_t^4)$, tenemos que

$$\mu_4 = 3(\alpha_0^2+2\alpha_0\alpha_1Var(w_t)+\alpha_1^2\mu_4) = 3\alpha_0^2\left(1+2\frac{\alpha_1}{1-\alpha_1}\right)+3\alpha_1^2\mu_4.$$

Despejando, obtenemos

$$\mu_4 = \frac{3\alpha_0^2(1+\alpha_1)}{(1-\alpha_1)(1-3\alpha_1^2)}.$$

Con la condición $0\leq\alpha_1^2&lt;\frac{1}{3}$, para asegurar que $\mu_4&gt;0$. or otra parte, la kurtosis incondicional de $w_t$ es 

$$k = \frac{\mathbb{E}(w_t^4)}{[Var(w_t)]^2} = 3\frac{1-\alpha_1^2}{1-3\alpha_1^2} &gt; 3.$$

En esta última ecuación vemos reflejado el exceso de kurtosis de $w_t$.

El modelo $ARCH$ tiene múltiles propiedades que en cierta forma pueden mejorar el modelado de series de tiempo financieras, en epsecial si queremos modeloar o simular la volatilidad.Sin embargo, este modelo como los ya vistos presentan limitaciones a la hora de modelar series de rentabilidad de activos financieros. Es habitual que períodos de rentabilidades negativas sean seguidos por períodos de gran volatilidad. Así, los modelos $ARCH$ no tienen la capacidad de captar esta característica debido a que la volatilidad responde igualmente ante impulsos negativos y positivos, pues dependen del cuadrado de los mismos. 

Por otro lado, las condiciones para la existencia de momentos de orden mayor, implica colocar restricciones muy estrictas sobre los parámetros del modelo. Como ya mencionamos, para un modelo $ARCH(1)$ con momento de cuarto orden finito exigimos que $0\leq\alpha_1^2&lt;1/3$, de modo que para un modelo $ARCH$ de mayor orden las restricciones tienden a complicarse.


### Estimación de un Modelo ARCH(p)

### Predicción con modelos ARCH

## Modelos GARCH

### Estimación de un Modelo GARCH

### Predicción con modelos GARCH



&lt;!--chapter:end:306-Modelos-ARCH-GARCH.Rmd--&gt;

# Análisis Espectral

La representación espectral de un proceso estacionario $x_t$ esencialmente descompone $x_t$ en suma de componentes senosoidales con coeficientes no correlacionados. En relación con esta descomposición existe una correspondiente descomposición en senosoidales de la función de autocovarianza de $x_t$. La descomposición espectral es así una analogía para procesos estocásticos estacionarios de la conocida representación de Fourier para funciones determinísticas. El análisis de procesos estacionarios por medio de su representación espectral es usualmente referido como el análisis en el *dominio de frecuencias* de la serie de tiempo. Este es equivalente al análisis en el *dominio de tiempo* basado en la función de autocovarianza, pero provee una manera alternativa de ver el proceso para el cual en algunas aplicaciones puede ser más significativo. Por ejemplo en el diseño de una estructura sujeta a una fluctuación de carga aleatoria es importante tener cuidado con la presencia en la fuerza de carga de una gran armónica con frecuencia particular para asegurar que la frecuencia en cuestión no sea una frecuencia resonante de la estructura. El punto de vista espectral es particularmente ventajoso en el análisis de procesos estacionarios multivariantes y en el análisis de conjuntos de datos grandes, para los cuales los cálculos numéricos se pueden realizar rápidamente usando la *Transformada Rápida de Fourier (FFT)*.

## Comportamiento Cíclico y Periodicidad

Ya hemos visto la noción de periodicidad en varios ejemplos de los capítulos anteriores. La noción general de periodicidad se puede hacer con más precisión introduciendo algunas terminologías. De interés descriptivo es el período de una serie temporal, definido como el número de puntos en un ciclo, es decir,

\begin{equation}
    T=\frac{1}{\omega}.
\tag{6.134}
\end{equation}

De manera de definir la tasa de cambio a la cual una serie oscila, primero definiremos un ciclo como un periodo completo de una función seno o de coseno sobre un intervalo de tiempo de longitud $2\pi$. Consideremos el siguiente proceso periódico

\begin{equation}
x_t=A\cos(2\pi\omega t+\phi)
\tag{6.135}
\end{equation}

para $t=0,\pm1,\pm2,\ldots$, donde $\omega$ es un índice de frecuencias, definida en ciclos por unidad de tiempo con $A$ la altura o **amplitud** de la función y $\phi$ la **fase** la cual determina el punto de inicio de la función coseno. Podemos introducir una variación aleatoria en esta serie de tiempo haciendo que la *amplitud* o la *fase* varíen aleatoriamente. De esta manera es fácil usar identidad trigonométrica [^nota10] y escribir \@ref(eq:eq-proceso-periodico) como

[^nota10]: $\cos(\alpha\pm\beta)=\cos(\alpha)\cos(\beta)\mp\sin(\alpha)\sin(\beta)$



\begin{equation}
    x_t=U_1\cos(2\pi\omega t)+U_2\sin(2\pi\omega t)
\tag{6.136}
\end{equation}

donde $U_1=A\cos\phi$ y $U_2=-A\sin\phi$ son en general tomados de manera que sean variables aleatorias normalmente distribuidas. En este caso la amplitud es $A=\sqrt{U_1^2+U_2^2}$ y la fase es $\phi=\arctan(-U_2/U_1)$. De este hecho se puede demostrar que si y solo si, en \@ref(eq:eq-proceso-periodico), $A$ y $\phi$ son variables aleatorias independientes, donde $A^2$ es una chi-cuadrado con 2 grados de libertad, y $\phi$ es uniforme en $(-\pi,\pi)$, entonces $U_1$ y $U_2$ son variables aleatorias normal estándar independientes.

Considere una generalización de \@ref(eq:eq-proceso-periodico-2) que nos permita mezclas de series periódicas con multiples frecuencias y amplitudes

\begin{equation}
    x_t=\sum_{k=1}^{q}\left[U_{k1}\cos(2\pi\omega_kt)+U_{k2}\sin(2\pi\omega_kt)\right]
\tag{6.137}
\end{equation}

donde $U_{k1},U_{k2}$ para $k=1,2,\ldots,q$, son variables aleatorias independientes con media cero y varianza $\sigma_k^2$ y las $\omega_k$ son distintas frecuencias. Note que \@ref(eq:eq-proceso-periodico-general) muestra el proceso como una suma de componentes independientes, con varianza $\sigma_k^2$ para frecuencia $\omega_k$. Usando la independencia de $Us$ e identidad trigonométrica, es fácil demostrar que la función de autocovarianza del proceso es

\begin{equation}
 \gamma(h)=\sum_{k=1}^{q}\sigma_k^2\cos(2\pi\omega_kh)
\tag{6.138}
\end{equation}


Note que la función de autocovarianza es la suma de componentes periódicas con pesos proporcionales a la varianza $\sigma_k^2$. Por consiguiente, $x_t$ es un proceso estacionario de media cero con varianza

\begin{equation}
    \gamma(0)=\mathbb{E}(x_t^2)=\sum_{k=1}^{q}\sigma_k^2
\tag{6.139}
\end{equation}

que muestra la variación total como la suma de las varianzas de cada una de las componentes.


\BeginKnitrBlock{example}\iffalse{-91-85-110-97-32-115-101-114-105-101-32-112-101-114-105-243-100-105-99-97-93-}\fi{}&lt;div class=&quot;example&quot;&gt;&lt;span class=&quot;example&quot; id=&quot;exm:ejem-serie-periodica&quot;&gt;&lt;strong&gt;(\#exm:ejem-serie-periodica)  \iffalse (Una serie periódica) \fi{} &lt;/strong&gt;&lt;/span&gt;
La Figura \@ref(fig:fig-componentes-periodicas) muestra un ejemplo de mezcla \@ref(eq:eq-proceso-periodico-general) con $q=3$ construido de la siguiente manera. Primero para $t=1,\ldots,100$ generamos tres series

\begin{eqnarray*}
  x_{t1} &amp;=&amp; 2\cos(2\pi t6/100)+3\sin(2\pi t6/100) \\
  x_{t2} &amp;=&amp; 4\cos(2\pi t10/100)+5\sin(2\pi t10/100) \\
  x_{t3} &amp;=&amp; 6\cos(2\pi t40/100)+7\sin(2\pi t40/100)
\end{eqnarray*}\]</span>
Estas tres series se muestran en la Figura <a href="modelos-arma.html#fig:fig-componentes-periodicas">6.25</a> junto con las correspondientes frecuencias y amplitudes cuadrada. Por ejemplo, la amplitud cuadrada de <span class="math inline">\(x_{t1}\)</span> es <span class="math inline">\(2^3+3^2=13\)</span>. Por consiguiente, los valores máximos y mínimos de la serie <span class="math inline">\(x_{t1}\)</span> están restringidos a <span class="math inline">\(\pm\sqrt{13}=\pm3.61\)</span>. Finalmente construimos la serie <span class="math display">\[x_t=x_{t1}+x_{t2}+x_{t3}\]</span> esta serie también se muestra en la Figura <a href="modelos-arma.html#fig:fig-componentes-periodicas">6.25</a>. Note que la serie <span class="math inline">\(x_t\)</span> parece tener el comportamiento de alguna de las series periódicas vistas en los Capítulos <a href="características-de-series-de-tiempo.html#características-de-series-de-tiempo">Características de series de tiempo</a> y <a href="modelos-de-series-de-tiempo.html#modelos-de-series-de-tiempo">Modelos de series de tiempo</a>. La clasificación sistemática de los componentes esenciales de frecuencia en una serie de tiempo, incluyendo sus contribuciones relativas, constituye uno de los principales objetivos del análisis espectral.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x1=<span class="dv">2</span><span class="op">*</span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span><span class="op">*</span><span class="dv">6</span><span class="op">/</span><span class="dv">100</span>)<span class="op">+</span><span class="dv">3</span><span class="op">*</span><span class="kw">sin</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span><span class="op">*</span><span class="dv">6</span><span class="op">/</span><span class="dv">100</span>) 
x2=<span class="dv">4</span><span class="op">*</span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span><span class="op">*</span><span class="dv">10</span><span class="op">/</span><span class="dv">100</span>)<span class="op">+</span><span class="dv">5</span><span class="op">*</span><span class="kw">sin</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span><span class="op">*</span><span class="dv">10</span><span class="op">/</span><span class="dv">100</span>)
x3=<span class="dv">6</span><span class="op">*</span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span><span class="op">*</span><span class="dv">40</span><span class="op">/</span><span class="dv">100</span>)<span class="op">+</span><span class="dv">7</span><span class="op">*</span><span class="kw">sin</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span><span class="op">*</span><span class="dv">40</span><span class="op">/</span><span class="dv">100</span>)
xt=x1<span class="op">+</span>x2<span class="op">+</span>x3   
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))   
<span class="kw">plot.ts</span>(x1, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>),
        <span class="dt">main=</span><span class="kw">expression</span>(omega<span class="op">==</span><span class="dv">6</span><span class="op">/</span><span class="dv">100</span><span class="op">~</span><span class="er">~~</span>A<span class="op">^</span><span class="dv">2</span><span class="op">==</span><span class="dv">13</span>))
<span class="kw">plot.ts</span>(x2, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>),
        <span class="dt">main=</span><span class="kw">expression</span>(omega<span class="op">==</span><span class="dv">10</span><span class="op">/</span><span class="dv">100</span><span class="op">~</span><span class="er">~~</span>A<span class="op">^</span><span class="dv">2</span><span class="op">==</span><span class="dv">41</span>))
<span class="kw">plot.ts</span>(x3, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>),
        <span class="dt">main=</span><span class="kw">expression</span>(omega<span class="op">==</span><span class="dv">40</span><span class="op">/</span><span class="dv">100</span><span class="op">~</span><span class="er">~~</span>A<span class="op">^</span><span class="dv">2</span><span class="op">==</span><span class="dv">85</span>))
<span class="kw">plot.ts</span>(xt, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>), <span class="dt">main=</span><span class="st">&quot;suma&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-componentes-periodicas"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-componentes-periodicas-1.svg" alt="Componentes periódicas y su suma como se describe en el Ejemplo"  />
<p class="caption">
Figura 6.25: Componentes periódicas y su suma como se describe en el Ejemplo
</p>
</div>
<hr />

<div class="example">
<p><span id="exm:ejem-periodograma-escalado" class="example"><strong>Ejemplo 6.33  (Periodograma escalado para el ejemplo anterior)  </strong></span> En el Ejemplo <a href="características-de-series-de-tiempo.html#exm:ejem-regresion-senal-ruido">2.7</a> introdujimos el periodograma como una manera de descubrir las componentes periódicas de una serie de tiempo. Recuerde que el periodograma escalado está dado por</p>
<span class="math display" id="eq:eq-periodograma-escalado">\[\begin{equation}
    P(j/n)=\left(\frac{2}{n}\sum_{t=1}^{n}x_t\cos(2\pi tj/n)\right)^2+\left(\frac{2}{n}\sum_{t=1}^{n}x_t\sin(2\pi tj/n)\right)^2
\tag{6.140}
\end{equation}\]</span>
<p>y se podia considerar como una medida de la correlación cuadrada de los datos con las oscilaciones senosoidales a frecuencia <span class="math inline">\(\omega_j=j/n\)</span> o <span class="math inline">\(j\)</span> ciclos en <span class="math inline">\(n\)</span> puntos de tiempo.</p>
<p>El periodograma escalado de los datos <span class="math inline">\(x_t\)</span> simulados en el Ejemplo <a href="#exm:ejem-serie-periodica"><strong>??</strong></a> se muestran en la Figura <a href="modelos-arma.html#fig:fig-periodograma-escalado">6.26</a> y claramente se identifican las tres componentes <span class="math inline">\(x_{t1},x_{t2}\)</span> y <span class="math inline">\(x_{t3}\)</span> de <span class="math inline">\(x_t\)</span>. Más aún, los pesos del periodograma escalado mostrados en la Figura <a href="modelos-arma.html#fig:fig-periodograma-escalado">6.26</a> son</p>
<p><span class="math display">\[P(6/100)=13\text{, }P(10/100)=41\text{, }P(40/100)=85\text{ y } P(j/n)=0 \text{ en otro caso.}\]</span></p>
<p>Estos son exactamente las amplitudes al cuadrado de las componentes generadas en el Ejemplo <a href="#exm:ejem-serie-periodica"><strong>??</strong></a>. Este resultado sugiere que el periodograma puede proporcionar una idea de la varianza de los componentes, <a href="modelos-arma.html#eq:eq-varianza-proceso-periodico">(6.139)</a>, de un conjunto real de los datos.</p>
Las instrucciones en R para calcular el Periodograma y graficarlo son:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">P=<span class="kw">abs</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">fft</span>(xt)<span class="op">/</span><span class="dv">100</span>)<span class="op">^</span><span class="dv">2</span>
f=<span class="dv">0</span><span class="op">:</span><span class="dv">50</span><span class="op">/</span><span class="dv">100</span>
<span class="kw">plot</span>(f,P[<span class="dv">1</span><span class="op">:</span><span class="dv">51</span>], <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Frecuencia&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;periodograma&quot;</span>) </code></pre></div>
<div class="figure"><span id="fig:fig-periodograma-escalado"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-periodograma-escalado-1.svg" alt="Periodograma de los datos generados en el Ejemplo ..."  />
<p class="caption">
Figura 6.26: Periodograma de los datos generados en el Ejemplo …
</p>
</div>
<hr />
<p>Si consideramos los datos <span class="math inline">\(x_t\)</span> en el Ejemplo <a href="#exm:ejem-serie-periodica"><strong>??</strong></a> como un color (forma de onda) hecho con colores primarios <span class="math inline">\(x_{t1},x_{t2},x_{t3}\)</span> en varias intensidades (amplitudes), entonces podemos considerar el periodograma como un prisma que descompone el color <span class="math inline">\(x_t\)</span> en sus colores primarios (espectro). Por consiguiente el término <strong>análisis espectral</strong>.</p>
<p>Otro hecho que podemos usar para entender el concepto de periodograma es que para cada muestra <span class="math inline">\(x_1,\ldots,x_n\)</span> de una serie temporal donde <span class="math inline">\(n\)</span> es impar, podemos escribir, exactamente</p>
<span class="math display" id="eq:eq-serie-periodograma-impar">\[\begin{equation}
x_t=a_0 + \sum_{j=1}^{(n-1)/2}\left[a_j\cos(2\pi tj/n) + b_j\sin(2\pi tj/n)\right],
\tag{6.141}
\end{equation}\]</span>
<p>para <span class="math inline">\(t=1,\ldots,n\)</span> y coeficientes convenientemente elegidos. Si <span class="math inline">\(n\)</span> es par, la representación () se puede modificar sumando hasta <span class="math inline">\((n/2-1)\)</span> y añadiendo una componente adicional dada por <span class="math inline">\(a_{n/2}\cos(2\pi t1/2)=a_{n/2}(-1)^t\)</span>. El punto crucial aquí es que <a href="modelos-arma.html#eq:eq-serie-periodograma-impar">(6.141)</a> es exacto para cada muestra. Dado que <a href="modelos-arma.html#eq:eq-proceso-periodico-general">(6.137)</a> se puede pensar como una aproximación de <a href="modelos-arma.html#eq:eq-serie-periodograma-impar">(6.141)</a>, la idea es que muchos de los coeficientes en <a href="modelos-arma.html#eq:eq-serie-periodograma-impar">(6.141)</a> pueden estar cerca de cero. Recuerde del Ejemplo 3.4.5 que</p>
<span class="math display" id="eq:eq-periodograma-simple">\[\begin{equation}
P(j/n) = a_j^2+b_j^2
\tag{6.142}
\end{equation}\]</span>
<p>de modo que el periodograma escalado indica cuales componentes periódicas en <a href="modelos-arma.html#eq:eq-serie-periodograma-impar">(6.141)</a> son grandes y cuales componentes son pequeñas.</p>
</div>
<div id="la-densidad-espectral" class="section level2">
<h2><span class="header-section-number">6.3</span> La Densidad Espectral</h2>
<p>La idea de que una serie de tiempo está formada por componentes periódicos, apareciendo en proporción a sus varianzas subyacentes es fundamental en la representación espectral dada por los siguientes Teoremas:</p>

<div class="theorem">
<p><span id="thm:teo-funcion-hermitiana-no-negativa" class="theorem"><strong>Teorema 6.1  </strong></span>Una función <span class="math inline">\(\gamma(h)\)</span> para <span class="math inline">\(h=0,\pm1,\pm2,\dots\)</span> es Hermitiana no-negativa definida si y solo si se puede expresar como</p>
<span class="math display" id="eq:eq-funcion-hermitiana">\[\begin{equation}
    \gamma(h)=\int_{-1/2}^{1/2}\exp(2\pi i\omega h)dF(\omega)
\tag{6.143}
\end{equation}\]</span>
donde <span class="math inline">\(F(\cdot)\)</span> es monótona no-decreciente. La función <span class="math inline">\(F(\cdot)\)</span> es continua a la derecha, acotada en <span class="math inline">\([-1/2,1/2]\)</span> y únicamente determinada por las condiciones <span class="math inline">\(F(-1/2)=0,F(1/2)=\gamma(0)\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span> Para demostrar el resultado, note primero que si <span class="math inline">\(\gamma(h)\)</span> tiene la representación de arriba</p>
<span class="math display">\[\begin{eqnarray*}
  \sum_{s=1}^{n}\sum_{t=1}^{n}\bar{a}_s\gamma(s-t)a_t &amp;=&amp; \int_{-1/2}^{1/2}\bar{a}_s\gamma(s-t)a_te^{2\pi i\omega(s-t)}dF(\omega) \\
        &amp;=&amp; \int_{-1/2}^{1/2}\left|\sum_{s=1}^{n}a_se^{-2\pi i\omega s}\right|^2dF(\omega) \\
        &amp;=&amp; \geq 0
\end{eqnarray*}\]</span>
<p>y <span class="math inline">\(\gamma(h)\)</span> es no-negativa definida. Recíprocamente, suponga que <span class="math inline">\(\gamma(h)\)</span> es una función no-negativa definida, y definamos la función no-negativa</p>
<span class="math display" id="eq:eq-funcion-no-negativa">\[\begin{eqnarray}
  f_n(\omega) &amp;=&amp; \frac{1}{n}\sum_{s=1}^{n}\sum_{t=1}^{n}e^{-2\pi i\omega s}\gamma(s-t)e^{2\pi i\omega t} \nonumber\\
         &amp;=&amp; \frac{1}{n}\sum_{u=-(n-1)}^{(n-1)}(n-|u|)e^{-2\pi i\omega u}\gamma(u) \tag{6.144} \\
         &amp;=&amp; \geq 0. \nonumber
\end{eqnarray}\]</span>
<p>Ahora, sea <span class="math inline">\(F_n(\omega)\)</span> la función de distribución correspondiente a <span class="math inline">\(f_n(\omega)I_{(-1/2,1/2]}\)</span> donde <span class="math inline">\(I_{(\cdot)}\)</span> denota la función indicatriz del intervalo en el subíndice. Note que <span class="math inline">\(F_n(\omega)=0, \omega\leq-1/2\)</span> y <span class="math inline">\(F_n(\omega)=F_n(1/2)\)</span> para <span class="math inline">\(\omega\geq1/2\)</span>. Entonces</p>
<span class="math display">\[\begin{eqnarray*}
  \int_{-1/2}^{1/2}e^{2\pi i\omega u}dF_n(\omega) &amp;=&amp; \int_{-1/2}^{1/2}e^{2\pi i\omega u}f_n(\omega)d\omega \\
         &amp;=&amp; \begin{cases}
         (1-|u|/n)\gamma(u)&amp;\text{, }|u|&lt;n\\
         0&amp;\text{, en otro caso}
         \end{cases}
\end{eqnarray*}\]</span>
<p>También tenemos</p>
<span class="math display">\[\begin{eqnarray*}
  F_n(1/2) &amp;=&amp; \int_{-1/2}^{1/2}f_n(\omega)d\omega \\
         &amp;=&amp; \int_{-1/2}^{1/2}\sum_{|u|&lt;n}(1-|u|/n)\gamma(u)e^{-2\pi i\omega u}d\omega \\
         &amp;=&amp; \gamma(0).
\end{eqnarray*}\]</span>
<p>Ahora, por el primer teorema de convergencia de Helly <a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>, existe una subsucesión <span class="math inline">\(F_{n_k}\)</span> convergente a <span class="math inline">\(F\)</span> y por el lema de Helly-Bray, esto implica que</p>
<p><span class="math display">\[\int_{-1/2}^{1/2}e^{2\pi i\omega u}dF_{n_k}(\omega)\to\int_{-1/2}^{1/2}e^{2\pi i\omega u}dF(\omega)\]</span></p>
<p>y del lado derecho de la ecuación anterior</p>
<p><span class="math display">\[(1-|u|/n_k)\gamma(u)\to\gamma(u)\]</span></p>
cuando <span class="math inline">\(n_k\to\infty\)</span>, y se obtiene el resultado requerido.
</div>

<hr />
<p>Ahora presentamos una versión del Teorema de Representación Espectral en términos de un proceso estacionario de media cero <span class="math inline">\(x_t\)</span>. Esta versión nos permite pensar en un proceso estacionario como un proceso generado (aproximadamente) por sumas aleatorias de senos y cosenos tal como se describe en <a href="modelos-arma.html#eq:eq-proceso-periodico-general">(6.137)</a>.</p>

<div class="theorem">
<span id="thm:teo-representacion-espectral-proceso-estacionario" class="theorem"><strong>Teorema 6.2  </strong></span>Si <span class="math inline">\(x_t\)</span> es un proceso estacionario de media cero, con distribución espectral <span class="math inline">\(F(\omega)\)</span> como la dada en el Teorema <a href="modelos-arma.html#thm:teo-funcion-hermitiana-no-negativa">6.1</a>, entonces existe un proceso estocástico a valores complejos <span class="math inline">\(z(\omega)\)</span> en el intervalo <span class="math inline">\(\omega\in[-1/2,1/2]\)</span> con incrementos estacionarios no-correlacionados, tal que <span class="math inline">\(x_t\)</span> se puede escribir como la integral estocástica <span class="math display">\[x_t=\int_{-1/2}^{1/2}\exp(-2\pi it\omega)dz(\omega)\]</span> donde, para <span class="math inline">\(-1/2\leq\omega_1\leq\omega_2\leq1/2\)</span> <span class="math display">\[\text{var}\{z(\omega_2)-z(\omega_1)\}=F(\omega_2)-F(\omega_1).\]</span>
</div>

<hr />
<p>Este resultado es muy técnico porque envuelve integración estocástica; es decir, integración respecto a un proceso estocástico. En términos no técnico, el Teorema <a href="modelos-arma.html#thm:teo-representacion-espectral-proceso-estacionario">6.2</a> dice que <a href="modelos-arma.html#eq:eq-proceso-periodico-general">(6.137)</a> es aproximadamente verdadero para cada serie de tiempo estacionaria. En otras palabras, <em>cada serie de tiempo estacionaria se puede pensar, aproximadamente, como una superposición aleatoria de senos y cosenos oscilando a distintas frecuencias.</em></p>
<p>Dado que <a href="modelos-arma.html#eq:eq-proceso-periodico-general">(6.137)</a> es aproximadamente cierta para toda serie de tiempo estacionaria, la siguiente pregunta es si una representación significativa para la función de autocovarianza, como la dada por <a href="modelos-arma.html#eq:eq-funcion-covarianza-proceso-periodico">(6.138)</a>, también existirá. La respuesta es sí, y su representación es dada por el Teorema <a href="modelos-arma.html#thm:teo-funcion-hermitiana-no-negativa">6.1</a>. El siguiente ejemplo, nos ayudará a explicar estos resultados.</p>

<div class="example">
<p><span id="exm:ejem-proceso-estacionario-periodico" class="example"><strong>Ejemplo 6.34  (Un proceso estacionario periódico)  </strong></span>Considere un proceso aleatorio estacionario periódico dado por <a href="modelos-arma.html#eq:eq-proceso-periodico-2">(6.136)</a>, con frecuencia fija <span class="math inline">\(\omega_0\)</span></p>
<p><span class="math display">\[x_t=U_1\cos(2\pi\omega_0t)+U_2\sin(2\pi\omega_0t)\]</span></p>
<p>donde <span class="math inline">\(U_1\)</span> y <span class="math inline">\(U_2\)</span> son variables aleatorias independientes de media cero y varianza igual <span class="math inline">\(\sigma^2\)</span>. El número de periodos de tiempo necesario para que la serie de arriba complete un ciclo es exactamente <span class="math inline">\(1/\omega_0\)</span>, y el proceso hace exactamente <span class="math inline">\(\omega_0\)</span> ciclos por puntos para <span class="math inline">\(t=0,\pm1,\pm2,\ldots\)</span>. Es fácil demostrar que <a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a></p>
<span class="math display">\[\begin{eqnarray*}
  \gamma(h) &amp;=&amp; \sigma^2\cos(2\pi\omega_0h)=\frac{\sigma^2}{2}e^{-2\pi i\omega_0h}+\frac{\sigma^2}{2}e^{2\pi i\omega_0h} \\
         &amp;=&amp; \int_{-1/2}^{1/2}e^{2\pi i\omega h}dF(\omega)
\end{eqnarray*}\]</span>
<p>usando la integración de Riemann-Stieltjes, donde <span class="math inline">\(F(\omega)\)</span> es la función definida por</p>
<p><span class="math display">\[F(\omega)=\begin{cases}
0,&amp;\omega&lt;-\omega_0\\
\sigma^2/2,&amp;-\omega_0\leq\omega&lt;\omega_0\\
\sigma^2,&amp;\omega\geq\omega_0
\end{cases}.\]</span></p>
La función <span class="math inline">\(F(\omega)\)</span> se comporta como una función de distribución acumulada para una variable aleatoria discreta, excepto que <span class="math inline">\(F(\infty)=\sigma^2=\gamma_x(0)\)</span> en vez de uno. De hecho, <span class="math inline">\(F(\omega)\)</span> es una función de distribución acumulada, no una probabilidad, sino más bien de varianza asociada con la frecuencia <span class="math inline">\(\omega_0\)</span> en un análisis de varianza, siendo <span class="math inline">\(F(\infty)\)</span> la varianza total del proceso <span class="math inline">\(x_t\)</span>. Por lo tanto, llamamos a <span class="math inline">\(F(\omega)\)</span> la <em>función de distribución espectral</em>.
</div>

<hr />
<p>El Teorema <a href="modelos-arma.html#thm:teo-funcion-hermitiana-no-negativa">6.1</a> establece que una representación como la dada en el Ejemplo <a href="modelos-arma.html#exm:ejem-proceso-estacionario-periodico">6.34</a> siempre existirá para un proceso estacionario. En particular, si <span class="math inline">\(x_t\)</span> es estacionario con autocovarianza <span class="math inline">\(\gamma(h)=\mathbb{E}[(x_{t+h}-\mu)(x_t-\mu)]\)</span>, entonces existe una única función monótona creciente <span class="math inline">\(F(\omega)\)</span>, llamada la <strong>función de distribución espectral</strong>, que es acotada, con <span class="math inline">\(F(-\infty)=F(-1/2)=0\)</span> y <span class="math inline">\(F(\infty)=F(1/2)=\gamma(0)\)</span> tal que</p>
<span class="math display" id="eq:eq-funcion-distribucion-espectral">\[\begin{equation}
  \gamma(h)=\int_{-1/2}^{1/2}e^{2\pi i\omega h}dF(\omega).
\tag{6.145}
\end{equation}\]</span>
<p>Una situación más importante que usaremos repetidamente es cubierta por el Teorema <a href="modelos-arma.html#thm:teo-densidad-espectral">6.3</a>, donde se muestra que, sujeto a la sumabilidad absoluta de la autocovarianza, la función de distribución espectral es absolutamente continua con <span class="math inline">\(dF(\omega)=f(\omega)d\omega\)</span> y la representación <a href="modelos-arma.html#eq:eq-funcion-distribucion-espectral">(6.145)</a> motiva la propiedad que sigue.</p>

<div class="theorem">
<p><span id="thm:teo-densidad-espectral" class="theorem"><strong>Teorema 6.3  </strong></span>Si <span class="math inline">\(\gamma(h)\)</span> es la función de autocovarianza de un proceso estacionario <span class="math inline">\(x_t\)</span> con</p>
<span class="math display" id="eq:eq-covarianza-convergente">\[\begin{equation}
  \sum_{h=-\infty}^{\infty}|\gamma(h)|&lt;\infty
\tag{6.146}
\end{equation}\]</span>
<p>entonces la densidad espectral de <span class="math inline">\(x_t\)</span> está dada por</p>
<span class="math display" id="eq:eq-densidad-espectral">\[\begin{equation}
  f(\omega)=\sum_{h=-\infty}^{\infty}\gamma(h)e^{-2\pi i\omega h}.
\tag{6.147}
\end{equation}\]</span>
</div>


<div class="proposition">
<p><span id="prp:propie-densidad-espectral" class="proposition"><strong>Proposición 6.10  (La Densidad Espectral)  </strong></span>Si la función de autocovarianza <span class="math inline">\(\gamma(h)\)</span> de un proceso estacionario satisface</p>
<span class="math display" id="eq:eq-covarianza-finita">\[\begin{equation}
  \sum_{h=-\infty}^{\infty}|\gamma(h)|&lt;\infty
\tag{6.148}
\end{equation}\]</span>
<p>entonces esta tiene representación espectral</p>
<span class="math display" id="eq:eq-representacion-covarianza">\[\begin{equation}
  \gamma(h)=\int_{-1/2}^{1/2}e^{2\pi i\omega h}f(\omega)d\omega\text{ para }h=0,\pm1,\pm2,\ldots
\tag{6.149}
\end{equation}\]</span>
<p>como la transformación inversa de la densidad espectral, la cual tiene la representación</p>
<span class="math display" id="eq:eq-densidad-espectral-covarianza">\[\begin{equation}
  f(\omega)=\sum_{h=-\infty}^{\infty}\gamma(h)e^{-2\pi i\omega h}\text{ con }-1/2\leq\omega\leq1/2.
\tag{6.150}
\end{equation}\]</span>
</div>

<p>La densidad espectral definida en la Proposición <a href="modelos-arma.html#prp:propie-densidad-espectral">6.10</a> es análoga a la función de densidad de probabilidad; el hecho de que <span class="math inline">\(\gamma(h)\)</span> es no negativa definida asegura que</p>
<p><span class="math display">\[f(\omega)\geq0\]</span></p>
<p>para todo <span class="math inline">\(\omega\)</span>. Se sigue inmediatamente de <a href="modelos-arma.html#eq:eq-representacion-covarianza">(6.149)</a> y <a href="modelos-arma.html#eq:eq-densidad-espectral-covarianza">(6.150)</a> que</p>
<p><span class="math display">\[f(\omega)=f(-\omega)\]</span></p>
<p>y</p>
<p><span class="math display">\[f(\omega+1)=f(\omega)\]</span></p>
<p>verificando que la densidad espectral es una función par de periodo uno. Debido a que <span class="math inline">\(f(\omega)\)</span> es una función par, normalmente se graficará solo <span class="math inline">\(f(\omega)\)</span> para <span class="math inline">\(\omega\geq0\)</span>.</p>
<p>Adicionalmente, haciendo <span class="math inline">\(h=0\)</span> en <a href="modelos-arma.html#eq:eq-representacion-covarianza">(6.149)</a> se obtiene</p>
<p><span class="math display">\[\gamma(0)=\text{var}(x_t)=\int_{-1/2}^{1/2}f(\omega)d\omega\]</span></p>
<p>lo cual expresa la varianza total como la integral de la densidad espectral sobre todas las frecuencias. Demostraremos luego, que un filtro lineal puede aislar la varianza en ciertos intervalos de frecuencias o bandas.</p>
<p>Análogo a la teoría de probabilidades, <span class="math inline">\(\gamma(h)\)</span> en <a href="modelos-arma.html#eq:eq-representacion-covarianza">(6.149)</a> es la función característica de la densidad espectral <span class="math inline">\(f(\omega)\)</span> en <a href="modelos-arma.html#eq:eq-densidad-espectral-covarianza">(6.150)</a>. Estos hechos deben dejar claro que, cuando la condición de la Proposición <a href="modelos-arma.html#prp:propie-densidad-espectral">6.10</a> es satisfecha, la función de autocovarianza <span class="math inline">\(\gamma(h)\)</span> y la función de densidad espectral <span class="math inline">\(f(\omega)\)</span> contienen la misma información. Esta información, sin embargo, es expresada de distintas maneras. La función de autocovarianza expresa la información en términos de pasos o saltos, mientras que la densidad espectral expresa la misma información en término de ciclos. Algunos de los problemas son más fáciles de trabajar cuando consideramos la información de pasos o saltos y tendemos a manejar los problemas en el dominio del tiempo. Sin embargo, otros problemas son más fáciles de trabajar teniendo en cuenta la información periódica y tendemos a manejar los problemas en el dominio espectral o de frecuencias.</p>
<p>También debemos mencionar, que hasta ahora nos hemos enfocado en la frecuencia <span class="math inline">\(\omega\)</span> expresada en ciclos por puntos de tiempo, en lugar de la más común (en estadística) alternativa <span class="math inline">\(\lambda=2\pi\omega\)</span> que nos da radianes por puntos. Finalmente, la condición de sumabilidad absoluta <a href="modelos-arma.html#eq:eq-covarianza-finita">(6.148)</a> no es satisfecha por <a href="modelos-arma.html#eq:eq-funcion-covarianza-proceso-periodico">(6.138)</a>, el ejemplo que introdujimos para dar las ideas de representación espectral. La condición, sin embargo, es satisfecha para modelos ARMA.</p>
<p>Note que la función de autocovarianza <span class="math inline">\(\gamma(h)\)</span> en <a href="modelos-arma.html#eq:eq-representacion-covarianza">(6.149)</a> y la densidad espectral <span class="math inline">\(f(\omega)\)</span> en <a href="modelos-arma.html#eq:eq-densidad-espectral-covarianza">(6.150)</a> son pares de transformadas de Fourier. En general, tenemos la siguiente definición:</p>

<div class="definition">
<p><span id="def:defi-par-transformadas-fourier" class="definition"><strong>Definición 6.11  </strong></span>Para una función general <span class="math inline">\(\{a_t;t=0,\pm1,\pm2,\ldots\}\)</span> que satisface la condición de sumabilidad absoluta</p>
<span class="math display" id="eq:eq-cond-sumabilidad-absoluta">\[\begin{equation}
  \sum_{t=-\infty}^{\infty}|a_t|&lt;\infty,
\tag{6.151}
\end{equation}\]</span>
<p>definimos el <strong>par de transformadas de Fourier</strong> de la forma</p>
<span class="math display" id="eq:eq-transformada-fourier-A">\[\begin{equation}\label{}
  A(\omega)=\sum_{t=-\infty}^{\infty}a_te^{-2\pi i\omega t}
\tag{6.152}
\end{equation}\]</span>
<p>y</p>
<span class="math display" id="eq:eq-transformada-fourier-a">\[\begin{equation}\label{}
  a_t=\int_{-1/2}^{1/2}A(\omega)e^{2\pi i\omega t}d\omega
\tag{6.153}
\end{equation}\]</span>
</div>

<p>El uso de <a href="modelos-arma.html#eq:eq-representacion-covarianza">(6.149)</a> y <a href="modelos-arma.html#eq:eq-densidad-espectral-covarianza">(6.150)</a> como par de transformadas de Fourier es fundamental en el estudio de procesos estacionarios a tiempo discreto. Bajo la condición de sumabilidad, el par de transformadas de Fourier <a href="modelos-arma.html#eq:eq-representacion-covarianza">(6.149)</a> y <a href="modelos-arma.html#eq:eq-densidad-espectral-covarianza">(6.150)</a> existirá y esta relación es única.</p>
<p>Si <span class="math inline">\(f(\omega)\)</span> y <span class="math inline">\(g(\omega)\)</span> son dos densidades espectrales para lo cual</p>
<span class="math display" id="eq:eq-igualdad-densidades-f-g">\[\begin{equation}
\int_{-1/2}^{1/2}f(\omega)e^{2\pi i\omega h}d\omega=\int_{-1/2}^{1/2}g(\omega)e^{2\pi i\omega h}d\omega
\tag{6.154}
\end{equation}\]</span>
<p>para todo <span class="math inline">\(h=0,\pm1,\pm2,\ldots\)</span>, entonces</p>
<span class="math display" id="eq:eq-igualdad-densidades-f-g-cs">\[\begin{equation}
f(\omega)=g(\omega)
\tag{6.155}
\end{equation}\]</span>
<p>casi siempre.</p>

<div class="example">
<p><span id="exm:ejem-espectro-serie-ruido-blanco" class="example"><strong>Ejemplo 6.35  (Serie de ruido blanco)  </strong></span> Como un ejemplo sencillo, consideremos el espectro de potencias teórica de una sucesión de variables aleatorias no correlacionadas <span class="math inline">\(w_t\)</span> con varianza <span class="math inline">\(\sigma_w^2\)</span>. Dado que la función de autocovarianza fue calculada en <a href="#eq:eq-funcion-autocovarianza-ruido-blanco">(<strong>??</strong>)</a> como <span class="math inline">\(\gamma_w(h)=\sigma_w^2\)</span> para <span class="math inline">\(h=0\)</span> y cero en cualquier otro caso, se sigue de <a href="modelos-arma.html#eq:eq-densidad-espectral-covarianza">(6.150)</a> que</p>
<p><span class="math display">\[f_w(\omega)=\sigma_w^2\]</span></p>
<p>para <span class="math inline">\(-1/2\leq\omega\leq1/2\)</span> con la misma potencia para todas las frecuencias. Esta propiedad se ve en la realización, el cual parece contener todas las diferentes frecuencias en proporciones similares. La figura <a href="modelos-arma.html#fig:fig-espectros-teoricos">6.27</a> (parte superior) muestra la gráfica del espectro de un ruido blanco con <span class="math inline">\(\sigma_w^2=1\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:ejem-espectro-promedio-movil-simple" class="example"><strong>Ejemplo 6.36  (Un promedio móvil simple)  </strong></span> Una serie que no tiene una proporción igual de frecuencias es la serie de ruido blanco suavizada que se muestra en la parte inferior de la primera Figura del Ejemplo <a href="modelos-de-series-de-tiempo.html#exm:ejem-promedio-movil-ruido-blanco">3.4</a>. Específicamente construimos una serie de promedio móvil de tres puntos definida por</p>
<p><span class="math display">\[v_t=\frac{1}{3}(w_{t-1}+w_t+w_{t+1}).\]</span></p>
<p>Es claro de la realización del ejemplo que la serie tiene menos frecuencias altas, calculando su espectro de potencias se verifica este hecho. En el Ejemplo <a href="modelos-de-series-de-tiempo.html#exm:ejem-ACF-MA">3.6</a> calculamos su función de autocovarianza, obteniendo</p>
<p><span class="math display">\[\gamma_v(h)=\frac{\sigma_w^2}{9}(3-|h|)\]</span></p>
<p>para <span class="math inline">\(|h|\leq2\)</span> y <span class="math inline">\(\gamma_v(h)=0\)</span> para <span class="math inline">\(|h|&gt;2\)</span>.</p>
<p>Entonces, usando <a href="modelos-arma.html#eq:eq-densidad-espectral-covarianza">(6.150)</a> nos da</p>
<span class="math display">\[\begin{eqnarray*}
f_v(\omega) &amp;=&amp; \sum_{h=-2}^{2}\gamma_v(h)e^{-2\pi i\omega h} \\
         &amp;=&amp; \frac{\sigma_w^2}{9}(e^{-4\pi i\omega}+e^{4\pi i\omega})+\frac{2\sigma_w^2}{9}(e^{-2\pi i\omega}+e^{2\pi\omega})+\frac{3\sigma_w^2}{9} \\
         &amp;=&amp; \frac{\sigma_w^2}{9}[3+4\cos(2\pi\omega)+2\cos(4\pi\omega)]
\end{eqnarray*}\]</span>
<p>Graficando el espectro para <span class="math inline">\(\sigma_w^2=1\)</span>, como en la Figura <a href="modelos-arma.html#fig:fig-espectros-teoricos">6.27</a>, se muestra que las frecuencias cercanas a cero tiene mayor potencia y las energías más grandes, <span class="math inline">\(\omega&gt;0.2\)</span> tienen menor potencia.</p>
</div>


<div class="example">
<p><span id="exm:ejem-espectro-AR2" class="example"><strong>Ejemplo 6.37  (Una serie autoregresiva de segundo orden)  </strong></span> Consideremos el espectro de una serie AR(2) de la forma</p>
<p><span class="math display">\[x_t-\phi_1x_{t-1}-\phi_2x_{t-2}=w_t\]</span></p>
<p>para el caso especial <span class="math inline">\(\phi_1=1\)</span> y <span class="math inline">\(\phi_2=-0.9\)</span>. Recuerde el Ejemplo <a href="modelos-de-series-de-tiempo.html#exm:ejem-ACF-MA">3.6</a> el cual muestra una realización de este proceso con <span class="math inline">\(\sigma_w^2=1\)</span>. Note que los datos exhiben una fuerte componente periódica de un ciclo cada seis puntos. Primero, calculemos la función de autocovarianza del lado derecho e igualemos este a la autocovarianza de la parte izquierda</p>
<span class="math display">\[\begin{eqnarray*}
  \gamma_w(h) &amp;=&amp; \mathbb{E}[(x_{t+h}-\phi_1x_{t+h-1}-\phi_2x_{t+h-2})(x_t-\phi_1x_{t-1}-\phi_2x_{t-2})] \\
         &amp;=&amp; [1+\phi_1^2+\phi_2^2]\gamma_x(h)+(\phi_1\phi_2-\phi_1)[\gamma_x(h+1)+\gamma_x(h-1)]-\phi_2[\gamma_x(h+2)+\gamma_x(h-2)] \\
         &amp;=&amp; 2.81\gamma_x(h)-1.9[\gamma_x(h+1)+\gamma_x(h-1)]+0.9[\gamma_x(h+2)+\gamma_x(h-2)],
\end{eqnarray*}\]</span>
<p>hemos sustituido los valores de <span class="math inline">\(\phi_1=1\)</span> y <span class="math inline">\(\phi_2=-0.9\)</span> en la ecuación.</p>
<p>Ahora, sustituyendo la representación espectral para <span class="math inline">\(\gamma_x(h)\)</span> en la ecuación anterior, se tiene</p>
<span class="math display">\[\begin{eqnarray*}
  \gamma_w(h) &amp;=&amp; \int_{-1/2}^{1/2}[2.81-1.90(e^{2\pi i\omega}+e^{-2\pi i\omega})+0.90(e^{4\pi i\omega}+e^{-4\pi i\omega})]e^{2\pi i\omega h}f_x(\omega)d\omega \\
         &amp;=&amp; \int_{-1/2}^{1/2}[2.81-3.80\cos(2\pi\omega)+1.80\cos(4\pi\omega)]e^{2\pi i\omega h}f_x(\omega)d\omega.
\end{eqnarray*}\]</span>
<p>Si el espectro del proceso de ruido blanco es <span class="math inline">\(g_w(\omega)\)</span>, la unicidad de la transformada de Fourier nos permite identificar <span class="math display">\[g_w(\omega)=[2.81-3.80\cos(2\pi\omega)+1.80\cos(4\pi\omega)]f_x(\omega).\]</span></p>
<p>Pero, como ya hemos visto, <span class="math inline">\(g_w(\omega)=\sigma_w^2\)</span> de donde se deduce que</p>
<p><span class="math display">\[f_x(\omega)=\frac{\sigma_w^2}{2.81-3.80\cos(2\pi\omega)+1.80\cos(4\pi\omega)}\]</span></p>
<p>es el espectro de la serie autoregresiva. Haciendo <span class="math inline">\(\sigma_w^2=1\)</span> se tiene el espectro <span class="math inline">\(f_x(\omega)\)</span> mostrado en la Figura <a href="modelos-arma.html#fig:fig-espectros-teoricos">6.27</a>, y donde muestra una componente de potencia fuerte alrededor de <span class="math inline">\(\omega=0.16\)</span> ciclos por puntos o un periodo entre seis y siete ciclos por puntos y potencias muy pequeñas en las otras frecuencias. En este caso, modificando la serie de ruido blanco aplicando un operador AR de orden dos ha concentrado la potencia o varianza de la serie resultante en una banda de frecuencia bastante estrecha.</p>
</div>


<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n=<span class="dv">100</span>
sigma2=<span class="dv">1</span>
w=<span class="kw">seq</span>(<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dt">length=</span>n)
<span class="co"># Calculo de las densidades espectrales</span>
fw=<span class="kw">numeric</span>(n)
fv=<span class="kw">numeric</span>(n)
fx=<span class="kw">numeric</span>(n)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n){
  fw[i]=sigma2
  fv[i]=(sigma2<span class="op">/</span><span class="dv">9</span>)<span class="op">*</span>(<span class="dv">3</span><span class="op">+</span><span class="dv">4</span><span class="op">*</span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>w[i])<span class="op">+</span><span class="dv">2</span><span class="op">*</span><span class="kw">cos</span>(<span class="dv">4</span><span class="op">*</span>pi<span class="op">*</span>w[i]))
  fx[i]=sigma2<span class="op">/</span>(<span class="fl">2.81</span><span class="op">-</span><span class="fl">3.80</span><span class="op">*</span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>w[i])<span class="op">+</span><span class="fl">1.80</span><span class="op">*</span><span class="kw">cos</span>(<span class="dv">4</span><span class="op">*</span>pi<span class="op">*</span>w[i]))
}
<span class="co"># Graficos</span>
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">1</span>))
<span class="kw">plot</span>(w,fw,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Ruido blanco&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Potencia&quot;</span>)
<span class="kw">plot</span>(w,fv,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Promedio movil del ruido blanco&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Potencia&quot;</span>)
<span class="kw">plot</span>(w,fx,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">main=</span><span class="st">&quot;AR(2)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Potencia&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Frecuencia&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:fig-espectros-teoricos"></span>
<img src="Serie-de-Tiempo-en-R_files/figure-html/fig-espectros-teoricos-1.svg" alt="Espectros teóricos de un ruido blanco (superior), promedio móvil de ruido blanco (medio) y proceso AR(2) (inferior)"  />
<p class="caption">
Figura 6.27: Espectros teóricos de un ruido blanco (superior), promedio móvil de ruido blanco (medio) y proceso AR(2) (inferior)
</p>
</div>
<hr />
<p>Los ejemplos anteriores han sido dados para motivar el uso de los espectros de potencias para describir las fluctuaciones de la varianza teórica de una serie estacionaria. Es más, la interpretación de la función de densidad espectral como la varianza de la serie de tiempo sobre una banda de frecuencia dada nos da una explicación intuitiva del significado físico. La gráfica de la función <span class="math inline">\(f(\omega)\)</span>sobre el argumento de frecuencia <span class="math inline">\(\omega\)</span> puede ser pensado como un análisis de varianza, en el cual las columnas o bloques efectivos son las frecuencias indexadas por <span class="math inline">\(\omega\)</span>.</p>
</div>
<div id="periodograma-y-transformada-discreta-de-fourier" class="section level2">
<h2><span class="header-section-number">6.4</span> Periodograma y Transformada Discreta de Fourier</h2>
<p>Ahora estamos listos para unir el periodograma, que es el concepto basada en la muestra presentado en la sección [Comportamiento Cíclico y Periodicidad], con la densidad espectral, que es el concepto basado en la población descrito en la sección <a href="modelos-arma.html#la-densidad-espectral">La Densidad Espectral</a>.</p>

<div class="definition">
<p><span id="def:defi-TDF" class="definition"><strong>Definición 6.12  </strong></span>Dado los datos <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span>, definimos la <strong>Transformada Discreta de Fourier (TDF)</strong> como</p>
<span class="math display" id="eq:eq-TDF">\[\begin{equation}
  d(\omega_j)=n^{-1/2}\sum_{t=1}^{n}x_te^{-2\pi i\omega_jt}
\tag{6.156}
\end{equation}\]</span>
para <span class="math inline">\(j=0,1,\ldots,n-1\)</span>, donde las frecuencias <span class="math inline">\(\omega_j=j/n\)</span> son llamadas las <em>frecuencias de Fourier</em> o <em>frecuencias fundamentales</em>.
</div>

<hr />
<p>Si <span class="math inline">\(n\)</span> es un número altamente compuesto (i.e., tiene muchos factores), la TDF se puede calcular usando la Transformada Rápida de Fourier (FFT). A veces es útil explotar el resultado de inversión para TDF que muestra que la transformación lineal es de uno a uno. Para la inversa de TDF, tenemos</p>
<span class="math display" id="eq:eq-inversa-TDF">\[\begin{equation}
  x_t=n^{-1/2}\sum_{j=0}^{n-1}d(\omega_j)e^{2\pi i\omega_jt}
\tag{6.157}
\end{equation}\]</span>
<p>para <span class="math inline">\(t=1,2,\ldots,n\)</span>.</p>

<div class="definition">
<p><span id="def:defi-periodograma" class="definition"><strong>Definición 6.13  </strong></span>Dados los datos <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> definimos el <strong>periodograma</strong> como</p>
<span class="math display" id="eq:eq-periodograma-datos">\[\begin{equation}
  I(\omega_j)=|d(\omega_j)|^2
\tag{6.158}
\end{equation}\]</span>
para <span class="math inline">\(j=0,1,2,\ldots,n-1\)</span>.
</div>

<hr />
<p>Note que <span class="math inline">\(I(0)=n\bar{x}^2\)</span>, donde <span class="math inline">\(\bar{x}\)</span> es la media muestral. Además, dado que <span class="math inline">\(\sum_{t=1}^{n}\exp(-2\pi i\omega_jt)=0\)</span> para <span class="math inline">\(j\neq0\)</span>, <a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> podemos escribir la TDF como</p>
<span class="math display" id="eq:eq-TDF-2">\[\begin{equation}
  d(\omega_j)=n^{-1/2}\sum_{t=1}^{n}(x_t-\bar{x})e^{-2\pi i\omega_jt}
\tag{6.159}
\end{equation}\]</span>
<p>para <span class="math inline">\(j\neq0\)</span>.</p>
<p>Entonces, para <span class="math inline">\(j\neq0\)</span>,</p>
<span class="math display" id="eq:eq-periodograma-acf">\[\begin{eqnarray}
  I(\omega_j)=|d(\omega_j)|^2 &amp;=&amp; n^{-1}\sum_{t=1}^{n}\sum_{s=1}^{n}(x_t-\bar{x})(x_s-\bar{x})e^{-2\pi i\omega_j(t-s)} \nonumber\\
         &amp;=&amp; n^{-1}\sum_{t=1}^{n}\sum_{s=1}^{n}(x_{t+|h|}-\bar{x})(x_t-\bar{x})e^{-2\pi i\omega_jh} \nonumber \\
         &amp;=&amp; \sum_{h=-(n-1)}^{n-1}\hat{\gamma}(h)e^{-2\pi i\omega_jh} \tag{6.160}
\end{eqnarray}\]</span>
<p>donde hemos hecho <span class="math inline">\(h=t-s\)</span> con <span class="math inline">\(\hat{\gamma}(h)\)</span> <a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a>. Recuerde que <span class="math inline">\(P(\omega_j)=(4/n)I(\omega_j)\)</span> donde <span class="math inline">\(P(\omega_j)\)</span> es el periodograma escalado definido en <a href="modelos-arma.html#eq:eq-periodograma-escalado">(6.140)</a>. Por consiguiente, trabajaremos con <span class="math inline">\(I(\omega_j)\)</span> en vez de <span class="math inline">\(P(\omega_j)\)</span>.</p>
<p>A veces es útil trabajar con las partes real e imaginarias de la TDF individualmente, de donde tenemos la siguiente definición:</p>

<div class="definition">
<p><span id="def:defi-transformadas-seno-coseno" class="definition"><strong>Definición 6.14  </strong></span>Dados las datos <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> definimos la <strong>transformada de cosenos</strong> como</p>
<span class="math display" id="eq:eq-transformada-coseno">\[\begin{equation}
  d_c(\omega_j)=n^{-1/2}\sum_{t=1}^{n}x_t\cos(2\pi\omega_jt)
\tag{6.161}
\end{equation}\]</span>
<p>y la <strong>transformada de senos</strong> como</p>
<span class="math display" id="eq:eq-transformada-seno">\[\begin{equation}
  d_s(\omega_j)=n^{-1/2}\sum_{t=1}^{n}x_t\sin(2\pi\omega_jt)
\tag{6.162}
\end{equation}\]</span>
donde <span class="math inline">\(\omega_j=j/n\)</span> para <span class="math inline">\(j=0,1,2,\ldots,n-1\)</span>.
</div>

<hr />
<p>Note que <span class="math inline">\(d(\omega_j)=d_c(\omega_j)-id_s(\omega_j)\)</span> y por lo tanto</p>
<span class="math display" id="eq:eq-periodograma-transf-seno-coseno">\[\begin{equation}
  I(\omega_j)=d_c^2(\omega_j)+d_s^2(\omega_j)
\tag{6.163}
\end{equation}\]</span>

<div class="example">
<p><span id="exm:ejem-ANOVA-espectral" class="example"><strong>Ejemplo 6.38  (ANOVA espectral)  </strong></span>Sea <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> una muestra de tamaño <span class="math inline">\(n\)</span>, donde para simplificar <span class="math inline">\(n\)</span> es impar. Entonces, recordando el Ejemplo <a href="modelos-arma.html#exm:ejem-periodograma-escalado">6.33</a>, se tiene</p>
<span class="math display" id="eq:eq-e4p27">\[\begin{equation}
  x_t=a_0+\sum_{j=1}^{m}[a_j\cos(2\pi\omega_jt)+b_j\sin(2\pi\omega_jt)]
\tag{6.164}
\end{equation}\]</span>
<p>donde <span class="math inline">\(m=(n-1)/2\)</span> es exacto para <span class="math inline">\(t=1,2,\ldots,n\)</span>. En particular, usando la fórmula de regresión multiples, tenemos <span class="math inline">\(a_0=\bar{x}\)</span></p>
<span class="math display">\[\begin{eqnarray*}
  a_j &amp;=&amp; \frac{2}{n}\sum_{t=1}^{n}x_t\cos(2\pi\omega_jt)=\frac{2}{\sqrt{n}}d_c(\omega_j) \\
  b_j &amp;=&amp; \frac{2}{n}\sum_{t=1}^{n}x_t\sin(2\pi\omega_jt)=\frac{2}{\sqrt{n}}d_s(\omega_j)
\end{eqnarray*}\]</span>
<p>Por consiguiente, podemos escribir</p>
<p><span class="math display">\[(x_t\bar{x})=\frac{2}{\sqrt{n}}\sum_{j=1}^{m}[d_c(\omega_j)\cos(2\pi\omega_jt)+d_s(\omega_j)\sin(2\pi\omega_jt)]\]</span></p>
<p>para <span class="math inline">\(t=1,2,\ldots,n\)</span>. Elevando al cuadrado ambos miembros y sumando tenemos <a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></p>
<p><span class="math display">\[\sum_{t=1}^{n}(x_t-\bar{x})^2=2\sum_{j=1}^{m}\left[d_c^2(\omega_j)+d_s^2(\omega_j)\right]=2\sum_{j=1}^{m}I(\omega_j)\]</span></p>
<p>En consecuencia, hemos particionado la suma de cuadrados en componentes armónicas representadas por las frecuencias <span class="math inline">\(\omega_j\)</span> con el periodograma <span class="math inline">\(I(\omega_j)\)</span> siendo la regresión cuadrada media.</p>
<p>Esto nos lleva a la tabla ANOVA</p>
<table>
<thead>
<tr class="header">
<th align="center">Fuente</th>
<th align="center">g.l.</th>
<th align="center">SC</th>
<th align="center">MS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\omega_1\)</span></td>
<td align="center">2</td>
<td align="center"><span class="math inline">\(2I(\omega_1)\)</span></td>
<td align="center"><span class="math inline">\(I(\omega_1)\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\omega_2\)</span></td>
<td align="center">2</td>
<td align="center"><span class="math inline">\(2I(\omega_2)\)</span></td>
<td align="center"><span class="math inline">\(I(\omega_2)\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\omega_m\)</span></td>
<td align="center">2</td>
<td align="center"><span class="math inline">\(2I(\omega_m)\)</span></td>
<td align="center"><span class="math inline">\(I(\omega_m)\)</span></td>
</tr>
<tr class="odd">
<td align="center">Total</td>
<td align="center"><span class="math inline">\(n-1\)</span></td>
<td align="center"><span class="math inline">\(\sum_{t=1}^{n}(x_t-\bar{x})^2\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Esta descomposición significa que si los datos contienen alguna componente periódica fuerte, entonces los valores del periodograma correspondientes a estas frecuencias (o cercano a estas frecuencias) serán grandes. Por otra parte, los valores del periodograma serán pequeños para componentes periódicas no presentes en los datos.</p>
</div>

<hr />
<p>Ahora estamos listos para presentar algunas propiedades de muestras grandes del periodograma. Primero, sea <span class="math inline">\(\mu\)</span> la media de un proceso estacionario <span class="math inline">\(x_t\)</span> con función de autocovarianza absolutamente sumable <span class="math inline">\(\gamma(h)\)</span> y densidad espectral <span class="math inline">\(f(\omega)\)</span>. Podemos usar el mismo argumento como en <a href="modelos-arma.html#eq:eq-periodograma-acf">(6.160)</a> reemplazando <span class="math inline">\(\bar{x}\)</span> por <span class="math inline">\(\mu\)</span> en <a href="modelos-arma.html#eq:eq-TDF-2">(6.159)</a> para escribir</p>
<span class="math display" id="eq:eq-periodograma-dobe-suma">\[\begin{equation}
  I(\omega_j)=n^{-1}\sum_{h=-(n-1)}^{n-1}\sum_{t=1}^{n-|h|}(x_{t+|h|}-\mu)(x_t-\mu)e^{-2\pi i\omega_jh}
\tag{6.165}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\omega_j\)</span> es una frecuencia fundamental no cero. Tomando esperanza en <a href="modelos-arma.html#eq:eq-periodograma-dobe-suma">(6.165)</a> obtenemos</p>
<span class="math display" id="eq:eq-esperanza-periodograma">\[\begin{equation}
  \mathbb{E}[I(\omega_j)]=\sum_{h=-(n-1)}^{n-1}\left(\frac{n-|h|}{n}\right)\gamma(h)e^{-2\pi i\omega_jh}.
\tag{6.166}
\end{equation}\]</span>
<p>Para cada <span class="math inline">\(\omega\neq0\)</span> dado, elegimos una frecuencia fundamental <span class="math inline">\(\omega_{j:n}\to\omega\)</span> cuando <span class="math inline">\(n\to\infty\)</span> <a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a> de lo cual se sigue por <a href="modelos-arma.html#eq:eq-esperanza-periodograma">(6.166)</a> que</p>
<span class="math display" id="eq:eq-convergencia-esperanza-periodograma-densidad">\[\begin{equation}
  \mathbb{E}(I(\omega_{j:n})]\to f(\omega)=\sum_{h=-\infty}^{\infty}\gamma(h)e^{-2\pi  ih\omega}
\tag{6.167}
\end{equation}\]</span>
<p>cuando <span class="math inline">\(n\to\infty\)</span>. <a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a> En otras palabras, bajo la sumabilidad absoluta de <span class="math inline">\(\gamma(h)\)</span>, la densidad espectral es la media a largo plazo del periodograma.</p>
<p>Para examinar la distribución asintótica del periodograma, note que si <span class="math inline">\(x_t\)</span> es una serie de tiempo normal, las transformadas de senos y cosenos serán conjuntamente normal, porque sus combinaciones lineales son variables aleatorias conjuntamente normal <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span>. En este caso, la suposición de que la función de covarianza satisface la condición</p>
<span class="math display" id="eq:eq-conv-absoluta-covarianza">\[\begin{equation}
  \theta=\sum_{h=-\infty}^{\infty}|h||\gamma(h)|&lt;\infty
\tag{6.168}
\end{equation}\]</span>
<p>es suficiente para obtener aproximaciones de muestras grandes simples de la varianza y la covarianza.</p>
<p>Usando el mismo argumento para desarrollar <a href="modelos-arma.html#eq:eq-esperanza-periodograma">(6.166)</a> tenemos</p>
<span class="math display" id="eq:eq-covarianza-seno-seno" id="eq:eq-covarianza-coseno-seno" id="eq:eq-covarianza-coseno-coseno">\[\begin{eqnarray}
  \text{cov}[d_c(\omega_j),d_c(\omega_k)] &amp;=&amp; n^{-1}\sum_{s=1}^{n}\sum_{t=1}^{n}\gamma(s-t)\cos(2\pi\omega_js)\cos(2\pi\omega_kt) \tag{6.169} \\
  \text{cov}[d_c(\omega_j),d_s(\omega_k)] &amp;=&amp; n^{-1}\sum_{s=1}^{n}\sum_{t=1}^{n}\gamma(s-t)\cos(2\pi\omega_js)\sin(2\pi\omega_kt) \tag{6.170} \\
  \text{cov}[d_s(\omega_j),d_s(\omega_k)] &amp;=&amp; n^{-1}\sum_{s=1}^{n}\sum_{t=1}^{n}\gamma(s-t)\sin(2\pi\omega_js)\sin(2\pi\omega_kt) \tag{6.171}
\end{eqnarray}\]</span>
<p>donde los términos de la varianza se obtienen haciendo <span class="math inline">\(\omega_j=\omega_k\)</span> en <a href="modelos-arma.html#eq:eq-covarianza-coseno-coseno">(6.169)</a> y <a href="modelos-arma.html#eq:eq-covarianza-seno-seno">(6.171)</a>.</p>
<p>Se puede demostrar que los términos en <a href="modelos-arma.html#eq:eq-covarianza-coseno-coseno">(6.169)</a> y <a href="modelos-arma.html#eq:eq-covarianza-seno-seno">(6.171)</a> tienen propiedades interesantes bajo la suposición <a href="modelos-arma.html#eq:eq-conv-absoluta-covarianza">(6.168)</a>, por ejemplo, para <span class="math inline">\(\omega_j,\omega_k\neq0\)</span> o 1/2.</p>
<span class="math display" id="eq:eq-cov-seno-seno-2" id="eq:eq-cov-coseno-coseno-2">\[\begin{eqnarray}
  \text{cov}[d_c(\omega_j),d_c(\omega_k)] &amp;=&amp; \begin{cases}f(\omega_j)/2+\epsilon_n, &amp; \omega_j=\omega_k\\
                                                            \epsilon_n,&amp; \omega_j\neq\omega_k \end{cases} \tag{6.172} \\
  \text{cov}[d_s(\omega_j),d_s(\omega_k)] &amp;=&amp; \begin{cases}f(\omega_j)/2+\epsilon_n, &amp; \omega_j=\omega_k\\
                                                            \epsilon_n,&amp; \omega_j\neq\omega_k \end{cases} \tag{6.173}
\end{eqnarray}\]</span>
<p>y</p>
<span class="math display" id="eq:eq-cov-coseno-seno-2">\[\begin{equation}
  \text{cov}[d_c(\omega_j),d_s(\omega_k)] = \epsilon_n
\tag{6.174}
\end{equation}\]</span>
<p>donde el término de error <span class="math inline">\(\epsilon_n\)</span> en la aproximación se puede acotar por</p>
<span class="math display" id="eq:eq-cota-epsilon-n">\[\begin{equation}
  |\epsilon_n|\leq\theta/n
\tag{6.175}
\end{equation}\]</span>
<p>y <span class="math inline">\(\theta\)</span> está dado por <a href="modelos-arma.html#eq:eq-conv-absoluta-covarianza">(6.168)</a>. Si <span class="math inline">\(\omega_j=\omega_k=0\)</span> o 1/2 en <a href="modelos-arma.html#eq:eq-cov-coseno-coseno-2">(6.172)</a> el múltiplo 1/2 desaparece; note que <span class="math inline">\(d_s(0)=d_s(1/2)=0\)</span>, de modo que <a href="modelos-arma.html#eq:eq-cov-seno-seno-2">(6.173)</a> no aplica.</p>

<div class="example">
<p><span id="exm:ejem-cov-seno-coseno-MA" class="example"><strong>Ejemplo 6.39  (Covarianzas de senos y cosenos para un proceso MA)  </strong></span>Para la serie de promedio móvil de tres puntos del Ejemplo <a href="modelos-arma.html#exm:ejem-espectro-promedio-movil-simple">6.36</a>, el espectro teórico se mostraba en la Figura <a href="modelos-arma.html#fig:fig-espectros-teoricos">6.27</a>. Para <span class="math inline">\(n=256\)</span> puntos, la matriz de covarianza teórica del vector</p>
<p><span class="math display">\[\textbf{d}=(d_c(\omega_{26}),d_s(\omega_{26}),d_c(\omega_{27}),d_s(\omega_{27}))^t\]</span></p>
<p>es</p>
<p><span class="math display">\[\text{cov}(\textbf{d})=\left(
                         \begin{array}{cccc}
                            0.3752 &amp; -0.0009 &amp; -0.0022 &amp; -0.0010 \\
                           -0.0009 &amp;  0.3777 &amp; -0.0009 &amp;  0.0003 \\
                           -0.0022 &amp; -0.0009 &amp;  0.3667 &amp; -0.0010 \\
                           -0.0010 &amp;  0.0003 &amp; -0.0010 &amp;  0.3692 \\
                         \end{array}
                       \right)\]</span></p>
<p>Los elementos de la diagonal se pueden comparar con los valores del espectro teórico de 0,7548 para el espectro en frecuencia <span class="math inline">\(\omega_{26}=0.102\)</span> y de 0,7378 para el espectro en <span class="math inline">\(\omega_{27}=0.105\)</span>.</p>
<p>Por consiguiente, las transformadas de senos y cosenos produce variables casi no correlacionadas con varianzas aproximadamente igual a un medio del espectro teórico. Para este caso particular, la cota uniforme es determinada por <span class="math inline">\(\theta=8/9\)</span> obteniéndose <span class="math inline">\(|\epsilon_{256}|\leq0.0035\)</span> para la cota del error de aproximación.</p>
</div>

<hr />
<p>Si <span class="math inline">\(x_t\sim\text{iid}(0,\sigma^2)\)</span>, entonces se sigue de <a href="modelos-arma.html#eq:eq-conv-absoluta-covarianza">(6.168)</a> a <a href="modelos-arma.html#eq:eq-cov-coseno-seno-2">(6.174)</a> y del Teorema Central del Límite <a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> que</p>
<span class="math display" id="eq:eq-convergencia-AN-transf-seno-coseno">\[\begin{equation}
  d_c(\omega_{j:n})\sim AN(0,\sigma^2/2)\text{   y   }d_s(\omega_{j:n})\sim AN(0,\sigma^2/2)
\tag{6.176}
\end{equation}\]</span>
<p>conjunta e independientemente, e independiente de <span class="math inline">\(d_c(\omega_{k:n})\)</span> y <span class="math inline">\(d_s(\omega_{k:n})\)</span> siempre que <span class="math inline">\(\omega_{j:n}\to\omega_1\)</span> y <span class="math inline">\(\omega_{k:n}\to\omega_2\)</span> donde <span class="math inline">\(0&lt;\omega_1\neq\omega_2&lt;1/2\)</span>. Note que en este caso <span class="math inline">\(f(\omega)=\sigma^2\)</span>. En vista de <a href="modelos-arma.html#eq:eq-convergencia-AN-transf-seno-coseno">(6.176)</a> se sigue inmediatamente que cuando <span class="math inline">\(n\to\infty\)</span></p>
<span class="math display" id="eq:eq-conv-distribucion-periodograma">\[\begin{equation}
  \frac{2I(\omega_{j:n})}{\sigma^2}\overset{d}{\to}\chi_2^2\text{   y   }\frac{2I(\omega_{k:n})}{\sigma^2}\overset{d}{\to}\chi_2^2
\tag{6.177}
\end{equation}\]</span>
<p>con <span class="math inline">\(I(\omega_{j:n})\)</span> e <span class="math inline">\(I(\omega_{k:n})\)</span> siendo asintóticamente independientes, donde <span class="math inline">\(\chi^2_{\nu}\)</span> denota una variable aleatoria chi-cuadrado con <span class="math inline">\(\nu\)</span> grados de libertad. Usando el Teorema Central del Límite es bastante fácil extender los resultados del caso iid al caso de procesos lineales.</p>

<div class="proposition">
<p><span id="prp:propie-distrib-ordenadas-periodograma" class="proposition"><strong>Proposición 6.11  (Distribución de las Ordenadas de un Periodograma)  </strong></span>Si</p>
<span class="math display" id="eq:eq-condicion-proceso-MA">\[\begin{equation}
  x_t=\sum_{j=-\infty}^{\infty}\psi_jw_{t-j}\text{,  }\sum_{j=-\infty}^{\infty}|\psi_j|&lt;\infty
\tag{6.178}
\end{equation}\]</span>
<p>donde <span class="math inline">\(w_t\sim\text{iid}(0,\sigma_w^2)\)</span> y <a href="modelos-arma.html#eq:eq-conv-absoluta-covarianza">(6.168)</a> vale, entonces para cada sucesión de <span class="math inline">\(m\)</span> frecuencias distintas <span class="math inline">\(\omega_j\)</span> con <span class="math inline">\(\omega_{j:n}\to\omega_j\)</span></p>
<span class="math display" id="eq:eq-distrib-ordenadas-periodograma">\[\begin{equation}
  \frac{2I(\omega_{j:n})}{f(\omega_j)}\overset{d}{\to}\text{iid}\chi^2_2
\tag{6.179}
\end{equation}\]</span>
siempre que <span class="math inline">\(f(\omega_j)&gt;0\)</span> para <span class="math inline">\(j=1,2,\ldots,m\)</span>.
</div>

<hr />
<p>La distribución resultante en <a href="modelos-arma.html#eq:eq-distrib-ordenadas-periodograma">(6.179)</a> se puede usar para obtener un intervalo de confianza aproximado para el espectro en la manera usual. Sea <span class="math inline">\(\chi^2_{\nu}(\alpha)\)</span> la probabilidad <span class="math inline">\(\alpha\)</span> de cola inferior para la distribución chi-cuadrado con <span class="math inline">\(\nu\)</span> grados de libertad, esto es,</p>
<span class="math display" id="eq:eq-probabilidad-chi-2-cola-inferior">\[\begin{equation}
  P\{\chi^2_{\nu}\leq\chi^2_{\nu}(\alpha)\}=\alpha.
\tag{6.180}
\end{equation}\]</span>
<p>Entonces, un intervalo de confianza aproximado del <span class="math inline">\(100(1-\alpha)\%\)</span> para la función de densidad espectral es de la forma</p>
<span class="math display" id="eq:eq-intervalo-confianza-densidad-espectral">\[\begin{equation}
  \frac{2I(\omega_{j:n})}{\chi^2_2(1-\alpha/2)}\leq f(\omega)\leq\frac{2I(\omega_{j:n})}{\chi^2_2(\alpha/2)}
\tag{6.181}
\end{equation}\]</span>

<div class="example">
<p><span id="exm:ejem-periodograma-SOI" class="example"><strong>Ejemplo 6.40  (Periodograma de SOI y serie de reclutamiento (nuevos peces))  </strong></span>La Figura <a href="#fig:fig-periodograma-SOI"><strong>??</strong></a> muestra el periodograma de las series SOI y nuevos peces.</p>
<p>Note que <span class="math inline">\(\chi^2_2(0.025)=0.0506\)</span> y <span class="math inline">\(\chi^2_2(0.975)=7.3778\)</span>, de allí podemos obtener un intervalo de confianza aproximado del 95% para las frecuencias de interés, en este caso <span class="math inline">\(\omega_j=1/12\)</span>. Para este valor, se tiene <span class="math inline">\(I_S(1/12)=2.6084\)</span>, luego un intervalo de confianza aproximado del 95% para el espectro <span class="math inline">\(f_S(1/12)\)</span> es</p>
<p><span class="math display">\[[2(2.6084)/7.3778; 2(2.6084)/0.0506]=[0.7071;103.0254]\]</span></p>
<p>lo cual es muy amplio para que sea de utilidad, sin embargo ese valor es mayor que cualquier otro valor de la ordenada del periodograma, así podemos decir que este valor es significativo. Por otra parte un intervalo de confianza aproximado del 95% para la otra frecuencia de interés (<span class="math inline">\(\omega_j=1/48\)</span>) para <span class="math inline">\(f_S(1/48)\)</span> es de la forma</p>
<p><span class="math display">\[[2(0.3804)/7.3778; 2(0.3804)/0.0506]=[0.1031; 15.0355]\]</span></p>
<p>el cual también es bastante amplio, pero en este caso no es posible establecer una significancia para el pico espectral.</p>
Los comandos en R para calcular los periodogramas y generar los gráficos son los siguientes:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#soi=scan(&#39;data/soi.txt&#39;)</span>
<span class="co">#rec=scan(&#39;data/recruit.txt&#39;)</span>
<span class="co">#par(mfrow=c(2,1)) </span>
<span class="co">#soi.per=spec.pgram(soi,taper=0,log=&#39;no&#39;)</span>
<span class="co">#abline(v=1/12,lty=&#39;dotted&#39;) </span>
<span class="co">#abline(v=1/48,lty=&#39;dotted&#39;) </span>
<span class="co">#rec.per=spec.pgram(rec,taper=0,log=&#39;no&#39;) </span>
<span class="co">#abline(v=1/12,lty=&#39;dotted&#39;) </span>
<span class="co">#abline(v=1/48,lty=&#39;dotted&#39;)</span></code></pre></div>
<p>Los intervalos de confianza de la serie SOI para el ciclo anual <span class="math inline">\(w=1/12=40/480\)</span> y los posibles ciclos de cuatro años de El Niño con <span class="math inline">\(w=1/48=10/480\)</span> se pueden calcular en Matlab y R con los siguientes comandos:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#li=qchisq(0.975,2) </span>
<span class="co">#ls=qchisq(0.025,2) </span>
<span class="co">#2*soi.per$spec[10]/li</span>
<span class="co">#2*soi.per$spec[10]/ls</span>
<span class="co">#2*soi.per$spec[40]/li </span>
<span class="co">#2*soi.per$spec[40]/ls</span></code></pre></div>
<hr />
</div>
<div id="estimación-espectral-no-paramétrica" class="section level2">
<h2><span class="header-section-number">6.5</span> Estimación Espectral No-paramétrica</h2>
<p>Definamos una banda de frecuencia <span class="math inline">\(\mathcal{B}\)</span> de <span class="math inline">\(L\ll n\)</span> frecuencias fundamentales contiguas centradas alrededor <span class="math inline">\(\omega_j=j/n\)</span> que estén cercanas a la frecuencia de interés <span class="math inline">\(\omega\)</span> como</p>
<span class="math display" id="eq:eq-banda-frecuencia">\[\begin{equation}
  \mathcal{B}=\left\{\omega:\omega_j\frac{m}{n}\leq\omega\leq\omega_j+\frac{m}{n}\right\}
\tag{6.182}
\end{equation}\]</span>
<p>donde</p>
<span class="math display" id="eq:eq-frecuencias-fundamentales">\[\begin{equation}
  L=2m+1
\tag{6.183}
\end{equation}\]</span>
<p>es un número impar, elegido tal que los valores espectrales en el intervalo <span class="math inline">\(\mathcal{B}\)</span></p>
<p><span class="math display">\[f(\omega_j+k/n)\text{, }k=-m,\ldots,0,\ldots,m\]</span></p>
<p>son aproximadamente igual a <span class="math inline">\(f(\omega)\)</span>. Esta estructura se puede desarrollar para un muestra grande. Los valores del espectro en esta banda de frecuencia serán relativamente constantes, así también será un buen estimador para el espectro suavizado que definimos a continuación.</p>
<p>Usando la banda anterior, podemos definir un periodograma suavizado o de media como el promedio de los valores del periodograma, esto es,</p>
<span class="math display" id="eq:eq-periodograma-suavizado">\[\begin{equation}
  \bar{f}(\omega)=\frac{1}{L}\sum_{k=-m}^{m}I(\omega_j+k/n)
\tag{6.184}
\end{equation}\]</span>
<p>como el promedio sobre la banda <span class="math inline">\(\mathcal{B}\)</span>.</p>
<p>Bajo la suposición que la densidad espectral es casi constante en la banda <span class="math inline">\(\mathcal{B}\)</span> y en vista de <a href="modelos-arma.html#eq:eq-distrib-ordenadas-periodograma">(6.179)</a> podemos demostrar que bajo condiciones apropiadas, <a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> para <span class="math inline">\(n\)</span> grande, los periodogramas en <a href="modelos-arma.html#eq:eq-periodograma-suavizado">(6.184)</a> son variables aleatorias distribuidas aproximadamente como variables <span class="math inline">\(f(\omega)\chi^2_2/2\)</span> independientes, para <span class="math inline">\(0&lt;\omega&lt;1/2\)</span>, siempre y cuando mantengamos <span class="math inline">\(L\)</span> bastante pequeño con relación a <span class="math inline">\(n\)</span>. Por consiguiente, bajo estas condiciones, <span class="math inline">\(L\bar{f}(\omega)\)</span> es la suma de <span class="math inline">\(L\)</span> variables aleatorias <span class="math inline">\(f(\omega)\chi^2_2/2\)</span> aproximadamente independientes.</p>
<p>Se sigue que para <span class="math inline">\(n\)</span> grande</p>
<span class="math display" id="eq:eq-distrib-periodograma-suavizado">\[\begin{equation}
  \frac{2L\bar{f}(\omega)}{f(\omega)}\overset{\cdot}{\sim}\chi^2_{2L}
\tag{6.185}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\overset{\cdot}{\sim}\)</span> significa <em>aproximadamente distribuida como</em>.</p>
<p>De esta manera, es razonable llamar a la longitud del intervalo definido por <a href="modelos-arma.html#eq:eq-banda-frecuencia">(6.182)</a></p>
<span class="math display" id="eq:eq-ancho-banda">\[\begin{equation}
  B_w=\frac{L}{n}
\tag{6.186}
\end{equation}\]</span>
<p>el <em>ancho de banda</em>. El ancho de banda en este caso, se refiere al ancho de la banda de frecuencia usada para suavizar el periodograma. El concepto de ancho de banda, sin embargo, se hace más complicado con la introducción de los estimadores espectrales que suavizan con pesos desiguales. Note que <a href="modelos-arma.html#eq:eq-ancho-banda">(6.186)</a> implica que los grados de libertad los podemos expresar como</p>
<span class="math display" id="eq:eq-grados-libertad">\[\begin{equation}
  2L=2B_wn
\tag{6.187}
\end{equation}\]</span>
<p>o dos veces el producto del ancho de banda por tiempo. El resultado <a href="modelos-arma.html#eq:eq-distrib-periodograma-suavizado">(6.185)</a> se puede reordenar para obtener un intervalo de confianza aproximado del <span class="math inline">\(100(1-\alpha)\%\)</span> de la forma</p>
<span class="math display" id="eq:eq-intervalo-confianza-espectro">\[\begin{equation}
  \frac{2L\bar{f}(\omega)}{\chi^2_{2L}(1-\alpha/2)}\leq f(\omega)\leq\frac{2L\bar{f}(\omega)}{\chi^2_{2L}(\alpha/2)}
\tag{6.188}
\end{equation}\]</span>
<p>para el espectro verdadero <span class="math inline">\(f(\omega)\)</span>.</p>
<p>Muchas veces el impacto visual del gráfico de la densidad espectral se puede mejorar, graficando el logaritmo del espectro en vez del espectro. <a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a> Este fenómeno puede ocurrir cuando en algunas regiones del espectro existen picos de interés mucho más pequeños que los de las componentes principales. Para el logaritmo del espectro obtenemos un intervalo de confianza de la forma</p>
<span class="math display" id="eq:eq-intervalo-confianza-log-espectro">\[\begin{equation}
  \left[\ln\bar{f}(\omega)+\ln2L-\ln\chi^2_{2L}(1-\alpha/2),\ln\bar{f}(\omega)+\ln2L-\ln\chi^2_{2L}(\alpha/2)\right].
\tag{6.189}
\end{equation}\]</span>
<p>Podemos realizar también una prueba de hipótesis relativa a la igualdad del espectro usando el hecho de que la distribución resultante <a href="modelos-arma.html#eq:eq-distrib-periodograma-suavizado">(6.185)</a> implica que el radio del espectro basado en una muestra aproximadamente independiente tiene distribución aproximada <span class="math inline">\(F_{2L}^{2L}\)</span>.</p>

<div class="example">
<span id="exm:ejem-periodograma-suavizado-SOI" class="example"><strong>Ejemplo 6.41  (Periodograma suavizado de las series SOI y reclutamiento (nuevos peces))  </strong></span>En la Figura <a href="#fig:fig-periodograma-SOI"><strong>??</strong></a> graficamos los periodogramas para las series SOI y Reclutamiento (nuevos peces). En la gráfica se puede notar una frecuencia baja en el efecto El Niño, lo que sugiere que un suavizado nos permitirá identificar las frecuencias dominantes sobre todos los periodos. La elección del valor de <span class="math inline">\(L=9\)</span> luce razonable para el suavizado. El ancho de banda en este caso es <span class="math inline">\(B_w=9/480=0.01875\)</span> ciclos por meses para el espectro estimado. La Figura <a href="#fig:fig-periodograma-suavizado-SOI"><strong>??</strong></a> muestra los periodogramas suavizados de ambas series. Allí se puede notar, (líneas punteadas) las cuatro frecuencias dominantes, estas son <span class="math inline">\(\omega_j=1/12,2/12,3/12\)</span> y <span class="math inline">\(1/48\)</span>. También puede observar el ancho de banda que es <span class="math inline">\(B_w=0.00541\)</span>.
</div>

<hr />

<div class="example">
<span id="exm:ejem-espectro-altura-olas" class="example"><strong>Ejemplo 6.42  (Serie de Alturas de Olas. Estación 144. ST. PETERSBURG)  </strong></span>La Figura <a href="#fig:fig-periodograma-altura-olas"><strong>??</strong></a> muestra el registro de alturas de olas y el correspondiente periodograma. Las alturas de olas fueron registrados por una boya ubicada en el Golfo de México, cercana a las costa de St. Petersburg, Florida, EE.UU, tomadas el 1ro. enero de 2009 con una frecuencia de muestreo de 1.28Hz. Los comandos en R son:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># SP=matrix(scan(&quot;data/station14401.txt&quot;), byrow=TRUE, ncol=2)</span>
<span class="co"># m&lt;-matrix(c(1,1:3),2,2,byrow=TRUE)</span>
<span class="co"># layout(m)</span>
<span class="co"># plot(SP[,1]/0.78,SP[,2],type=&quot;l&quot;, xlab=&quot;Tiempo (seg)&quot;,ylab=&quot;Alturas (m)&quot;, main=&quot;Altura de olas, Estacion 431, St. Petersburg, FL&quot;)</span>
<span class="co"># I1=spectrum(SP[,2],spans=3,log=&quot;no&quot;, main=&quot;Periodograma suavizado estacion 144&quot;)</span>
<span class="co"># I2=spectrum(SP[,2],log=&quot;no&quot;, main=&quot;Periodograma estacion 144&quot;)</span></code></pre></div>
<hr />

<div class="example">
<span id="exm:ejem-periodograma-terremoto-explosiones" class="example"><strong>Ejemplo 6.43  (Periodogramas para las series de Terremotos y Explosiones)  </strong></span>La Figura <a href="#fig:fig-periodograma-terremoto-explosiones"><strong>??</strong></a> muestra el espectro calculado por separado de las dos fases del terremoto y explosión en la Figura 2.7 del capítulo 2.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># x=matrix(scan(&quot;data/eq5exp6.txt&quot;),ncol=2)</span>
<span class="co"># eqP=x[1:1024,1]; eqS=x[1025:2048,1]</span>
<span class="co"># exP=x[1:1024,2]; exS=x[1025:2048,2]</span>
<span class="co"># par(mfrow=c(2,2))</span>
<span class="co"># eqPs=spectrum(eqP, main=&quot;Espectro del sismo (fase P)&quot;, log=&quot;no&quot;, xlim=c(0,0.25), ylim=c(0,0.04))</span>
<span class="co"># eqSs=spectrum(eqS, main=&quot;Espectro del sismo (fase S)&quot;, log=&quot;no&quot;, xlim=c(0,0.25), ylim=c(0,0.4))</span>
<span class="co"># exPs=spectrum(exP, main=&quot;Espectro de explosiones (fase P)&quot;, log=&quot;no&quot;, xlim=c(0,0.25), ylim=c(0,0.04))</span>
<span class="co"># exSs=spectrum(exS, main=&quot;Espectro de explosiones (fase S)&quot;, log=&quot;no&quot;, xlim=c(0,0.25), ylim=c(0,0.4))</span></code></pre></div>
<hr />
</div>
<div id="procesos-de-incremento-ortogonal-sobre--pipi" class="section level2">
<h2><span class="header-section-number">6.6</span> Procesos de Incremento Ortogonal sobre <span class="math inline">\([-\pi,\pi]\)</span></h2>
<p>Con el fin de dar un significado preciso a la representación espectral <a href="modelos-arma.html#eq:eq-funcion-distribucion-espectral">(6.145)</a> mencionada anteriormente, es necesario introducir el concepto de integración estocástica de una función no-aleatoria con respecto a un proceso de incremento ortogonal <span class="math inline">\(\{Z(\lambda)\}\)</span> .</p>

<div class="definition">
<span id="def:defi-proceso-estacionario-complejo" class="definition"><strong>Definición 6.15  </strong></span>Un proceso <span class="math inline">\(\{x_t\}\)</span> es un <em>proceso estacionario a valores complejos</em> si <span class="math inline">\(\mathbb{E}|x_t^2|&lt;\infty\)</span>, <span class="math inline">\(\mathbb{E}(X_t)\)</span> es independiente de <span class="math inline">\(t\)</span> y <span class="math inline">\(\mathbb{E}(x_{t+h}\bar{x}_t)\)</span> es independiente de <span class="math inline">\(t\)</span>
</div>

<hr />

<div class="definition">
<p><span id="def:defi-autocovarianza-proceso-complejo" class="definition"><strong>Definición 6.16  </strong></span>La función de autocovarianza <span class="math inline">\(\gamma(\cdot)\)</span> de un proceso estacionario a valores complejos <span class="math inline">\(\{x_t\}\)</span> es</p>
<span class="math display" id="eq:eq-autocovarianza-proceso-complejo">\[\begin{equation}
\gamma(h) = \mathbb{E}(x_{t+h}\bar{x}_t) - \mathbb{E}(x_{t+h})\mathbb{E}(\bar{x}_t).
\tag{6.190}
\end{equation}\]</span>
</div>

<hr />

<div class="theorem">
<p><span id="thm:teo-autocovarianza-hermitiana" class="theorem"><strong>Teorema 6.4  </strong></span>Una función <span class="math inline">\(K(\cdot)\)</span> definida sobre los enteros en la función de autocovarianza de una serie estacionaria (posiblemente a valores complejos) si y solo si <span class="math inline">\(K(\cdot)\)</span> es Hermitiana y no-negativa definida, esto es, si y solo si <span class="math inline">\(K(n)=\overline{K(-n)}\)</span> y</p>
<span class="math display" id="eq:eq-k-no-negativa-definida">\[\begin{equation}
\sum_{i,j=1}^na_iK(i-j)\bar{a}_j\geq0,
\tag{6.191}
\end{equation}\]</span>
para todo entero positivo <span class="math inline">\(n\)</span> y todo vector <span class="math inline">\(\mathbf{a}=(a_1,\ldots,a_n)^t\in\mathbb{C}^n\)</span>.
</div>

<hr />
<p>El Teorema <a href="modelos-arma.html#thm:teo-autocovarianza-hermitiana">6.4</a> caracteriza la función de autocovarianza a valores complejos sobre los enteros como aquellas funciones que son Hermitianas y no-negativa definida. El Teorema de Herglotz, el cual presentaremos a continuación, caracteriza estas como las funciones que pueden ser escritas en la forma <a href="modelos-arma.html#eq:eq-funcion-distribucion-espectral">(6.145)</a> para alguna función de distribución acotada <span class="math inline">\(F\)</span> con masa concentrada en <span class="math inline">\((-\pi,\pi]\)</span>.</p>

<div class="theorem">
<p><span id="thm:teo-Herglotz" class="theorem"><strong>Teorema 6.5  (Teorema de Herglotz)  </strong></span>Una función a valores complejos <span class="math inline">\(\gamma(\cdot)\)</span> definida sobre los enteros es no-negativa definida si y solo si</p>
<span class="math display" id="eq:eq-no-negativa-definida-Herglotz">\[\begin{equation}
\gamma(h) = \int_{-\pi}^{\pi}e^{ihv}dF(v)\text{ para todo }h=0,\pm1,\pm2,\ldots,
\tag{6.192}
\end{equation}\]</span>
donde <span class="math inline">\(F(\cdot)\)</span> es una función acotada en <span class="math inline">\([-\pi,\pi]\)</span> continua a la derecha, no decreciente y <span class="math inline">\(F(-\pi)=0\)</span>.
</div>

<hr />

<div class="definition">
<p><span id="def:defi-proceso-incremento-ortogonal" class="definition"><strong>Definición 6.17  </strong></span>Un <em>proceso de incremento ortogonal sobre <span class="math inline">\([-\pi,\pi]\)</span></em> es un proceso estocástico a valores complejos <span class="math inline">\(\{Z(\lambda),-\pi\leq\lambda\leq\pi\}\)</span> tal que</p>
<span class="math display" id="eq:eq-e4p6p2" id="eq:eq-e4p6p1">\[\begin{eqnarray}
\langle Z(\lambda),Z(\lambda)\rangle &amp;&lt;&amp; \infty\text{, con }-\pi\leq\lambda\leq\pi \tag{6.193}  \\
\langle Z(\lambda),1\rangle &amp;=&amp; 0 \text{, con }-\pi\leq\lambda\leq\pi \tag{6.194}
\end{eqnarray}\]</span>
<p>y</p>
<span class="math display" id="eq:eq-e4p6p3">\[\begin{equation}
\langle Z(\lambda_4)-Z(\lambda_3),Z(\lambda_2)-Z(\lambda_1)\rangle=0\text{, si }(\lambda_1,\lambda_2]\cap(\lambda_3,\lambda_4]=\emptyset
\tag{6.195}
\end{equation}\]</span>
donde el producto interno se define como <span class="math inline">\(\langle X,Y\rangle=\mathbb{E}(X\bar{Y})\)</span>.
</div>

<hr />
<p>El proceso <span class="math inline">\(\{Z(\lambda),-\pi\leq\lambda\leq\pi\}\)</span> se llamará <strong>continuo a la derecha</strong> si para todo <span class="math inline">\(\lambda\in[-\pi,\pi)\)</span></p>
<p><span class="math display">\[\|Z(\lambda+\delta)-Z(\lambda)\|^2=\mathbb{E}|Z(\lambda+\delta)-Z(\lambda)|^2\to0\text{ cuando }\delta\downarrow0.\]</span></p>

<div class="proposition">
<p><span id="prp:prop-incremento-ortogonal-distrib-unica" class="proposition"><strong>Proposición 6.12  </strong></span>Si <span class="math inline">\(\{Z(\lambda),-\pi\leq\lambda\leq\pi\}\)</span> es un proceso de incremento ortogonal, entonces existe una única función de distribución <span class="math inline">\(F\)</span> (es decir, una única función continua a derecha no decreciente) tal que</p>
<span class="math display" id="eq:eq-e4p6p4">\[\begin{equation}
\begin{array}{lclc}
F(\lambda) &amp;=&amp;0, &amp;   \lambda\leq-\pi  \\
F(\lambda) &amp;=&amp; F(\pi), &amp;  \lambda\geq\pi  \\
F(\mu)-F(\lambda) &amp;=&amp; \|Z(\mu)-Z(\lambda)\|^2, &amp;   -\pi\leq\lambda\leq\mu\leq\pi\\
\end{array}
\tag{6.196}
\end{equation}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span> Para <span class="math inline">\(F\)</span> satisfaciendo las condiciones prescritas es claro, haciendo <span class="math inline">\(\lambda=-\pi\)</span> que</p>
<span class="math display" id="eq:eq-e4p6p5">\[\begin{equation}
  F(\mu)=\|Z(\mu)-Z(-\pi)\|^2\text{, }-\pi\leq\mu\leq\pi
\tag{6.197}
\end{equation}\]</span>
<p>Para verificar que la función así definida es no decreciente, usamos la ortogonalidad de <span class="math inline">\(Z(\mu)-Z(\lambda)\)</span> y <span class="math inline">\(Z(\lambda)-Z(-\pi), -\pi\leq\lambda\leq\mu\leq\pi\)</span> para escribir</p>
<span class="math display">\[\begin{eqnarray*}
  F(\mu) &amp;=&amp; \|Z(\mu)-Z(\lambda)+Z(\lambda)-Z(-\pi)\|^2 \\
         &amp;=&amp; \|Z(\mu)-Z(\lambda)\|^2+\|Z(\lambda)-Z(-\pi)\|^2 \\
         &amp;\geq&amp; F(\lambda)
\end{eqnarray*}\]</span>
<p>El mismo procedimiento nos da para <span class="math inline">\(-\pi\leq\mu\leq\mu+\delta\leq\pi\)</span></p>
<p><span class="math display">\[F(\mu+\delta)-F(\mu)=\|Z(\mu+\delta)-Z(\mu)\|^2\to0\text{, cuando }\delta\downarrow0,\]</span></p>
por la suposición de continuidad a derecha de <span class="math inline">\(\{Z(\lambda)\}\)</span>
</div>

<hr />

<div class="remark">
<p> <span class="remark"><em>Nota. </em></span> La función de distribución <span class="math inline">\(F\)</span> de la Proposición <a href="modelos-arma.html#prp:prop-incremento-ortogonal-distrib-unica">6.12</a>, definida en <span class="math inline">\([-\pi,\pi]\)</span> por <a href="modelos-arma.html#eq:eq-e4p6p5">(6.197)</a> será referida como la función de distribución asociada con el proceso de incremento ortogonal <span class="math inline">\(\{Z(\lambda),-\pi\leq\lambda\leq\pi\}\)</span>. Es común en la práctica en el análisis de series de tiempo usar la notación corta</p>
<p><span class="math display">\[\mathbb{E}(dZ(\lambda),d\bar{Z(\mu)})=\delta_{\lambda,\mu}dF(\lambda)\]</span></p>
para las ecuaciones <a href="modelos-arma.html#eq:eq-e4p6p3">(6.195)</a> y <a href="modelos-arma.html#eq:eq-e4p6p4">(6.196)</a>.
</div>

<hr />

<div class="definition">
<p><span id="def:defi-movimiento-browniano" class="definition"><strong>Definición 6.18  </strong></span>Un <strong>Movimiento Browniano Estándar</strong> iniciando en nivel cero es un proceso <span class="math inline">\(\{B(t),t\geq0\}\)</span> que satisface las siguientes condiciones:</p>
<ul>
<li><p><span class="math inline">\(B(0)=0\)</span>,</p></li>
<li><p><span class="math inline">\(B(t_2)-B(t_1),B(t_3)-B(t_2),\ldots,B(t_n)-B(t_{n-1})\)</span> son independientes para cada <span class="math inline">\(n\in\{3,4,\ldots\}\)</span> y cada <span class="math inline">\(t=(t_1,\ldots,t_n)^t\)</span> tal que <span class="math inline">\(0\leq t_1&lt;t_2&lt;\ldots&lt;t_n\)</span>,</p></li>
<li><span class="math inline">\(B(t)-B(s)\sim N(0,t-s)\)</span> para <span class="math inline">\(t\geq s\)</span>.
</div>
</li>
</ul>
<hr />

<div class="example">
<p><span id="exm:ejem-movimiento-browniano" class="example"><strong>Ejemplo 6.44  </strong></span>Un movimiento browniano <span class="math inline">\(\{B(\lambda),-\pi\leq\lambda\leq\pi\}\)</span> con <span class="math inline">\(\mathbb{E}B(\lambda)=0\)</span> y <span class="math inline">\(\text{var}(B(\lambda))=\sigma^2(\lambda+\pi)/2\pi\text{, }-\pi\leq\lambda\leq\pi\)</span>, es un proceso de incremento ortogonal sobre <span class="math inline">\([-\pi,\pi]\)</span>. La función de distribución asociada satisface <span class="math inline">\(F(\lambda)=0\text{, para }\lambda\leq-\pi, F(\lambda)=\sigma^2\text{, para }\lambda\geq\pi\)</span> y</p>
<span class="math display">\[F(\lambda)=\sigma^2(\lambda+\pi)/2\pi\text{, para }-\pi\leq\lambda\leq\pi.\]</span>
</div>

<hr />

<div class="definition">
<p><span id="def:defi-proceso-poisson" class="definition"><strong>Definición 6.19  </strong></span>Un <strong>Proceso de Poisson con media <span class="math inline">\(\lambda&gt;0\)</span></strong> es un proceso <span class="math inline">\(\{N(t),t\geq0\}\)</span> que satisface:</p>
<ul>
<li><p><span class="math inline">\(N(0)=0\)</span>,</p></li>
<li><p><span class="math inline">\(N(t_2)-N(t_1),N(t_3)-N(t_2),\ldots,N(t_n)-N(t_{n-1})\)</span> son independientes para cada <span class="math inline">\(n\in\{3,4,\ldots\}\)</span> y cada <span class="math inline">\(t=(t_1,\ldots,t_n)\)</span> tal que <span class="math inline">\(0\leq t_1&lt;t_2&lt;\ldots&lt;t_n\)</span>,</p></li>
<li><span class="math inline">\(N(t)-N(s)\)</span> tiene distribución de Poisson con media <span class="math inline">\(\lambda(t-s)\)</span> para <span class="math inline">\(t\geq s\)</span>.
</div>
</li>
</ul>
<hr />

<div class="example">
<p><span id="exm:ejem-proceso-poison" class="example"><strong>Ejemplo 6.45  </strong></span>Si <span class="math inline">\(\{N(\lambda),-\pi\leq\lambda\leq\pi\}\)</span> es un proceso de Poisson sobre <span class="math inline">\([-\pi,\pi]\)</span> con intensidad constante <span class="math inline">\(c\)</span> entonces el proceso <span class="math inline">\(Z(\lambda)=N(\lambda)-\mathbb{E}N(\lambda)\text{, }-\pi\leq\lambda\leq\pi\)</span>, es un proceso de incremento ortogonal con función de distribución asociada</p>
<p><span class="math display">\[F(\lambda)=\begin{cases}
   0&amp;\text{, para }\lambda\leq-\pi\\
   2\pi c&amp;\text{, para }\lambda\geq\pi\\
   c(\lambda+\pi)&amp;\text{, para }-\pi\leq\lambda\leq\pi
\end{cases}
\]</span></p>
Si escogemos <span class="math inline">\(c\)</span> como <span class="math inline">\(\sigma^2/2\pi\)</span> entonces <span class="math inline">\(\{Z(\lambda)\}\)</span> tiene exactamente la misma función de distribución asociada como la de <span class="math inline">\(\{B(\lambda)\}\)</span> del Ejemplo <a href="modelos-arma.html#exm:ejem-movimiento-browniano">6.44</a>.
</div>

<hr />
</div>
<div id="integración-con-respecto-a-un-proceso-de-incremento-ortogonal" class="section level2">
<h2><span class="header-section-number">6.7</span> Integración con Respecto a un Proceso de Incremento Ortogonal</h2>
<p>En esta sección demostraremos como definir la integral estocástica</p>
<p><span class="math display">\[I(f)=\int_{(-\pi,\pi]}f(v)dZ(v)\]</span></p>
<p>donde <span class="math inline">\(\{Z(\lambda)\text{, }-\pi\leq\lambda\leq\pi\}\)</span> es un proceso de incremento ortogonal definido sobre el espacio de probabilidad <span class="math inline">\((\Omega,\mathcal{F},P)\)</span> y <span class="math inline">\(f\)</span> es cada función en <span class="math inline">\([-\pi,\pi]\)</span> cuadrado integrable con respecto a la función de distribución <span class="math inline">\(F\)</span> asociada con <span class="math inline">\(Z(\lambda)\)</span>. Procederemos paso por paso, primero definiremos <span class="math inline">\(I(f)\)</span> para cada función <span class="math inline">\(f\)</span> de la forma</p>
<span class="math display" id="eq:eq-e4p7p1">\[\begin{equation}
  f(\lambda)=\sum_{i=0}^{n}f_iI_{(\lambda_i,\lambda_{i+1}]}(\lambda)\text{,    }-\pi=\lambda_0&lt;\lambda_1&lt;\cdots&lt;\lambda_{n+1}=\pi
\tag{6.198}
\end{equation}\]</span>
<p>como</p>
<span class="math display" id="eq:eq-e4p7p2">\[\begin{equation}
    I(f)=\sum_{i=0}^{n}f_i[Z(\lambda_{i+1})-Z(\lambda_i)]
\tag{6.199}
\end{equation}\]</span>
<p>Entonces, extendemos la aplicación <span class="math inline">\(I\)</span> a un isomorfismo de <span class="math inline">\(L^2([-\pi,\pi],\mathcal{B},F)\equiv L^2(F)\)</span> a un subespacio de <span class="math inline">\(L^2(\Omega,\mathcal{F},P)\)</span>.</p>
<p>Sea <span class="math inline">\(\mathcal{D}\)</span> la clase de todas las funciones que tiene la forma <a href="modelos-arma.html#eq:eq-e4p7p1">(6.198)</a> para algún <span class="math inline">\(n\in\{0,1,2,\ldots\}\)</span>. Entonces la definición <a href="modelos-arma.html#eq:eq-e4p7p2">(6.199)</a> es consistente en <span class="math inline">\(\mathcal{D}\)</span> dado que para cada <span class="math inline">\(f\in\mathcal{D}\)</span> existe una <em>única</em> representación de <span class="math inline">\(f\)</span>,</p>
<p><span class="math display">\[f(\lambda)=\sum_{i=0}^{n}r_iI_{(v_i,v_{i+1}]}(\lambda)\text{,    }-\pi=v_0&lt;v_1&lt;\cdots&lt;v_{m+1}=\pi,\]</span></p>
<p>en la cual <span class="math inline">\(r_i\neq r_{i+1}, 0\leq i&lt;m\)</span>. Todas las otras representaciones de <span class="math inline">\(f\)</span> que tienen la forma <a href="modelos-arma.html#eq:eq-e4p7p1">(6.198)</a> son obtenidas por medio de reexpresar una o más funciones indicatrices <span class="math inline">\(I_{(v_i,v_{i+1}]}\)</span> como una suma de funciones indicatrices de intervalos adjuntos. Sin embargo, esto no hace ninguna diferencia en el valor de <span class="math inline">\(I(f)\)</span>, y por consiguiente la definición <a href="modelos-arma.html#eq:eq-e4p7p2">(6.199)</a> es la misma para todas las representaciones <a href="modelos-arma.html#eq:eq-e4p7p1">(6.198)</a> de <span class="math inline">\(f\)</span>. Es claro que <a href="modelos-arma.html#eq:eq-e4p7p2">(6.199)</a> define <span class="math inline">\(I\)</span> como una aplicación lineal sobre <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>Más aún, la aplicación preserva el producto interno ya que si <span class="math inline">\(f\in\mathcal{D}\)</span> y <span class="math inline">\(g\in\mathcal{D}\)</span> entonces existen representaciones</p>
<p><span class="math display">\[f(\lambda)=\sum_{i=0}^{n}f_iI_{(\lambda_i,\lambda_{i+1}]}(\lambda)\]</span></p>
<p><span class="math display">\[g(\lambda)=\sum_{i=0}^{n}g_iI_{(\lambda_i,\lambda_{i+1}]}(\lambda)\]</span></p>
<p>en términos de una partición simple <span class="math inline">\(-\pi=\lambda_0&lt;\lambda_1&lt;\cdots&lt;\lambda_{n+1}=\pi\)</span>. Por lo tanto, el producto interno de <span class="math inline">\(I(f)\)</span> e <span class="math inline">\(I(g)\)</span> en <span class="math inline">\(L^2(\Omega,\mathcal{F},P)\)</span> es</p>
<span class="math display">\[\begin{eqnarray*}
  \langle I(f),I(g)\rangle &amp;=&amp; \left\langle\sum_{i=0}^{n}f_i[Z(\lambda_{i+1})-Z(\lambda_i)],\sum_{i=0}^{n}g_i[Z(\lambda_{i+1})-Z(\lambda_i)\right\rangle \\
         &amp;=&amp; \sum_{i=0}^{n}f_i\bar{g}_i(F(\lambda_{i+1})-F(\lambda_i))
\end{eqnarray*}\]</span>
<p>por la ortogonalidad de los incrementos de <span class="math inline">\(\{Z(\lambda)\}\)</span> y la Proposición <a href="modelos-arma.html#prp:prop-incremento-ortogonal-distrib-unica">6.12</a>.</p>
<p>La última expresión la podemos escribir como</p>
<p><span class="math display">\[\int_{(-\pi,\pi]}f(v)\bar{g}(v)dF(v)=\langle f,g\rangle_{L^2(F)}\]</span></p>
<p>el producto interno en <span class="math inline">\(L^2(F)\)</span> de <span class="math inline">\(f\)</span> y <span class="math inline">\(g\)</span>. Por lo tanto la aplicación I sobre <span class="math inline">\(\mathcal{D}\)</span> preserva los productos internos.</p>
<p>Ahora, denotemos <span class="math inline">\(\bar{\mathcal{D}}\)</span> la clausura en <span class="math inline">\(L^2(F)\)</span> del conjunto <span class="math inline">\(\mathcal{D}\)</span>. Si <span class="math inline">\(f\in\bar{\mathcal{D}}\)</span> entonces existe una sucesión <span class="math inline">\(\{f_n\}\)</span> de elementos de <span class="math inline">\(\mathcal{D}\)</span> tal que <span class="math inline">\(\|f_n-f\|_{L^2(f)}\to0\)</span>. Por lo tanto definimos <span class="math inline">\(I(f)\)</span> como el límite en media cuadrado</p>
<span class="math display" id="eq:eq-e4p7p3">\[\begin{equation}
    I(f)=\underset{n\to\infty}{m.s.\lim}I(f_n)
\tag{6.200}
\end{equation}\]</span>
<p>Primero comprobemos (a) que el límite existe y (b) que el límite es el mismo para todas las sucesiones <span class="math inline">\(\{f_n\}\)</span> tal que <span class="math inline">\(\|f_n-f\|_{L^2(F)}\to0\)</span>.</p>
<p>Para comprobar (a) simplemente observe que para <span class="math inline">\(f_m,f_n\in\mathcal{D}\)</span>,</p>
<span class="math display">\[\begin{eqnarray*}
\|I(f_n)-I(f_m)\| &amp;=&amp; \|I(f_n-f_m)\| \\
                  &amp;=&amp; \|f_n-f_m\|_{L^2(F)},
\end{eqnarray*}\]</span>
<p>de modo que si <span class="math inline">\(\|f_n-f_m\|_{L^2(F)}\to0\)</span>, la sucesión <span class="math inline">\(\{I(f_n)\}\)</span> es una sucesión de Cauchy y por lo tanto converge en <span class="math inline">\(L^2(\Omega,\mathcal{F},P)\)</span>.</p>
<p>Para comprobar (b), supóngase que <span class="math inline">\(\|f_n-f\|_{L^2(F)}\to0\)</span> y <span class="math inline">\(\|g_n-f\|_{L^2(F)}\to0\)</span> donde <span class="math inline">\(f_n,g_n\in\mathcal{D}\)</span>. Entonces la sucesión <span class="math inline">\(f_1,g_1,f_2,g_2,\ldots\)</span>, debe converger en norma y por lo tanto la sucesión <span class="math inline">\(I(f_1), I(g_1)\)</span>, <span class="math inline">\(I(f_2), I(g_2), \ldots\)</span>, debe converger en <span class="math inline">\(L^2(\Omega,\mathcal{D},P)\)</span>. Sin embargo, esto no es posible a menos que las subsucesiones <span class="math inline">\(I(f_n)\)</span> e <span class="math inline">\(I(g_n)\)</span> tengan el mismo límite en media cuadrado. Esto completa la prueba de que la definición <a href="modelos-arma.html#eq:eq-e4p7p3">(6.200)</a> es válida y consistente para <span class="math inline">\(f\in\bar{\mathcal{D}}\)</span>.</p>
<p>La aplicación <span class="math inline">\(I\)</span> sobre <span class="math inline">\(\bar{\mathcal{D}}\)</span> es lineal y preserva el producto interno ya que si <span class="math inline">\(f^{(i)}\in\bar{\mathcal{D}}\)</span> y \ <span class="math inline">\(\|f_n^{(i)}-f^{(i)}\|_{L^2(F)}\to0, f_n^{(i)}\in\mathcal{D},i=1,2\)</span>, entonces por linealidad de <span class="math inline">\(I\)</span> en <span class="math inline">\(\mathcal{D}\)</span></p>
<span class="math display">\[\begin{eqnarray*}
I(a_1f^{(1)}+a_2f^{(2)}) &amp;=&amp; \lim_{n\to\infty}I(a_1f_n^{(1)}+a_2f_n^{(2)}) \\
     &amp;=&amp; \lim_{n\to\infty}(a_1I(f_n^{(1)})+a_2I(f_n^{(2)})) \\
     &amp;=&amp; a_1I(f^{(1)})+a_2I(f^{(2)})
\end{eqnarray*}\]</span>
<p>y por continuidad del producto interno</p>
<span class="math display">\[\begin{eqnarray*}
  \langle I(f^{(1)},I(f^{(2)}\rangle &amp;=&amp; \lim_{n\to\infty}\langle I(f_n^{(1)}),I(f_n^{(2)})\rangle \\
         &amp;=&amp; \lim_{n\to\infty}\langle f_n^{(1)},f_n^{(2)}\rangle_{L^2(F)} \\
         &amp;=&amp; \langle f^{(1)},f^{(2)}\rangle_{L^2(F)}\rangle.
\end{eqnarray*}\]</span>
<p>Falta solo demostrar que <span class="math inline">\(\bar{\mathcal{D}}=L^2(F)\)</span>. Para hacer esto primero observe que las funciones continuas en <span class="math inline">\([-\pi,\pi]\)</span> son densas en <span class="math inline">\(L^2(F)\)</span> ya que <span class="math inline">\(F\)</span> es una función de distribución acotada. Más aún <span class="math inline">\(\mathcal{D}\)</span> es un subconjunto denso (en el sentido <span class="math inline">\(L^2(F)\)</span>) del conjunto de funciones continuas sobre <span class="math inline">\([-\pi,\pi]\)</span>. Por consiguiente <span class="math inline">\(\bar{\mathcal{D}}=L^2(F)\)</span>.</p>
<p>Las ecuaciones <a href="modelos-arma.html#eq:eq-e4p7p2">(6.199)</a> y <a href="modelos-arma.html#eq:eq-e4p7p3">(6.200)</a> entonces definen <span class="math inline">\(I\)</span> como una aplicación lineal que preserva el producto interno sobre <span class="math inline">\(\bar{\mathcal{D}}=L^2(F)\)</span> en <span class="math inline">\(L^2(\Omega,\mathcal{F},P)\)</span>. La imagen <span class="math inline">\(I(\bar{\mathcal{D}})\)</span> de <span class="math inline">\(\bar{\mathcal{D}}\)</span> es claramente un subespacio lineal cerrado de <span class="math inline">\(L^2(\Omega,\mathcal{F},P)\)</span> y la aplicación <span class="math inline">\(I\)</span> es un isomorfismo de <span class="math inline">\(\bar{\mathcal{D}}\)</span> en <span class="math inline">\(I(\bar{\mathcal{D}})\)</span>. La aplicación <span class="math inline">\(I\)</span> que nos proporciona la definición necesita de la integral estocástica.</p>

<div class="definition">
<p><span id="def:defi-integral-estocastica" class="definition"><strong>Definición 6.20  </strong></span>Si <span class="math inline">\(\{Z(\lambda)\}\)</span> es un proceso de incremento ortogonal sobre <span class="math inline">\([-\pi,\pi]\)</span> con función de distribución asociada <span class="math inline">\(F\)</span> y si <span class="math inline">\(f\in L^2(F)\)</span>, entonces  <span class="math inline">\(\int_{(-\pi,\pi]}f(\lambda)dZ(\lambda)\)</span> se define como la variable aleatoria <span class="math inline">\(I(f)\)</span> construida arriba, esto es,</p>
<span class="math display" id="eq:eq-integral-estocastica">\[\begin{equation}
\int_{(-\pi,\pi]}f(v)dZ(v):=I(f).
\tag{6.201}
\end{equation}\]</span>
</div>

<hr />
<div id="propiedades-de-la-integral-estocástica" class="section level3">
<h3><span class="header-section-number">6.7.1</span> Propiedades de la Integral Estocástica</h3>
<p>Para cada par de funciones <span class="math inline">\(f\)</span> y <span class="math inline">\(g\)</span> en <span class="math inline">\(L^2(F)\)</span> hemos establecidos las propiedades</p>
<span class="math display" id="eq:eq-e4p7p5" id="eq:eq-e4p7p4">\[\begin{eqnarray}
  I(a_1f+a_2g) &amp;=&amp; a_1I(f)+a_2I(g)\text{, }a_1,a_2\in\mathbb{C} \tag{6.202}\\
  \mathbb{E}(I(f)\bar{I(g)}) &amp;=&amp; \int_{(-\pi,\pi]}f(v)\bar{g(v)}dF(v) \tag{6.203}
\end{eqnarray}\]</span>
<p>Más aún, si <span class="math inline">\(\{f_n\}\)</span> y <span class="math inline">\(\{g_n\}\)</span> son sucesiones en <span class="math inline">\(L^2(F)\)</span> tal que <span class="math inline">\(\|f_n-f\|_{L^2(F)}\to0\)</span> y <span class="math inline">\(\|g_n-g\|_{L^2(F)}\to0\)</span>, entonces por continuidad del producto interno</p>
<span class="math display" id="eq:eq-e4p7p6">\[\begin{equation}
  \mathbb{E}(I(f_n)\bar{I(g_n)})\to\mathbb{E}(I(f)\bar{I(g)})=\int_{(-\pi,\pi]}f(v)\bar{g(v)}dF(v)
\tag{6.204}
\end{equation}\]</span>
<p>De <a href="modelos-arma.html#eq:eq-e4p7p2">(6.199)</a> es claro que</p>
<span class="math display" id="eq:eq-e4p7p7">\[\begin{equation}
    \mathbb{E}(I(f))=0
\tag{6.205}
\end{equation}\]</span>
<p>para todo <span class="math inline">\(f\in\mathcal{D}\)</span>; si <span class="math inline">\(f\in\bar{\mathcal{D}}\)</span> entonces existe una sucesión <span class="math inline">\(\{f_n\}, f_n\in\mathcal{D}\)</span> tal que <span class="math inline">\(f_n\overset{L^2(F)}{\longrightarrow}f\)</span> y <span class="math inline">\(I(f_n)\overset{m.s.}{\longrightarrow}I(f)\)</span>, de modo que <span class="math inline">\(\mathbb{E}(I(f))=\lim_{n\to\infty}\mathbb{E}(I(f_n))\)</span> y <a href="modelos-arma.html#eq:eq-e4p7p7">(6.205)</a> sigue siendo válido. Este argumento es frecuentemente usado para establecer las propiedades de integral estocástica.</p>
<p>Finalmente notamos de <a href="modelos-arma.html#eq:eq-e4p7p5">(6.203)</a> y <a href="modelos-arma.html#eq:eq-e4p7p7">(6.205)</a> que si <span class="math inline">\(\{Z(\lambda)\}\)</span> es cada proceso de incremento ortogonal sobre <span class="math inline">\([-\pi,\pi]\)</span> con función de distribución asociada <span class="math inline">\(F\)</span>, entonces</p>
<span class="math display" id="eq:eq-e4p7p8">\[\begin{equation}
  X_t=I(e^{it})=\int_{(-\pi,\pi]}e^{itv}dZ(v)\text{,  }t\in\mathbb{Z},
\tag{6.206}
\end{equation}\]</span>
<p>es un proceso estacionario con media cero y función de autocovarianza</p>
<span class="math display" id="eq:eq-e4p7p9">\[\begin{equation}
  \mathbb{E}(X_{t+h}\bar{X}_t)=\int_{(-\pi,\pi]}e^{ivh}dF(v).
\tag{6.207}
\end{equation}\]</span>
</div>
</div>
<div id="la-representación-espectral" class="section level2">
<h2><span class="header-section-number">6.8</span> La Representación Espectral</h2>
<p>Sea <span class="math inline">\(\{X_t\}\)</span> un proceso estacionario de media cero con función de distribución espectral <span class="math inline">\(F\)</span>. Para establecer la representación espectral</p>
<span class="math display" id="eq:eq-e4p2p5">\[\begin{equation}
X_t=\int_{(-\pi,\pi]}e^{itv}dZ(v)
\tag{6.208}
\end{equation}\]</span>
<p>del proceso <span class="math inline">\(\{X_t\}\)</span> necesitamos primero identificar un proceso de incremento ortogonal apropiado <span class="math inline">\(\{Z(\lambda),\lambda\in[-\pi,\pi]\}\)</span>. La identificación de <span class="math inline">\(\{Z(\lambda)\}\)</span> y la demostración de la representación se logrará mediante la definición de un isomorfismo entre ciertos subespacios <span class="math inline">\(\overline{\mathcal{H}}=\overline{sp}\{X_t,t\in\mathbb{Z}\}\)</span> de <span class="math inline">\(L^2(\Omega,\mathfrak{F},P)\)</span> y <span class="math inline">\(\overline{\mathcal{K}}=\overline{sp}\{e^{it},t\in\mathbb{Z}\}\)</span> <a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a> de <span class="math inline">\(L^2(F)\)</span>. Este isomorfismo proporcionará un vínculo entre las variables aleatorias en el <em>dominio del tiempo</em> y las funciones sobre <span class="math inline">\([-\pi, \pi]\)</span> en el <em>dominio de la frecuencia</em>.</p>
<p>Sean <span class="math inline">\(\mathcal{H}=\{X_t,t\in\mathbb{Z}\}\)</span> y <span class="math inline">\(\mathcal{K}=sp\{e^{it},t\in\mathbb{Z}\}\)</span> subespacios (no necesariamente cerrados) de <span class="math inline">\(\mathcal{H}\subset L^2(\Omega,\mathfrak{F},P)\)</span> y <span class="math inline">\(\mathcal{K}\subset L^2(F)\)</span> consistentes de combinaciones lineales finitas de <span class="math inline">\(X_t,t\in\mathbb{Z}\)</span> y <span class="math inline">\(e^{it},t\in\mathbb{Z}\)</span>, respectivamente. Demostraremos primero que la aplicación</p>
<span class="math display" id="eq:eq-e4p8p1">\[\begin{equation}
  T\left(\sum_{j=1}^{n}a_jX_{t_j}\right)=\sum_{j=1}^{n}a_je^{it_j}
\tag{6.209}
\end{equation}\]</span>
<p>define un isomorfismo entre <span class="math inline">\(\mathcal{H}\)</span> y <span class="math inline">\(\mathcal{K}\)</span>.</p>
<p>Para verificar que <span class="math inline">\(T\)</span> está bien definida, supóngase que <span class="math inline">\(\|\sum_{j=1}^{n}a_jX_{t_j}-\sum_{k=1}^{m}b_kX_{t_k}\|=0\)</span>. Entonces por definición de la norma <span class="math inline">\(L^2(F)\)</span> y el teorema de Herglotz (Teorema <a href="modelos-arma.html#thm:teo-Herglotz">6.5</a>)</p>
<span class="math display">\[\begin{eqnarray*}
  \left\|T\left(\sum_{j=1}^{n}a_jX_{t_j}\right)-T\left(\sum_{k=1}^{m}b_kX_{t_k}\right)\right\|^2_{L^2(F)}&amp;=&amp;\int_{(-\pi,\pi]}\left|\sum_{j=1}^{n}a_je^{it_jv}-\sum_{k=1}^{m}b_ke^{it_kv}\right|^2dF(v)\\
            &amp;=&amp;\mathbb{E}\left|\sum_{j=1}^{n}a_jX_{t_j}-\sum_{k=1}^{m}b_kX_{t_k}\right|^2=0,
\end{eqnarray*}\]</span>
<p>muestra que <a href="modelos-arma.html#eq:eq-e4p8p1">(6.209)</a> define <span class="math inline">\(T\)</span> consistentemente en <span class="math inline">\(\mathcal{H}\)</span>. La linealidad de <span class="math inline">\(T\)</span> se sigue de este hecho.</p>
<p>Adicionalmente</p>
<span class="math display">\[\begin{eqnarray*}
  \left\langle T\left(\sum_{j=1}^{n}a_jX_{t_j}\right),T\left(\sum_{k=1}^{m}b_kX_{s_k}\right)\right\rangle &amp;=&amp; \sum_{j=1}^{n}\sum_{k=1}^{m}a_j\bar{b}_k\langle e^{it_j},e^{is_k}\rangle_{L^2(F)} \\
   &amp;=&amp; \sum_{j=1}^{n}\sum_{k=1}^{m}a_j\bar{b}_k\int_{(-\pi,\pi]}e^{i(t_j-s_k)v}dF(v) \\
   &amp;=&amp; \sum_{j=1}^{n}\sum_{k=1}^{m}a_j\bar{b}_k\langle X_{t_j},X_{s_k}\rangle \\
   &amp;=&amp; \left\langle\sum_{j=1}^{n}a_jX_{t_j},\sum_{k=1}^{m}b_kX_{s_k}\right\rangle
\end{eqnarray*}\]</span>
<p>mostrando que <span class="math inline">\(T\)</span> de hecho define un isomorfismo entre <span class="math inline">\(\mathcal{H}\)</span> y <span class="math inline">\(\mathcal{K}\)</span>.</p>
<p>Demostraremos ahora que la aplicación <span class="math inline">\(T\)</span> se puede extender de manera única a un isomorfismo de <span class="math inline">\(\overline{\mathcal{H}}\)</span> en <span class="math inline">\(\overline{\mathcal{K}}\)</span>. Si <span class="math inline">\(Y\in\overline{\mathcal{H}}\)</span> entones existe una sucesión <span class="math inline">\(Y_n\in\mathcal{H}\)</span> tal que <span class="math inline">\(\|Y_n-Y\|\to0\)</span>. Esto implica que <span class="math inline">\(\{Y_n\}\)</span> es una sucesión de Cauchy y por consiguiente, dado que <span class="math inline">\(T\)</span> preserva la norma, la sucesión <span class="math inline">\(\{TY_n\}\)</span> es Cauchy en <span class="math inline">\(L^2(F)\)</span>. La sucesión <span class="math inline">\(\{TY_n\}\)</span> por lo tanto converge en norma a un elemento de <span class="math inline">\(\overline{\mathcal{K}}\)</span>. Si <span class="math inline">\(T\)</span> preserva la norma sobre <span class="math inline">\(\overline{\mathcal{H}}\)</span> definimos</p>
<p><span class="math display">\[TY=m.s.\lim_{n\to\infty}TY_n.\]</span></p>
<p>Esta es una definición consistente de <span class="math inline">\(T\)</span> en <span class="math inline">\(\overline{\mathcal{H}}\)</span> ya que si <span class="math inline">\(\|\tilde{Y}_n-Y\|\to0\)</span> entonces la sucesión <span class="math inline">\(TY_1,T\tilde{Y}_1\)</span>, <span class="math inline">\(TY_2,T\tilde{Y}_2,\ldots\)</span> es convergente, lo que implica que las subsucesiones <span class="math inline">\(\{TY_n\}\)</span> y <span class="math inline">\(\{T\tilde{Y}_n\}\)</span> tienen el mismo límite, llamémoslo <span class="math inline">\(TY\)</span>. Más aún, usando el mismo argumento dado en la sección <a href="modelos-arma.html#integración-con-respecto-a-un-proceso-de-incremento-ortogonal">Integración con Respecto a un Proceso de Incremento Ortogonal</a> es fácil demostrar que la aplicación <span class="math inline">\(T\)</span> extendida a <span class="math inline">\(\overline{\mathcal{H}}\)</span> es lineal y preserva el producto interno.</p>
<p>Finalmente, del teorema siguiente se tiene que <span class="math inline">\(\mathcal{K}\)</span> es uniformemente denso en el espacio de funciones continuas <span class="math inline">\(\phi\)</span> en <span class="math inline">\([-\pi,\pi]\)</span> con <span class="math inline">\(\phi(\pi)=\phi(-\pi)\)</span>, que a su vez es denso en <span class="math inline">\(L^2(F)\)</span>. Por consiguiente <span class="math inline">\(\overline{\mathcal{K}}=L^2(F)\)</span>.</p>

<div class="theorem">
<p><span id="thm:teo-t2p11p1" class="theorem"><strong>Teorema 6.6  </strong></span>Sea <span class="math inline">\(f\)</span> una función continua en <span class="math inline">\([-\pi,\pi]\)</span> tal que <span class="math inline">\(f(\pi)=f(-\pi)\)</span>. Entonces</p>
<span class="math display" id="eq:eq-e2p11p1">\[\begin{equation}
  n^{-1}(S_0f+S_1f+\cdots+S_{n-1}f)\to f
\tag{6.210}
\end{equation}\]</span>
uniformemente en <span class="math inline">\([-\pi,\pi]\)</span> cuando <span class="math inline">\(n\to\infty\)</span>.
</div>

<hr />
<p>De los hechos anteriores, se tiene el siguiente teorema</p>

<div class="theorem">
<p><span id="thm:teo-t4p8p1" class="theorem"><strong>Teorema 6.7  </strong></span>Si <span class="math inline">\(F\)</span> es la función de distribución espectral del proceso estacionario <span class="math inline">\(\{X_t,t\in\mathbb{Z}\}\)</span>, entonces existe un único isomorfismo <span class="math inline">\(T\)</span> de <span class="math inline">\(\overline{sp}\{X_t,t\in\mathbb{Z}\}\)</span> en <span class="math inline">\(L^2(F)\)</span> tal que</p>
<span class="math display">\[TX_t=e^{it}\text{,   }t\in\mathbb{Z}.\]</span>
</div>

<hr />
<p>El Teorema <a href="modelos-arma.html#thm:teo-t4p8p1">6.7</a> es particularmente útil en la teoría de predicción lineal. También es la clave para la identificación de los procesos de incremento ortogonal <span class="math inline">\(\{Z(\lambda),-\pi\leq\lambda\leq\pi\}\)</span> que aparecen en la representación espectral <a href="modelos-arma.html#eq:eq-e4p2p5">(6.208)</a>. Introducimos el proceso <span class="math inline">\(\{Z(\lambda)\}\)</span> en la siguiente proposición.</p>

<div class="proposition">
<p><span id="prp:propo-p4p8p1" class="proposition"><strong>Proposición 6.13  </strong></span>Si <span class="math inline">\(T\)</span> es definimos como en el Teorema <a href="modelos-arma.html#thm:teo-t4p8p1">6.7</a> entonces el proceso <span class="math inline">\(\{Z(\lambda),-\pi\leq\lambda\leq\pi\}\)</span> definido por</p>
<p><span class="math display">\[Z(\lambda)=T^{-1}(I_{(-\pi,\lambda]}(\cdot))\text{, }-\pi\leq\lambda\leq\pi,\]</span></p>
es un proceso de incremento ortogonal. Más aún, la función de distribución asociada con <span class="math inline">\(\{Z(\lambda)\}\)</span> es exactamente la función de distribución espectral <span class="math inline">\(F\)</span> de <span class="math inline">\(\{X_t\}\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span> Para cada <span class="math inline">\(\lambda\in[-\pi,\pi]\)</span>, <span class="math inline">\(Z(\lambda)\)</span> es un elemento bien definido de <span class="math inline">\(\overline{sp}\{X_t,t\in\mathbb{Z}\}\)</span> por el Teorema <a href="modelos-arma.html#thm:teo-t4p8p1">6.7</a>. Por lo tanto <span class="math inline">\(\langle Z(\lambda),Z(\lambda)\rangle&lt;\infty\)</span>. Dado que <span class="math inline">\(Z(\lambda)\in\overline{sp}\{X_t,t\in\mathbb{Z}\}\)</span> existe una sucesión <span class="math inline">\(\{Y_n\}\)</span> de elementos de <span class="math inline">\(\overline{sp}\{X_t,t\in\mathbb{Z}\}\)</span> tal que <span class="math inline">\(\|Y_n-Z(\lambda)\|\to0\)</span> cuando <span class="math inline">\(n\to\infty\)</span>. Por la continuidad del producto interior tenemos</p>
<p><span class="math display">\[\langle Z(\lambda),1\rangle=\lim_{n\to\infty}\langle Y_n,1\rangle=0\]</span></p>
<p>ya que cada <span class="math inline">\(X_t\)</span>, y por consiguiente cada <span class="math inline">\(Y_t\)</span> tiene media cero. Finalmente, si <span class="math inline">\(-\pi\leq\lambda_1\leq\lambda_2\leq\lambda_3\leq\lambda_4\leq\pi\)</span>,</p>
<span class="math display">\[\begin{eqnarray*}
  \langle Z(\lambda_4)-Z(\lambda_3),Z(\lambda_2)-Z(\lambda_1)\rangle &amp;=&amp; \langle TZ(\lambda_4)-TZ(\lambda_3),TZ(\lambda_2)-TZ(\lambda_1)\rangle \\
     &amp;=&amp; \langle I_{(\lambda_3,\lambda_4]}(\cdot),I_{(\lambda_1,\lambda_2]}(\cdot)\rangle_{L^2(F)} \\
     &amp;=&amp; \int_{(-\pi,\pi]}I_{(\lambda_3,\lambda_4]}(v)I_{(\lambda_1,\lambda_2]}(v)dF(v)=0
\end{eqnarray*}\]</span>
<p>completando la demostración de que <span class="math inline">\(\{Z(\lambda)\}\)</span> tiene incrementos ortogonales. Un cálculo casi idéntico a los cálculos previos nos da</p>
<p><span class="math display">\[\langle Z(\mu)-Z(\lambda),Z(\mu)-Z(\mu)\rangle=F(\mu_-F(\lambda),\]</span></p>
demostrando que <span class="math inline">\(\{Z(\lambda)\}\)</span> es continua a derecha con función de distribución asociada <span class="math inline">\(F\)</span> como afirma la proposición
</div>

<hr />
<p>Ahora es fácil establecer la representación espectral <a href="modelos-arma.html#eq:eq-e4p2p5">(6.208)</a>.</p>

<div class="theorem">
<p><span id="thm:teo-representacion-espectral" class="theorem"><strong>Teorema 6.8  (El Teorema de Representación Espectral)  </strong></span>Si <span class="math inline">\(\{X_t\}\)</span> es una sucesión estacionaria con media cero y función de distribución espectral <span class="math inline">\(F\)</span>, entonces existe un proceso de incremento ortogonal continua a la derecha <span class="math inline">\(\{Z(\lambda),-\pi\leq\lambda\leq\pi\}\)</span> tal que</p>
<p><span class="math display">\[\text{(i) }\mathbb{E}|Z(\lambda)-Z(-\pi)|^2=F(\lambda)\text{, }-\pi\leq\lambda\leq\pi,\]</span></p>
<p>y</p>
<span class="math display">\[\text{(ii) }X_t=\int_{(-\pi,\pi]}E^{itv}dZ(v)\text{ con probabilidad uno.}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span> Sea <span class="math inline">\(\{Z(\lambda)\}\)</span> el proceso definido en la Proposición <a href="modelos-arma.html#prp:propo-p4p8p1">6.13</a> y sea <span class="math inline">\(I\)</span> el isomorfismo</p>
<p><span class="math display">\[I(f)=\int_{(-\pi,\pi]}f(v)dZ(v),\]</span></p>
<p>de <span class="math inline">\(\overline{\mathcal{D}}=L^2(F)\)</span> en <span class="math inline">\(I(\overline{\mathcal{D}}\subseteq L^2(\Omega,\mathfrak{F},P)\)</span> discutido en la Sección <a href="modelos-arma.html#integración-con-respecto-a-un-proceso-de-incremento-ortogonal">Integración con Respecto a un Proceso de Incremento Ortogonal</a>. Si <span class="math inline">\(f\in\mathcal{D}\)</span> tiene la representación <a href="modelos-arma.html#eq:eq-e4p7p1">(6.198)</a> entonces</p>
<span class="math display">\[\begin{eqnarray*}
  I(f) &amp;=&amp; \sum_{i=0}^{n}f_i(Z(\lambda_{i+1})-Z(\lambda_i)) \\
       &amp;=&amp; T^{-1}(f).
\end{eqnarray*}\]</span>
<p>Esta relación se mantiene válida para toda <span class="math inline">\(f\in\overline{\mathcal{D}}=L^2(F)\)</span> ya que tanto <span class="math inline">\(I\)</span> como <span class="math inline">\(T^{-1}\)</span> son isomorfismos.</p>
<p>Por lo tanto tenemos que <span class="math inline">\(I=T^{-1}\)</span> (i.e. <span class="math inline">\(TI(f)=f\)</span> para todo <span class="math inline">\(f\in L^2(F)\)</span>) y por consiguiente del Teorema <a href="modelos-arma.html#thm:teo-t4p8p1">6.7</a></p>
<p><span class="math display">\[X_t=I(e^{it\cdot})=\int_{(-\pi,\pi]}e^{itv}dZ(v),\]</span></p>
dando la representación requerida para <span class="math inline">\(\{X_t\}\)</span>. La primera afirmación del Teorema es una consecuencia inmediata de la Proposición <a href="modelos-arma.html#prp:propo-p4p8p1">6.13</a>.
</div>

<hr />

<div class="corollary">
<p><span id="cor:cor-c4p8p1" class="corollary"><strong>Corolario 6.1  </strong></span>Si <span class="math inline">\(\{X_t\}\)</span> es una sucesión estacionaria de media cero entonces existe un proceso de incremento ortogonal continuo a la derecha <span class="math inline">\(\{Z(\lambda),-\pi\leq\lambda\leq\pi\}\)</span> tal que <span class="math inline">\(Z(-\pi)=0\)</span> y</p>
<p><span class="math display">\[X_t=\int_{(-\pi,\pi]}e^{itv}dZ(v)\text{ con probabilidad uno.}\]</span></p>
<p>Si <span class="math inline">\(\{Y(\lambda)\}\)</span> y <span class="math inline">\(\{Z(\lambda)\}\)</span> son dos de tales procesos entonces</p>
<span class="math display">\[P(Y(\lambda)=Z(\lambda))=1\text{ para cada}\lambda\in[-\pi,\pi].\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span> Si denotamos por <span class="math inline">\(\{Z^{\star}(\lambda)\}\)</span> el proceso de incremento ortogonal definido por la Proposición <a href="modelos-arma.html#prp:propo-p4p8p1">6.13</a>, entonces el proceso</p>
<p><span class="math display">\[Z(\lambda)=Z^{\star}(\lambda)-Z^{\star}(-\pi)\text{, }-\pi\leq\lambda\leq\pi,\]</span></p>
<p>no solo satisface <span class="math inline">\(Z(-\pi)=0\)</span>, sino también tiene exactamente el mismo incremento que <span class="math inline">\(\{Z^{\star}(\lambda)\}\)</span>. Por consiguiente</p>
<p><span class="math display">\[X_t=\int_{(-\pi,\pi]}e^{itv}dZ^{\star}(v)=\int_{(-\pi,\pi]}e^{itv}dZ(v).\]</span></p>
<p>Supóngase ahora que <span class="math inline">\(\{Y(\lambda)\}\)</span> es otro proceso de incremento ortogonal tal que <span class="math inline">\(Y(-\pi)=0\)</span> y</p>
<span class="math display" id="eq:eq-e4p8p2">\[\begin{equation}
  X_t=\int_{(-\pi,\pi]}e^{itv}dY(v)=\int_{(-\pi,\pi]}e^{itv}dZ(v)\text{ con probabilidad uno.}
\tag{6.211}
\end{equation}\]</span>
<p>Si definimos para <span class="math inline">\(f\in L^2(F)\)</span></p>
<p><span class="math display">\[I_Y(f)=\int_{(-\pi,\pi]}f(v)dY(v)\]</span></p>
<p>e</p>
<p><span class="math display">\[I_Z(f)=\int_{(-\pi,\pi]}f(v)dZ(v)\]</span></p>
<p>entonces tenemos de <a href="modelos-arma.html#eq:eq-e4p8p2">(6.211)</a></p>
<span class="math display" id="eq:eq-e4p8p3">\[\begin{equation}
  I_y(e^{it\cdot})=I_z(e^{it\cdot})\text{ para todo }t\in\mathbb{Z}.
\tag{6.212}
\end{equation}\]</span>
<p>Dado que <span class="math inline">\(I_Y\)</span> e <span class="math inline">\(I_Z\)</span> son iguales en <span class="math inline">\(sp\{e^{it\cdot},t\in\mathbb{Z}\}\)</span> el cual es denso en <span class="math inline">\(L^2(F)\)</span>, se sigue que <span class="math inline">\(I_Y(f)=I_Z(f)\)</span> para todo <span class="math inline">\(f\in L^2(F)\)</span>. Eligiendo <span class="math inline">\(f(v)=I_{(-\pi,\lambda]}(v)\)</span> obtenemos (con probabilidad uno)</p>
<span class="math display">\[Y(\lambda)=\int_{(-\pi,\pi]}f(v)dZ(v)=Z(\lambda)\text{,  }-\pi\leq\lambda\leq\pi\]</span>
</div>

<ul>
<li><p><strong>Observación 1.</strong> En el transcurso de la demostración del Teorema <a href="modelos-arma.html#thm:teo-representacion-espectral">6.8</a> se estableció el siguiente resultado: <span class="math inline">\(Y\in\overline{sp}\{X_t,t\in\mathbb{Z}\}\)</span> si y solo si existe una función <span class="math inline">\(f\in L^2(F)\)</span> tal que <span class="math inline">\(Y=I(f)=\int_{(-\pi,\pi]}f(v)dZ(v)\)</span>. Esto significa que <span class="math inline">\(I\)</span> es un isomorfismo de <span class="math inline">\(L^2(F)\)</span> en <span class="math inline">\(\overline{sp}\{X_t,t\in\mathbb{Z}\}\)</span> (con la propiedad de que <span class="math inline">\(I(e^{it\cdot})=X_t\)</span>).</p></li>
<li><p><strong>Observación 2.</strong> Los argumentos aportados por el Teorema <a href="modelos-arma.html#thm:teo-representacion-espectral">6.8</a> es una prueba de existencia que no revela de manera explícita cómo se construye <span class="math inline">\(\{Z(\lambda)\}\)</span>.</p></li>
<li><p><strong>Observación 3.</strong> El corolario establece que el proceso de incremento ortogonal en la representación espectral es único si usamos la normalización de <span class="math inline">\(Z(-\pi)=0\)</span>. Dos proceso estacionarios diferentes pueden tener la misma función de distribución espectral, por ejemplo los procesos <span class="math inline">\(X_t=\int_{(-\pi,\pi]}e^{it\lambda}dB(\lambda)\)</span> e <span class="math inline">\(Y_t=\int_{(-\pi,\pi]}e^{it\lambda}dN(\lambda)\)</span> con <span class="math inline">\(\{B(\lambda)\}\)</span> y <span class="math inline">\(\{N(\lambda)\}\)</span> definidos como en los Ejemplos <a href="modelos-arma.html#exm:ejem-movimiento-browniano">6.44</a> y <a href="modelos-arma.html#exm:ejem-proceso-poison">6.45</a>. En tales casos los procesos deben de hecho tener la misma función de autocovarianza.</p></li>
</ul>

<div class="example">
<p><span id="exm:ejem-representacion-espectral-movimiento-browniano" class="example"><strong>Ejemplo 6.46  (Representación espectral de un movimiento browniano)  </strong></span>Sea <span class="math inline">\(Z(\lambda)=B(\lambda)\)</span> un movimiento browniano en <span class="math inline">\([-\pi,\pi]\)</span> como el definido en el Ejemplo  con <span class="math inline">\(\mathbb{E}Z(\lambda)=0\)</span> y <span class="math inline">\(Var(Z(\lambda))=\sigma^2(\lambda+\pi)/2\pi,-\pi\leq\lambda\leq\pi\)</span>. Para <span class="math inline">\(t\in\mathbb{Z}\)</span>, hagamos <span class="math inline">\(g_t(v)=\sqrt{2}\cos(tv)I_{(-\pi,0]}(v)+\sqrt{2}\sin(tv)I_{(0,\pi]}(v)\)</span> y</p>
<span class="math display" id="eq:eq-e4p8p4">\[\begin{equation}
  X_t=\int_{(-\pi,\pi]}g_t(v)dB(v)=\sqrt{2}\left(\int_{(-\pi,0]}\cos(tv)dB(v)+\int_{(0,\pi]}\sin(tv)dB(v)\right).
\tag{6.213}
\end{equation}\]</span>
<p>Entonces <span class="math inline">\(\mathbb{E}X_t=0\)</span> por <a href="modelos-arma.html#eq:eq-e4p7p7">(6.205)</a> y por <a href="modelos-arma.html#eq:eq-e4p7p5">(6.203)</a>,</p>
<span class="math display" id="eq:eq-e4p8p5">\[\begin{equation}
  \mathbb{E}(X_{t+h}X_t)=\frac{\sigma^2}{2\pi}\int_{(-\pi,\pi]}g_{t+h}(v)g_t(v)dv=\frac{\sigma^2}{2\pi}2\int_0^{\pi}\cos(hv)dv.
\tag{6.214}
\end{equation}\]</span>
Por lo tanto <span class="math inline">\(\mathbb{E}(X_{t+h}X_t)=\sigma^2\delta_{h,0}\)</span> y en consecuencia <span class="math inline">\(\{X_t\}\sim WN(0,\sigma^2)\)</span>.
</div>

<hr />
<p>Sin embargo, dado que <span class="math inline">\(B(\lambda)\)</span> es gaussiano podemos ir más allá y demostrar que las variables aleatorias <span class="math inline">\(X_t,t=0,\pm1,\ldots\)</span>, son independientes con <span class="math inline">\(X_t\sim N(0,\sigma^2)\)</span>. Para demostrar esto, sea <span class="math inline">\(s_1,\ldots,s_k\)</span>, <span class="math inline">\(k\)</span> enteros distintos y para cada <span class="math inline">\(j\)</span> fijo sea <span class="math inline">\(\{f_j^{(n)}\}\)</span> una sucesión de elementos de <span class="math inline">\(\mathcal{D}\)</span>, esto es, funciones de la forma (), tal que <span class="math inline">\(f_j^{(n)}\to g_{sj}(\cdot)\)</span> en <span class="math inline">\(L^2(F)\)</span>. Dado que la aplicación <span class="math inline">\(I_n\)</span> es un isomorfismo de <span class="math inline">\(\overline{\mathcal{D}}=L^2(F)\)</span> en <span class="math inline">\(I_B(\overline{\mathcal{D}})\)</span>, concluimos de <a href="modelos-arma.html#eq:eq-e4p8p4">(6.213)</a> que</p>
<span class="math display" id="eq:eq-e4p8p6">\[\begin{equation}
  \theta_1I_B(f_1^{(n)})+\cdots+\theta_kI_B(f_k^{(n)})\overset{m.s}{\to}\theta_1X_{s_1}+\cdots+\theta_kX_{s_k}.
\tag{6.215}
\end{equation}\]</span>
<p>El lado izquierdo <span class="math inline">\(I_B(\sum_{j=1}^{k}\theta_jf_j^{(n)})\)</span> es claramente normalmente distribuido con media cero y varianza <span class="math inline">\(\|\sum_{j=1}^{k}\theta_jf_j^{(n)}\|^2\)</span>.</p>
<p>La función característica de <span class="math inline">\(I_B(\sum_{j=1}^{k}\theta_jf_j^{(n)})\)</span> es por lo tanto</p>
<p><span class="math display">\[\phi_n(u)=\exp\left[-\frac{1}{2}u^2\left\|\sum_{j=1}^{k}\theta_jf_j^{(n)}\right\|_{L^2(F)}^2\right].\]</span></p>
<p>Por continuidad del producto interior en <span class="math inline">\(L^2(F)\)</span>, cuando <span class="math inline">\(n\to\infty\)</span></p>
<p><span class="math display">\[\left\|\sum_{j=1}^{k}\theta_jf_j^{(n)}\right\|_{L^2(F)}^2\to\left\|\sum_{j=1}^{n}\theta_je^{s_j\cdot}\right\|_{L^2(F)}^2=\sigma^2\sum_{j=1}^{k}\theta_j^2.\]</span></p>
<p>De <a href="modelos-arma.html#eq:eq-e4p8p6">(6.215)</a> concluimos por lo tanto que <span class="math inline">\(\sum_{j=1}^{k}\theta_jX_{s_j}\)</span> tiene función característica gaussiana</p>
<p><span class="math display">\[\phi(u)=\lim_{n\to\infty}\phi_n(u)=\exp\left[-\frac{1}{2}u^2\sigma^2\sum_{j=1}^{k}\theta_j^2\right].\]</span></p>
<p>Ya que esto es cierto para toda elección de <span class="math inline">\(\theta_1,\ldots,\theta_k\)</span> deducimos que <span class="math inline">\(X_{s_1},\ldots,X_{s_k}\)</span> son conjuntamente normal. De la covarianza <a href="modelos-arma.html#eq:eq-e4p8p5">(6.214)</a> se sigue entonces que las variables aleatorias <span class="math inline">\(X_t,t=0,\pm1,\ldots\)</span>, son iid<span class="math inline">\(N(0,\sigma^2)\)</span>.</p>
<ul>
<li><strong>Observación 4.</strong> Si <span class="math inline">\(A\)</span> es un subconjunto de Borel de <span class="math inline">\([-\pi,\pi]\)</span>, es conveniente en la siguiente proposición (y en otros lugares) definir
<span class="math display" id="eq:eq-e4p8p7">\[\begin{equation}
  \int_A f(v)dZ(v)=\int_{(-\pi,\pi]}f(v)I_A(v)dZ(v),
\tag{6.216}  
\end{equation}\]</span>
donde el lado derecho ya ha sido definido en la Sección <a href="modelos-arma.html#integración-con-respecto-a-un-proceso-de-incremento-ortogonal">Integración con Respecto a un Proceso de Incremento Ortogonal</a>.</li>
</ul>

<div class="proposition">
<p><span id="prp:propo-p4p8p2" class="proposition"><strong>Proposición 6.14  </strong></span>Supóngase que la función de distribución espectral <span class="math inline">\(F\)</span> de un proceso estacionario <span class="math inline">\(\{X_t\}\)</span> tiene un punto de discontinuidad en <span class="math inline">\(\lambda_0\)</span> donde <span class="math inline">\(-\pi&lt;\lambda_0&lt;\pi\)</span>. Entonces con probabilidad uno,</p>
<p><span class="math display">\[X_t=\int_{(-\pi,\pi]\backslash\{\lambda_0\}}e^{itv}dZ(v)+(Z(\lambda_0)-Z(\lambda_0^-))e^{it\lambda_0}\]</span></p>
<p>donde los dos términos del lado derecho son no-correlacionados y</p>
<span class="math display">\[Var(Z(\lambda_0)-Z(\lambda_0^-))=F(\lambda_0)-F(\lambda_0^-).\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span> El límite izquierdo <span class="math inline">\(Z(\lambda_0^-)\)</span> se define como</p>
<span class="math display" id="eq:eq-e4p8p8">\[\begin{equation}
  Z(\lambda_0^-)=m.s.\lim_{n\to\infty}Z(\lambda_n)
\tag{6.217}
\end{equation}\]</span>
<p>donde <span class="math inline">\(\lambda_n\)</span> es una sucesión tal que <span class="math inline">\(\lambda_n\uparrow\lambda_0\)</span>.\ Para verificar que <a href="modelos-arma.html#eq:eq-e4p8p8">(6.217)</a> tiene sentido, note que <span class="math inline">\(\{Z(\lambda_n)\}\)</span> es una sucesión de Cauchy ya que <span class="math inline">\(\|Z(\lambda_n)-Z(\lambda_m)\|^2=|F(\lambda_n)-F(\lambda_m)|\to0\)</span> cuando <span class="math inline">\(m,n\to\infty\)</span>. Por lo tanto el límite en <a href="modelos-arma.html#eq:eq-e4p8p8">(6.217)</a> existe. Más aún, si <span class="math inline">\(\nu_n\uparrow\lambda_0\)</span> cuando <span class="math inline">\(n\to\infty\)</span> entonces <span class="math inline">\(\|Z(\lambda_n)-Z(\nu_n)\|^2=|F(\lambda_n)-F(\nu_n)|\to0\)</span> cuando <span class="math inline">\(n\to\infty\)</span>, y en consecuencia el límite () es el mismo para toda sucesión no decreciente con límite <span class="math inline">\(\lambda_0\)</span>.\ Para <span class="math inline">\(\delta&gt;0\)</span> definamos <span class="math inline">\(\lambda_{\pm\delta}=\lambda_0\pm\delta\)</span>. Ahora por representación espectral, si <span class="math inline">\(0&lt;\delta&lt;\pi-|\lambda_0|\)</span>,</p>
<span class="math display" id="eq:eq-e4p8p9">\[\begin{equation}
  X_t=\int_{(-\pi,\pi]\backslash(\lambda_{-\delta},\lambda_{\delta}]}e^{itv}dZ(v)+\int_{(\lambda_{-\delta},\lambda_{\delta}]}e^{itv}dZ(v).
\tag{6.218}
\end{equation}\]</span>
<p>Note que los dos términos son no-correlacionados ya que las regiones de integración son disjuntas. Ahora cuando <span class="math inline">\(\delta\to0\)</span> el primer término converge en media cuadrado a <span class="math inline">\(\int_{(-\pi,\pi]\backslash\{\lambda_0\}}e^{itv}dZ(v)\)</span> dado que</p>
<p><span class="math display">\[e^{it\cdot}I_{90-\pi,\pi]\backslash(\lambda_{-\delta},\lambda_{\delta}]}\to e^{it\cdot}I_{(-\pi,\pi]\backslash\{\lambda_0\}}\text{ en }L^2(F).\]</span></p>
<p>Para ver cómo el último término de <a href="modelos-arma.html#eq:eq-e4p8p9">(6.218)</a> se comporta como <span class="math inline">\(\delta\to0\)</span> usamos la desigualdad</p>
<span class="math display" id="eq:eq-e4p8p10">\[\begin{eqnarray}
  \left\|\int_{(\lambda_{-\delta},\lambda_{\delta}]}e^{itv}dZ(v)-e^{it\lambda_0}(Z(\lambda_0)-Z(\lambda_0^-))\right\|&amp;\leq&amp;\left\|\int_{(\lambda_{-\delta},\lambda_{\delta}]}e^{itv}dZ(v)-e^{it\lambda_0}(Z(\lambda_{\delta})-Z(\lambda_{-\delta}))\right\| \nonumber\\
  &amp;+&amp;\left\|Z(\lambda_{\delta})-Z(\lambda_{-\delta})-(Z(\lambda_0)-Z(\lambda_0^-))\right\| \tag{6.219}
\end{eqnarray}\]</span>
<p>Cuando <span class="math inline">\(\delta\to0\)</span> el segundo término de la derecha de <a href="modelos-arma.html#eq:eq-e4p8p10">(6.219)</a> tiende a cero por la continuidad a la derecha de <span class="math inline">\(\{Z(\lambda)\}\)</span> y la definición de <span class="math inline">\(Z(\lambda_0^-)\)</span>. El primer término del lado derecho de <a href="modelos-arma.html#eq:eq-e4p8p10">(6.219)</a> se puede escribir como</p>
<span class="math display">\[\begin{eqnarray*}
\left\|\int_{(-\pi,\pi]}(e^{itv}-e^{it\lambda_0})I_{(\lambda_{-\delta},\lambda_{\delta}]}(v)dZ(v)\right\|&amp;=&amp;\left\|(e^{it\cdot}-e^{it\lambda_0})I_{(\lambda_{-\delta},\lambda_{\delta}]}(\cdot)\right\|_{L^2(F)}\\
&amp;\leq&amp;\left[\sup_{\lambda_{-\delta}\leq\lambda\leq\lambda_{\delta}}|e^{it\lambda}-e^{it\lambda_0}|^2(F(\lambda_{\delta})-F(\lambda_{-\delta}))\right]^{1/2}
\end{eqnarray*}\]</span>
<p>este tiende 0 cuando <span class="math inline">\(\delta\to0\)</span>, por la continuidad de la función <span class="math inline">\(e^{it\cdot}\)</span>. Por lo tanto, deducimos de <a href="modelos-arma.html#eq:eq-e4p8p10">(6.219)</a> que</p>
<p><span class="math display">\[\int_{(\lambda_{-\delta},\lambda_{\delta}]}e^{itv}dZ(v)\overset{m.s.}{\to}e^{it\lambda_0}(Z(\lambda_0)-Z(\lambda_0^-))\text{ cuando }\delta\to0.\]</span></p>
<p>La continuidad del producto interior y la ortogonalidad de las dos integrales en <a href="modelos-arma.html#eq:eq-e4p8p9">(6.218)</a> garantiza que sus límites en media cuadrado son también ortogonales.</p>
<p>Más aún</p>
<span class="math display">\[Var(Z(\lambda_0)-Z(\lambda_0^-))=\lim_{\lambda_n\uparrow\lambda_0}Var(Z(\lambda_0)-Z(\lambda_n))=F(\lambda_0)-F(\lambda_0^-).\]</span>
</div>

<hr />
<p>Si la función de densidad espectral tiene <span class="math inline">\(k\)</span> puntos de discontinuidad en <span class="math inline">\(\lambda_1,\ldots,\lambda_k\)</span> entonces <span class="math inline">\(\{X_t\}\)</span> tiene la representación</p>
<span class="math display" id="eq:eq-e4p8p11">\[\begin{equation}
  X_t=\int_{(-\pi,\pi]\backslash\{\lambda_1,\ldots,\lambda_k\}}e^{itv}dZ(v)+\sum_{j=1}^{k}(Z(\lambda_j)-Z(\lambda_j^-))e^{it\lambda_j},
\tag{6.220}
\end{equation}\]</span>
<p>donde los <span class="math inline">\((k+1)\)</span> términos del lado derecho son no-correlacionados.</p>
<p>La importancia de <a href="modelos-arma.html#eq:eq-e4p8p11">(6.220)</a> en el análisis de series de tiempo es inmenso. El proceso <span class="math inline">\(Y_t=(Z(\lambda_0)-Z(\lambda_0^-))e^{it\lambda_0}\)</span> se dice ser determinístico ya que <span class="math inline">\(Y_t\)</span> está determinado para todo <span class="math inline">\(t\)</span> si <span class="math inline">\(Y_{t_0}\)</span> es conocido para algún <span class="math inline">\(t_0\)</span>. La existencia de una discontinuidad en la función de densidad espectral en una frecuencia dada <span class="math inline">\(\lambda_0\)</span> por lo tanto indica la presencia en la serie de tiempo de una componente determinística sinusoidal con frecuencia <span class="math inline">\(\lambda_0\)</span>.</p>


</div>
</div>



</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>Véase Bhat, R.R. (1985). <em>Modern Probability Theory, 2nd ed.</em> New York, Wiley, pag157.<a href="modelos-arma.html#fnref9">↩</a></p></li>
<li id="fn10"><p>Algunas identidades que pueden ayudar aquí: <span class="math inline">\(e^{i\alpha}=\cos(\alpha)+i\sin(\alpha)\)</span>, así <span class="math inline">\(\cos(\alpha)=(e^{i\alpha}+e^{-i\alpha})/2\)</span> y <span class="math inline">\(\sin(\alpha)=(e^{i\alpha}-e^{-i\alpha})/2i\)</span>.<a href="modelos-arma.html#fnref10">↩</a></p></li>
<li id="fn11"><p><span class="math inline">\(\ln(1+p)=p-\frac{p^2}{2}+\frac{p^3}{3}-\cdots\)</span> para <span class="math inline">\(-1&lt;p\leq1\)</span>. Si <span class="math inline">\(p\)</span> es un porcentaje de cambio pequeño, entonces los términos de orden superior en el desarrollo son despreciables.<a href="modelos-arma.html#fnref11">↩</a></p></li>
<li id="fn12"><p>Véase Bhat, R.R. (1985). <em>Modern Probability Theory, 2nd ed.</em> New York, Wiley, pag157.<a href="modelos-arma.html#fnref12">↩</a></p></li>
<li id="fn13"><p>Algunas identidades que pueden ayudar aquí: <span class="math inline">\(e^{i\alpha}=\cos(\alpha)+i\sin(\alpha)\)</span>, así <span class="math inline">\(\cos(\alpha)=(e^{i\alpha}+e^{-i\alpha})/2\)</span> y <span class="math inline">\(\sin(\alpha)=(e^{i\alpha}-e^{-i\alpha})/2i\)</span>.<a href="modelos-arma.html#fnref13">↩</a></p></li>
<li id="fn14"><p>Note que <span class="math inline">\(\sum_{t=1}^{n}z^t=z\frac{1-z^n}{1-z}\)</span> para <span class="math inline">\(z\neq1\)</span>.<a href="modelos-arma.html#fnref14">↩</a></p></li>
<li id="fn16"><p>Recuerde que <span class="math inline">\(\sum_{t=1}^{n}\cos^2(2\pi\omega_jt)=\sum_{t=1}^{n}\sin^2(2\pi\omega_jt)=n/2\)</span> para <span class="math inline">\(j\neq0\)</span> o un múltiplo de <span class="math inline">\(n\)</span>. También <span class="math inline">\(\sum_{t=1}^{n}\cos(2\pi\omega_jt)\sin(2\pi\omega_kt)=0\)</span> para cada <span class="math inline">\(j\)</span> y <span class="math inline">\(k\)</span>.<a href="modelos-arma.html#fnref16">↩</a></p></li>
<li id="fn17"><p>Esto significa que <span class="math inline">\(\omega_{j:n}\)</span> es una frecuencia de la forma <span class="math inline">\(j_n/n\)</span> donde <span class="math inline">\(\{j_n\}\)</span> es una sucesión de enteros elegidos de modo que <span class="math inline">\(j_n/n\to\omega\)</span> cuando <span class="math inline">\(n\to\infty\)</span>.<a href="modelos-arma.html#fnref17">↩</a></p></li>
<li id="fn18"><p>De la definición <a href="modelos-arma.html#def:defi-periodograma">6.13</a> tenemos que <span class="math inline">\(I(0)=n\bar{x}^2\)</span>, así, el resultado análogo para el caso <span class="math inline">\(\omega=0\)</span> es <span class="math inline">\(\mathbb{E}[I(0)]-n\mu^2=n\text{var}(\bar{x})\to f(0)\)</span> cuando <span class="math inline">\(n\to\infty\)</span>.<a href="modelos-arma.html#fnref18">↩</a></p></li>
<li id="fn19"><p>Si <span class="math inline">\(Y_j\sim\text{iid}(0,\sigma^2)\)</span> y <span class="math inline">\(\{a_j\}\)</span> son constantes para las cuales <span class="math inline">\(\sum_{j=1}^{n}a_j^2/\max_{1\leq j\leq n}a_j^2\to\infty\)</span> cuando <span class="math inline">\(n\to\infty\)</span>, entonces <span class="math inline">\(\sum_{j=1}^{n}a_jY_j\sim AN\left(0,\sigma^2\sum_{j=1}^{n}a_j^2\right)\)</span>; la notación <span class="math inline">\(AN\)</span> significa asintóticamente normal.<a href="modelos-arma.html#fnref19">↩</a></p></li>
<li id="fn20"><p>Las condiciones, las cuales son suficientes, son que <span class="math inline">\(x_t\)</span> es un proceso lineal, como el descrito en la Proposición <a href="modelos-arma.html#prp:propie-distrib-ordenadas-periodograma">6.11</a>, con <span class="math inline">\(\sum_{j&gt;0}\sqrt{j}|\psi_j|&lt;\infty\)</span>, y <span class="math inline">\(w_t\)</span> tiene momento finito de orden cuarto.<a href="modelos-arma.html#fnref20">↩</a></p></li>
<li id="fn21"><p>La transformación logarítmica es la transformación de estabilización de la varianza en este caso.<a href="modelos-arma.html#fnref21">↩</a></p></li>
<li id="fn22"><p>El espacio cerrado <span class="math inline">\(\overline{sp}\{x_t,t\in T\}\)</span> de cada subconjunto <span class="math inline">\(\{x_t,t\in T\}\)</span> de un espacio de Hilbert <span class="math inline">\(\mathcal{H}\)</span> se define como el subespacio cerrado más pequeño de <span class="math inline">\(\mathcal{H}\)</span> el cual contiene todos los elementos <span class="math inline">\(x_t, t\in T\)</span>.<a href="modelos-arma.html#fnref22">↩</a></p></li>
</ol>
</div>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-78759535-1', 'auto');
ga('send', 'pageview');  
</script>
            </section>

          </div>
        </div>
      </div>
<a href="modelos-ma.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="referencias.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/synergyvision/Teoria-de-Portafolio/edit/master/bookdown/303-modelos-ARMA.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Serie-de-Tiempo-en-R.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
