[
["index.html", "Series de Tiempo en R Prefacio ¿Por qué leer este libro? Estructura del libro Información sobre los programas y convenciones Prácticas interactivas con R Agradecimientos", " Series de Tiempo en R Ciencia de los Datos Financieros Synergy Vision 2018-05-25 Prefacio La versión en línea de este libro se comparte bajo la licencia Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ¿Por qué leer este libro? Estructura del libro Información sobre los programas y convenciones Este libro es posible gracias a una gran cantidad de desarrolladores que contribuyen en la construcción de herramientas para generar documentos enriquecidos e interactivos. En particular al autor de los paquetes Yihui Xie xie2015. Prácticas interactivas con R Vamos a utilizar el paquete Datacamp Tutorial que utiliza la librería en JavaScript Datacamp Light para crear ejercicios y prácticas con R. De esta forma el libro es completamente interactivo y con prácticas incluidas. De esta forma estamos creando una experiencia única de aprendizaje en línea. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImIgPC0gNSIsInNhbXBsZSI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5cblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGEiLCJzb2x1dGlvbiI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5hIDwtIDVcblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGFcbmEiLCJzY3QiOiJ0ZXN0X29iamVjdChcImFcIilcbnRlc3Rfb3V0cHV0X2NvbnRhaW5zKFwiYVwiLCBpbmNvcnJlY3RfbXNnID0gXCJBc2VnJnVhY3V0ZTtyYXRlIGRlIG1vc3RyYXIgZWwgdmFsb3IgZGUgYGFgLlwiKVxuc3VjY2Vzc19tc2coXCJFeGNlbGVudGUhXCIpIn0= Agradecimientos Synergy Vision, Caracas, Venezuela "],
["acerca-del-autor.html", "Acerca del Autor", " Acerca del Autor Este material es un esfuerzo de equipo en Synergy Vision, (http://synergy.vision/nosotros/). El propósito de este material es ofrecer una experiencia de aprendizaje distinta y enfocada en el estudiante. El propósito es que realmente aprenda y practique con mucha intensidad. La idea es cambiar el modelo de clases magistrales y ofrecer una experiencia más centrada en el estudiante y menos centrado en el profesor. Para los temas más técnicos y avanzados es necesario trabajar de la mano con el estudiante y asistirlo en el proceso de aprendizaje con prácticas guiadas, material en línea e interactivo, videos, evaluación contínua de brechas y entendimiento, entre otros, para procurar el dominio de la materia. Nuestro foco es la Ciencia de los Datos Financieros y para ello se desarrollará material sobre: Probabilidad y Estadística Matemática en R, Programación Científica en R, Mercados, Inversiones y Trading, Datos y Modelos Financieros en R, Renta Fija, Inmunización de Carteras de Renta Fija, Teoría de Riesgo en R, Finanzas Cuantitativas, Ingeniería Financiera, Procesos Estocásticos en R, Series de Tiempo en R, Ciencia de los Datos, Ciencia de los Datos Financieros, Simulación en R, Desarrollo de Aplicaciones Interactivas en R, Minería de Datos, Aprendizaje Estadístico, Estadística Multivariante, Riesgo de Crédito, Riesgo de Liquidez, Riesgo de Mercado, Riesgo Operacional, Riesgo de Cambio, Análisis Técnico, Inversión Visual, Finanzas, Finanzas Corporativas, Valoración, Teoría de Portafolio, entre otros. Nuestra cuenta de Twitter es (https://twitter.com/bysynergyvision) y nuestros repositorios están en GitHub (https://github.com/synergyvision). Somos Científicos de Datos Financieros "],
["introduccion.html", "Capítulo 1 Introducción 1.1 Conceptos financieros básicos 1.2 Conceptos básicos 1.3 Ejemplos 1.4 Componentes de una serie de tiempo", " Capítulo 1 Introducción Las series de tiempo ya han desempeñado un papel importante en las primeras ciencias naturales. La astronomía babilónica utilizó series de tiempo de las posiciones relativas de estrellas y planetas para predecir eventos astronómicos. Las observaciones de los movimientos de los planetas formaron la base de las leyes que Johannes Kepler descubrió. El análisis de las series de tiempo ayuda a detectar las regularidades en las observaciones de una variable y a derivar “leyes” de ellas, y/o explotar toda la información incluida en esta variable para predecir mejor los desarrollos futuros. La idea metodológica básica detrás de estos procedimientos, que también eran válidos para los babilonios, es que es posible descomponer series de tiempos en un número finito de componentes independientes pero no directamente observables que se desarrollan regularmente y que por lo tanto pueden ser calculados de antemano. Para este procedimiento es necesario que existan diferentes factores independientes que incidan en la variable. A mediados del siglo XIX, este enfoque metodológico de la astronomía fue asumido por los economistas Charles Babbage y William Stanley Jevons. La descomposición en componentes no observados que dependen de diferentes factores causales, como suele emplearse en el análisis clásico de series de tiempo, fue desarrollada por Warren M. Persons (1919). Distinguía cuatro componentes diferentes: Desarrollo a largo plazo, tendencia, Componente cíclico con períodos de más de un año, el ciclo económico, Componente que contiene los altibajos dentro de un año, el ciclo estacional, y Componente que contiene todos los movimientos que no pertenecen ni a la tendencia ni al ciclo económico ni al componente estacional, el residual. Suponiendo que los diferentes factores no observables son independientes, su recubrimiento aditivo genera las series de tiempo que, sin embargo, sólo podemos observar en su conjunto. Para obtener información sobre el proceso de generación de datos, tenemos que hacer suposiciones sobre sus componentes no observados. El análisis clásico de series de tiempo supone que los componentes sistemáticos, es decir, la tendencia, el ciclo económico y el ciclo estacional, no están influenciados por perturbaciones estocásticas y, por lo tanto, pueden representarse mediante funciones determinísticas del tiempo. El impacto estocástico se limita a los residuos, que, por otra parte, no contienen movimientos sistemáticos. Por lo tanto, se modela como una serie de variables aleatorias independientes o no correlacionadas con esperanza cero y varianza constante, es decir, como un proceso aleatorio puro. Este enfoque cambió desde la presentación de los trabajos de George E. P. Box and Gwilym M. Jenkins, “Time Series Analysis: Forecasting and Control”, en los años 70 del siglo XX. Se abandonaron los procedimientos puramente descriptivos del análisis clásico de series de tiempo y, en su lugar, se han utilizado los resultados y métodos de la teoría de la probabilidad y las estadísticas matemáticas. Desde ese entonces, el análisis de series ha tenido un desarrollo creciente. Se han presentado una gran variedad de libros sobre este tópico, cada uno de ellos influenciado principalmente por la orientación de las series que se discuten en sus contenidos. Una gran parte de la literatura está dirigida a exponer los aspectos teóricos alrededor de las series de tiempo, siendo en muchos casos, rigurosamente desarrollados y descritos, sin embargo poco de ellos presentan implementaciones de las técnicas estudiadas y su compresión en ejemplos reales lo que a veces puede dificultar su comprensión en especial para aquellos que no posean una apropiada formación matemática. Los primeros intentos de estudiar el comportamiento de las series de tiempo financieras fueron realizados por profesionales financieros y periodistas en lugar de por académicos. De hecho, esto parece haberse convertido en una tradición de larga data, ya que, incluso hoy en día, gran parte de la investigación y el desarrollo empíricos todavía se originan en la propia industria financiera. Esto puede explicarse por el carácter práctico de los problemas, la necesidad de datos especializados y las posibles ventajas de dicho análisis. El primer y más conocido ejemplo de la investigación publicada sobre series de tiempo financieras es el legendario Charles Dow, como se expresa en sus editoriales en el Wall Street Times entre 1900 y 1902. Estos escritos formaron la base de la “teoría del Dow” e influyeron en lo que más tarde se conoció como análisis técnico y carisma. Aunque Dow no coleccionó y publicó sus editoriales por separado, esto fue hecho póstumamente por su seguidor Samuel Nelson (Nelson, 1902). Las ideas originales de Dow fueron posteriormente interpretadas y ampliadas por Hamilton (1922) y Rhea (1932). Estas ideas gozaron de cierto reconocimiento entre los académicos de la época: por ejemplo, Hamilton fue elegido miembro de la Royal Statistical Society. Aunque Dow y sus seguidores discutieron muchas de las ideas que encontramos en el análisis moderno de finanzas y series de tiempo, incluyendo estacionalidad, eficiencia del mercado, correlación entre rendimiento de activos e índices, diversificación e imprevisibilidad, no hicieron ningún esfuerzo serio para adoptar métodos estadísticos formales. La mayor parte del análisis empírico consistió en la interpretación minuciosa de gráficos detallados de las medias bursátiles sectoriales, formando así los famosos Índices Dow-Jones. Se argumentó que estos índices descuentan toda la información necesaria y proporcionan el mejor pronóstico de eventos futuros. Una idea fundamental, muy relevante para la teoría de los ciclos de Stanley Jevons y la metodología de descomposición de tendencias de la “curva Harvard A-B-C” de Warren Persons, fue que las variaciones de precios del mercado consistían en tres movimientos primarios: diarios, a medio y largo plazo. La investigación empírica más temprana que utiliza métodos estadísticos formales se remonta a los documentos de Working (1934), Cowles (1933,1944) y Cowles and Jones (1937). El trabajo centró la atención en una característica previamente señalada de los precios de las materias primas y las acciones: que se asemejan a la acumulación de cambios puramente aleatorios. Alfred Cowles 3rd, analista financiero cuantitativamente entrenado y fundador de Econometric Society and the Cowles Foundation, investigó la habilidad de los analistas de mercado y servicios financieros para predecir los futuros cambios de precios, encontrando que había pocas pruebas de que pudieran hacerlo. Cowles y Jones reportaron evidencia de correlación positiva entre sucesivas variaciones de precios, pero, como posteriormente Cowles (1960) comentó, esto fue probablemente debido a que tomaron promedios mensuales de precios diarios o semanales antes de computar los cambios: un fenómeno de “correlación espuria”, analizado por Working (1960). La previsibilidad de los cambios de precios se ha convertido desde entonces en un tema importante de la investigación financiera, pero, sorprendentemente, poco más se publicó hasta el estudio de Kendall (1953), en el que encontró que los cambios semanales en una amplia variedad de precios financieros no podían predecirse ni a partir de los cambios pasados en las series ni a partir de los cambios pasados en otras series de precios. Este parece haber sido el primer informe explícito de esta propiedad de los precios financieros a menudo citada, aunque la investigación sobre la previsibilidad de los precios sólo se vio impulsada por la publicación de los documentos de Roberts (1959) y Osborne (1959). El primero presenta un argumento en gran medida heurístico sobre por qué las sucesivas variaciones de precios deben ser independientes, mientras que el segundo desarrolla la proposición de que no se trata de cambios absolutos de precios, sino de cambios logarítmicos de precios independientes entre sí. Con la suposición auxiliar de que las propias modificaciones se distribuyen normalmente, esto implica que los precios se generan como movimiento Browniano. El análisis de series de tiempo desempeña un papel importante en el análisis requerido para el pronóstico de eventos futuros. Existen varias formas o métodos de calcular cual va a ser la tendencia del comportamiento del proceso en estudio. Un pronóstico es una predicción de algún evento o eventos futuros. Como sugirió Neils Bohr, hacer buenas predicciones no siempre es fácil. Los pronósticos famosamente “malos” incluyen lo siguiente del libro “Malas Predicciones”: “La población es de tamaño constante y se mantendrá hasta el fin de la humanidad.” La Enciclopedia, 1756. “1930 será un espléndido año de empleo.” Departamento de Trabajo de los EE.UU., pronóstico de Año Nuevo en 1929, justo antes de que el mercado se desplomara el 29 de octubre. “Las computadoras se multiplican a un ritmo rápido. Para el cambio de siglo habrá 220,000 en los EE.UU.” Wall Street Journal, 1966. Algunos ejemplos donde se puede utilizar y hacer precciones con series de tiempo: Dirección de Operaciones. Las organizaciones empresariales utilizan habitualmente las previsiones de ventas de productos o la demanda de servicios para programar la producción, controlar los inventarios, gestionar la cadena de suministro, determinar las necesidades de personal y planificar la capacidad. Las previsiones también pueden utilizarse para determinar la combinación de productos o servicios que deben ofrecerse y las ubicaciones en las que deben fabricarse los productos. Marketing. La previsión es importante en muchas decisiones de marketing. Las previsiones de respuesta de las ventas a los gastos publicitarios, las nuevas romociones o los cambios en las políticas de precios permiten a las empresas evaluar su eficacia, determinar si se están alcanzando los objetivos y realizar ajustes. Finanzas y Gestión de Riesgos. Los inversores en activos financieros están interesados en pronosticar los rendimientos de sus inversiones. Estos activos incluyen, pero no se limitan a acciones, bonos y materias primas; otras decisiones de inversión se pueden tomar en relación con las previsiones de tasas de inter?s, opciones y tipos de cambio. La gestión del riesgo financiero requiere previsiones de la volatilidad de la rentabilidad de los activos para que se puedan evaluar y asegurar los riesgos asociados a las carteras de inversion, y para que los derivados financieros puedan cotizarse adecuadamente. Economía. Los gobiernos, las instituciones financieras y las organizaciones de política requieren pronósticos de las principales variables económicas, como el producto interno bruto, el crecimiento demográfico, el desempleo, las tasas de interés, la inflación, el crecimiento del empleo, la producción y el consumo. Estas previsiones son parte integrante de la orientación de la política monetaria y fiscal, así como de los planes y decisiones presupuestarias adoptadas por los gobiernos. también son fundamentales en las decisiones de planificación estratégica tomadas por organizaciones empresariales e instituciones financieras. Control de Procesos Industriales. Las previsiones de los valores futuros de las características de calidad crítica de un proceso de producción pueden ayudar a determinar cuándo deben cambiarse las variables controlables importantes del proceso, o si el proceso debe detenerse y revisarse. Los esquemas de retroalimentación y control feedforward son ampliamente utilizados en el monitoreo y ajuste de procesos industriales, y las predicciones de la producción del proceso son una parte integral de estos esquemas. Demografía. Las previsiones de población por país y región se realizan de manera rutinaria, a menudo estratificadas por variables como el género, la edad y la raza. Los demógrafos también pronostican nacimientos, muertes y patrones migratorios de las poblaciones. Los gobiernos utilizan estas previsiones para planificar políticas y acciones de servicio social, como el gasto en atención médica, programas de jubilación y programas de lucha contra la pobreza. Muchas empresas utilizan pronósticos de poblaciones por grupos de edad para hacer planes estratégicos en relación con el desarrollo de nuevas líneas de productos o tipos de servicios que será ofrecido. 1.1 Conceptos financieros básicos La mayoría de los estudios financieros y econ?micos implican rendimiento, en lugar de precios de los activos. Existen dos buenas razones para ello. Primero, para los inversores medios, el rendimiento de un activo es un resumen completo y libre de escala de la oportunidad de inversión. Segundo, las series de rendimiento son más fáciles de manejar que las series de precios porque las primeras tienen propiedades estadísticas más atractivas. Sin embargo, existen varias definiciones de rendimiento de activos. Sea \\(P_t\\) el precio de un activo en tiempo \\(t\\). Discutiremos algunas definiciones de rendimiento que utilizaremos a lo largo del libro. Supongamos por el momento que el activo no paga dividendos. Definición 1.1 (Rendimiento simple de un periodo) Mantener el activo fijo durante un periodo a partir de tiempo \\(t-1\\) hasta tiempo \\(t\\) da lugar a una rentabilidad bruta simple \\[\\begin{equation} 1+R_t=\\frac{P_t}{P_{t-1}}\\quad\\text{ o }\\quad P_t=P_{t-1}(1+R_t) \\tag{1.1} \\end{equation}\\] El correspondiente rendimiento neto simple de un periodo o rendimiento simple es \\[\\begin{equation} R_t=\\frac{P_t}{P_{t-1}}-1=\\frac{P_t-P_{t-1}}{P_{t-1}} \\tag{1.2} \\end{equation}\\] Definición 1.2 (Rendimiento simple multiperiodo) Mantener el activo fijo durante \\(k\\) periodos entre los tiempos \\(t-k\\) y \\(t\\) da un rendimiento bruto simple de periodo \\(k\\) \\[\\begin{eqnarray*} 1+ R_t[k] &amp;=&amp; \\frac{P_t}{P_{t-k}} = \\frac{P_t}{P_{t-1}}\\times\\frac{P_{t-1}}{P_{t-2}}\\times \\cdots \\times\\frac{P_{t-k+1}}{P_{t-k}} \\\\ &amp;=&amp; (1+R_t)(1+R_{t-1})\\cdots(1+R_{t-k+1}) \\\\ &amp;=&amp; \\prod_{j=0}^{k-1}(1+R_{t-j}) \\end{eqnarray*}\\] De la definición, se tiene que la rentabilidad bruta simple de periodo \\(k\\) es el producto de las \\(k\\) rentabilidades brutas simples de un periodo. Esto se llama rendimiento compuesto. El rendimiento neto simple de periodo \\(k\\) es \\(R_t[k]=(P_t-P_{t-k})/P_{t-k}\\). En la práctica, el intervalo de tiempo real es importante para discutir y comparar los rendimientos (por ejemplo, rendimiento mensual o rendimiento anual). Si no se indica el intervalo de tiempo, se asume implícitamente que es de un año. Si el activo se mentuvo durante \\(k\\) años, entonces el rendimiento anualizado se define como \\[\\text{Anualizado}\\{R_t[k]\\} = \\left[\\prod_{j=0}^{k-1}(1+R_{t-j})\\right]^{1/k}-1.\\] Esta es una media geométrica de los \\(k\\) rendimientos brutos simple de un periodo y lo podemos calcular por \\[\\text{Anualizado}\\{R_t[k]\\} = \\exp\\left[\\frac{1}{k}\\sum_{j=0}^{k-1}\\ln(1+R_{t-j})\\right]-1.\\] Debido a que es más fácil calcular el promedio aritmético que la media geométrica y los rendimientos de un periodo tienden a ser pequeños, podemos utilizar el desarrollo de Taylor de primer orden para aproximar el rendimiento anualizado y obtener \\[\\begin{equation} \\text{Anualizado}\\{R_t[k]\\} \\approx \\frac{1}{k}\\sum_{j=0}^{k-1}R_{t-j} \\tag{1.3} \\end{equation}\\] Sin embargo, la exactitud de la aproximación en la ecuación (1.3) puede no ser suficiente en algunas aplicaciones. Otra definición útil es la de rendimiento compuesto continuo, pero antes de discutir tales definiciones, discutamos el efecto de la capitalización. Supongamos que la tasa de interés de un depósito bancario es del 10% anual y el depósito inicial es de \\(\\$1,00\\). Si el banco paga intereses una vez al año, entonces el valor neto del depósito se convierte en \\(\\$1,00(1+0.10)=\\$1,10\\) un año después. Si el banco paga intereses semestralmente, el tipo de interés a 6 meses es \\(10\\%/2=5\\%\\) y el valor del depósito será \\(\\$1,00(1+0.10/2)2=\\$1,1025\\) después del primer año. En general, si el banco para intereses \\(m\\) veces al año, entonces la tasa de interés para cada pago es \\(10\\%/m\\) y el valor neto del depósito se convierte en \\(\\$1.00(1+0.1/m)m\\) un año después. La tabla siguiente da los resultados para algunos intervalos de tiempo comúnmente usados. En particular, el valor neto se aproxima a \\(\\$1,1052\\), que se obtiene con \\(\\exp(0.1)\\) y se refiere al resultado de la capitalización continua. Tipo Número de pagos Tasa de interés por periodo Valor neto Anual 1 0.1 $1.10000 Semestral 2 0.05 $1.10250 Trimestral 4 0.025 $1.10831 Mensual 12 0.0083 $1.10471 Semanal 52 0.1/52 $1.10506 Diario 365 0.1/365 $1.10516 Continuo Inf. $1.10517 Tabla. Ilustración de los efectos de la combinación. El intervalo de tiempo es de 1 año y la tasa de interés es del 10% anual. En general, el valor liquidativo \\(A\\) de la capitalización continua es \\[\\begin{equation} A=C\\exp(r\\times n) \\tag{1.4} \\end{equation}\\] donde \\(r\\) es el tipo de interés anual, \\(C\\) es el capital inicial y \\(n\\) es el número de años. A partir de la ecuación (1.4), tenemos \\[\\begin{equation} C=A\\exp(-r\\times n) \\tag{1.5} \\end{equation}\\] el cual se refiere como el valor presente de un activo que vale \\(A\\) dolares \\(n\\) años a partir de ahora, asumiendo que la tasa de interés compuesta continua es \\(r\\) por año. Definición 1.3 (Rendimiento compuesto continuo) El logaritmo natural de rendimiento bruto simple de un activo se denomina rendimiento compuesto continuo o rendimiento logarítmico \\[\\begin{equation} r_t=\\ln(1+R_t) = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right) = p_t-o_{t-1} \\tag{1.6} \\end{equation}\\] donde \\(p_t=\\ln(P_t)\\). Los rendimientos compuestos continuos deisfrutan de algunas ventajas sobre los rendimientos netos simples \\(R_t\\). En primer lugar, consideremos los rendimientos multiperiodos. Tenemos \\[\\begin{eqnarray*} r_t[k] &amp;=&amp; \\ln(1+R_t[k]) = \\ln[(1+R_t)(1+R_{t-1})\\cdots(1+R_{t-k+1})]\\\\ &amp;=&amp; \\ln(1+R_t)+\\ln(1+R_{t_1})+\\cdots+\\ln(1+R_{t-k+1})\\\\ &amp;=&amp; r_t+r_{t-1}+\\cdots+r_{t-k+1}. \\end{eqnarray*}\\] Por lo tanto, el rendimiento multiperiodo compuesto continuo es simplemente la suma de los rendimientos compuesto continuo de un periodo involucrados. En segundo lugar, las propiedades estadísticas de los logaritmos de los rendimientos son más manejables. Definición 1.4 (Rentabilidad de la cartera) El rendimiento neto simple de una cartera de inversión compuesta por \\(N\\) activos es una media ponderada de los rendimientos netos simples de los activos en cuestión, en la que la ponderación de cada activo es el porcentaje del valor de la cartera invertido en ese activo. Sea \\(p\\) un portafolio que ponga peso con el activo \\(i\\), entonces el rendimineto simple \\(p\\) en el tiempo \\(t\\) es \\[R_{p,t} = \\sum_{i=1}^N\\omega_iR_{it}\\] donde \\(R_{it}\\) es el rendimiento simple del activo \\(i\\). Los rendimientos compuestos continuos de una cartera, sin embargo, no tienen la propiedad conveniente anterior. Si los rendimientos simples \\(R_t\\) son todos pequeños en magnitud, entonces tenemos \\[r_{p,t} \\approx \\sum_{i=1}^N\\omega_ir_{it}\\] donde \\(r_{p,t}\\) es el rendimiento compuesto continuo de la cartera en el momento \\(t\\). Esta aproximación se utiliza a menudo para estudiar los rendimientos de las carteras. Definición 1.5 (Pago de dividendos) Si un activo paga dividendos periódicamente, debemos modificar las definiciones de rendimientos de activos. Sea \\(D_t\\) el pago de dividendos de un activo entre los tiempos \\(t-1\\) y \\(t\\) y sea \\(P_t\\) el precio del activo al final del periodo \\(t\\). Entonces el dividendo no se incluye en \\(P_t\\). Entonces el rendimiento neto simple y el rendimiento compuesto continuo en el tiempo \\(t\\) están dados por \\[\\begin{eqnarray*} R_t &amp;=&amp; \\frac{P_t+D_t}{P_{t-1}}-1 \\\\ r_t &amp;=&amp; \\ln(P_t+D_t) - \\ln(P_{t-1}) \\end{eqnarray*}\\] Definición 1.6 (Exceso de rendimiento) El rendimiento excesivo de un activo en el momento \\(t\\) es la diferencia entre el rendimiento del activo y el rendimiento de algún activo de referencia. A menudo se considera que el activo de referencia no tiene riesgo, como una devolución de letras del Tesoro de EE.UU. a corto plazo. El exceso de rentabilidad simple y el logaritmo de exceso de rentabilidad de un activo se definen como \\[\\begin{equation} Z_t = R_t-R_{0t}; \\quad z_t=r_t-r_{0t} \\tag{1.7} \\end{equation}\\] donde \\(R_{0t}\\) y \\(r_{0t}\\) son los rendimientos simples y logarítmicos del activo de referencia, respectivamente. En la literatura financiera, el exceso de rentabilidad se considera como el pago de una cartera de arbitraje que va larga en un activo y corta en el activo de referencia sin inversión inicial neta. ** Resumen de la relación** Las relaciones entre el rendimiento simple \\(R_t\\) y el rendimiento compuesto continuo (o logarítmico) \\(r_t\\) son \\[r_t=\\ln(1+R_t), \\qquad R_t=e^{r_t}-1\\] La agregación temporal de los rendimientos produce \\[\\begin{eqnarray*} 1+R_t[k] &amp;=&amp; (1+R_t)(1+R_{t-1})\\cdots(1+R_{t-k+1}) \\\\ r_t[k] &amp;=&amp; r_t+r_{t-1}+\\cdots+r_{t-k+1} \\end{eqnarray*}\\] Si el tipo de interés compuesto continuo es por año, entonces la relación entre los valores presentes y futuros de un activo fijo es \\[A = C\\exp(r\\times n),\\qquad C=A\\exp(-r\\times n).\\] 1.2 Conceptos básicos Una serie tiempo es una secuencia de observaciones, medidos en determinados momentos del tiempo, ordenados cronológicamente y, espaciados entre sí de manera uniforme, así los datos usualmente son dependientes entre sí. El principal objetivo de una serie de tiempo es su análisis para hacer pronóstico. Formalmente se tiene la siguiente definición. Definición 1.7 Una serie de tiempo es un conjunto de observaciones \\(x_t\\), cada una registrada a un tiempo específico \\(t\\). Definición 1.8 Un modelo de series de tiempo para los datos observados \\(\\{x_t\\}\\) es una especificación de una distribución conjunta (o posiblemente solo de medias y covarianzas) de una sucesión de variables aleatorias \\(\\{X_t\\}\\) de las cuales \\(\\{x_t\\}\\) es una realización. A continuación presentaremos una serie de ejemplos que demuestran la utilidad y lo cotidiano de las series de tiempo, también se mostrarán los códigos en R para cargar los archivos de datos y graficar las respectivas series de tiempo. 1.3 Ejemplos Ejemplo 1.1 (Beneficios de acciones) Beneficios por acción trimestrales para la compañía Johnson y Johnson. Se tienen 84 trimestres iniciando el primer trimestre de 1960 hasta el último trimestre de 1980. Los métodos para analizar tales datos se verán en el Tema 3 usando técnicas de regresión. El archivo es “jj.txt”. Los comandos en R para cargar el archivo y graficar la serie de tiempo son los siguientes: jj=ts(scan(&quot;data/jj.txt&quot;),start=1960,freq=4) plot(jj, type=&quot;l&quot;,ylab=&quot;Beneficios por acción trimestrales&quot;) Figura 1.1: Beneficios por acción trimestrales para la compañía Johnson y Johnson Ejemplo 1.2 El archivo “ReservasInternacionales.xlsx”, contiene el registro mensual de Reservas Internacionales Venezolanas en millones de dólares ($), iniciando en el mes de enero de 1996 hasta el mes de diciembre de 2017 library(readxl) reservas &lt;- read_excel(&quot;data/ReservasInternacionales.xlsx&quot;) reservas=ts(reservas,start = 1996,frequency = 12) plot.ts(reservas[,2], xlab=&quot;Año&quot;,ylab=&quot;Monto&quot;, main=&quot;Reservas Internacionales de Venezuela (millones $)&quot;) Figura 1.2: Reservas Internacionales de Venezuela (millones $) 1996-2017 Ejemplo 1.3 El archivo “PreciosPetroleoVzla.xlsx” contiene el precio promedio mensual de venta para el petróleo venezolano (en dólares) desde enero 2006 hasta noviembre 2017 library(readxl) petroleo &lt;- read_excel(&quot;data/PreciosPetroleoVzla.xlsx&quot;) petroleo=ts(petroleo,start = 2006,frequency = 12) plot.ts(petroleo[,2], xlab=&quot;Año&quot;,ylab=&quot;Monto&quot;, main=&quot;Precio promedio del petróleo venezolano (en dolares $)&quot;) Figura 1.3: Precio promedio del petróleo venezolano (en dolares $) 2006-2017 Ejemplo 1.4 El archivo “IndiceDowJones.xlsx” contiene los valores histórico del Índice Dow-Jones desde enero de 1930 hasta octubre de 2017. En el archivo podems notar que desde enero de 1930 hasta diciembre de 1994, los registros son el promedio semanal, a partir de enero de 1995, los registros son diarios. La primera columa es la fecha, la segunda columna es el valor de apertura, la tercera columna el valor máximo, la cuarta el valor mínimo, la quinta el último valor del índice o valor de cierre y la sexta columna es el volumen de acciones. DJ=read_excel(&quot;data/IndiceDowJones.xlsx&quot;) DJ=ts(DJ) plot.ts(DJ[,-1], xlab=&quot;Días&quot;, main=&quot;Índice Dow-Jones desde enero 1930 hasta octubre 2017&quot;) Figura 1.4: Índice Dow-Jones desde enero 1930 hasta octubre 2017 Ejemplo 1.5 La figura siguiente muestra los porcentajes de cambio diario de la Bolsa de Valores de New York desde el 2 de febrero de 1984 hasta el 31 de diciembre de 1991. Como se ve hay una caída fuerte, esta ocurrió el 19 de octubre de 1987 en \\(t=938\\). El archivo de datos es “nyse.txt”. NYSE=ts(scan(&quot;data/nyse.txt&quot;)) plot(NYSE,xlab=&quot;Tiempo&quot;,ylab=&quot;Porcentaje de cambio, NYSE&quot;) Figura 1.5: Porcentaje de cambio de la bolsa de New York Ejemplo 1.6 La evolución del EURIBOR es algo que fluctúa a diario. Se entiende por EURIBOR (Euro Interbank Offered Rate) el tipo de interés, promovido por el Instituto Europeo de Mercados Monetarios (EMMI), consistente en la media aritmética simple de los valores diarios con días de mercado para operaciones de depósitos en euros a plazo de uno/tres/seis/doce meses y referido al día quince del mes anterior al comienzo de cada período de interés o al día siguiente hábil si aquel no lo fuese, calculado a partir del ofertado por una muestra de Bancos para operaciones entre entidades de similar calificación. A continuación mostramos dos series del EURIBOR. La primera es la evolución histórica anual del EURIBOR desde su implantación en 1999 hasta 2018, los datos se corresponden al mes de enero de cada año. La segunda es la evolución mensual desde enero de 2007 hasta marzo de 2018. EURIBORa&lt;-read_excel(&quot;data/EURIBOR-anual.xlsx&quot;) plot(EURIBORa,type=&quot;l&quot;, col = &quot;blue&quot;, xlab = &quot;Periodo&quot;, main=&quot;Serie EURIBOR anual (1999-2018)&quot;) grid(col = &quot;gray&quot;) Figura 1.6: Evolución anual del EURIBOR (1999-2018) EURIBORm&lt;-read_excel(&quot;data/EURIBOR-mensual.xlsx&quot;) EURIts&lt;-ts(EURIBORm[,2],start = 2007, frequency = 12) plot.ts(EURIts,xlab = &quot;Periodo&quot;, col = &quot;blue&quot;, main=&quot;Serie EURIBOR mensual (enero 2007- marzo 2018)&quot;) grid(col = &quot;gray&quot;) Figura 1.7: Evolución mensual del EURIBOR (2007-2018) Ejemplo 1.7 El archivo Cambio-EUR-USD.xlsx contiene el histórico de la cotización dolar estadounidense versus el euro desde el 01/05/2017 hasta el 26/04/2018. En la primera columna se muestra la fecha, la segunda columna el precio de apertura, la tercera el precio de cierre, la cuarta la diferencia en %, la quinta el precio máximo del día, la sexta el precio mínimo y la utlima el volumen de transacciones. A continuación presentamos los gráficos de apertura, cierre, máximo y mínimo. Cambio&lt;-read_excel(&quot;data/Cambio-EUR-USD.xlsx&quot;) par(mfrow=c(3,2)) plot(Cambio$Fecha,Cambio$Apertura, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Cotizacion&quot;, main = &quot;Apertura&quot;) plot(Cambio$Fecha,Cambio$Cierre, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Cotizacion&quot;, main = &quot;Cierre&quot;) plot(Cambio$Fecha,Cambio$Máximo, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Cotizacion&quot;, main = &quot;Máximo&quot;) plot(Cambio$Fecha,Cambio$Mínimo, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Cotizacion&quot;, main = &quot;Mínimo&quot;) plot(Cambio$Fecha,Cambio$`Dif.%`, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Porcentaje&quot;, main = &quot;Diferencia (apetura-cierre) %&quot;) plot(Cambio$Fecha,Cambio$Volumen, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Monto&quot;, main = &quot;Volumen&quot;) Figura 1.8: Histórico de cambio del USD vs. EUR (01/05/2017 al 26/04/2018) 1.3.1 Clasificación de las series de tiempo Como se ha mostrado en los ejemplos anteriores, hay una amplia variedad de series de tiempo que pueden clasificarse en varias categorías desde varios puntos de vista. Series de tiempo continuas y discretas. Los datos registrados continuamente, por ejemplo, por un dispositivo analógico, se denominan series de tiempo continuas. Por otra parte, los datos observados en ciertos intervalos de tiempo, como la presión atmosférica medida cada hora, se denominan series de tiempo discretas. Existen dos tipos de series de tiempo discretas: una en la que las observaciones de los datos se realizan a intervalos de igual espaciamiento y otra en la que las observaciones de los datos se realizan a intervalos de espaciamiento desigual. Aunque las series de tiempo mostradas en los ejemplos anteriores están conectadas continuamente por líneas sólidas, todas ellas son series de tiempo discretas. A partir de ahora en este libro, consideramos sólo series de tiempo discretas registradas a intervalos igualmente espaciados, porque las series de tiempo que analizamos en ordenadores digitales son generalmente series de tiempo discretas. Series de tiempo univariadas y multivariadas. Las series de tiempo que consisten en una sola observación en cada punto temporal, como se muestran en los ejemplos 1.1, 1.2, 1.3 y 1.5, se denominan series de tiempo univariadas. Por otra parte, las series de tiempo que se obtienen grabando simultáneamente dos o más fenómenos como los ilustrados en el ejemplo 1.4 se denominan series de tiempo multivariadas. Sin embargo, puede ser difícil distinguir entre series de tiempo univariadas y multivariadas desde su naturaleza; más bien, la distinción se hace desde el punto de vista del analista y por varios otros factores, como la restricción de la medición y los conocimientos empíricos o teóricos sobre el tema. Desde el punto de vista del modelado estadístico, la selección de variables en sí misma es un problema importante en el análisis de series de tiempo. Series de tiempo estacionarias y no estacionarias. Una serie de tiempo es un registro de un fenómeno que varía irregularmente con el tiempo. En el análisis de series de tiempo, las series de tiempo de variación irregular se expresan generalmente mediante modelos estocásticos. En algunos casos, un fenómeno aleatorio puede ser considerado como la realización de un modelo estocástico con una estructura de variación temporal. Estas series de tiempo se denominan series de tiempo estacinarias. El ejemplo 1.5 es un ejemplo típico de una serie de tiempo estacionaria. Por otra parte, si la estructura estoc?stica de una serie de tiempo cambia con el tiempo, se denomina serie de tiempo no estacionaria. Como ejemplos típicos de series de tiempo no estacionarias, considere la serie en los ejemplos 1.1 a 1.4 . Se puede observar que los valores medios cambian a lo largo del tiempo. Series de tiempo gaussianas y no gaussianas. Cuando una distribución de una serie de tiempo sigue una distribución normal, la serie de tiempo se denomina serie de tiempo gaussiana; de lo contrario, se denomina serie de tiempo no gausiana. La mayoría de los modelos considerados en este libro son modelos gaussianos, asumiendo que las series de tiempo siguen distribuciones gaussianas. Al igual que en el caso del ejemplo 1.3, el patrón de las series de tiempo es a veces asimétrico, de modo que la distribución marginal no puede considerarse gaussiana. Incluso en tal situación, podemos obtener una serie de tiempo gaussiana aproximada mediante una transformación de datos apropiada. Series de tiempo lineales y no lineales. Una serie de tiempo expresable como la salida de un modelo lineal se denomina serie de tiempo lineal. Por el contrario, la salida de un modelo no lineal se denomina serie de tiempo no lineal. Datos faltantes y valores atípicos. En el modelado de series de tiempo de problemas del mundo real, a veces necesitamos tratar con observaciones faltante y valores atípicos. Algunos valores de las series de tiempo que no se han registrado por algunas razones se denominan observaciones que faltan en las series de tiempo. Los valores atípicos (observaciones exteriores) pueden ocurrir debido al comportamiento extraordinario del objeto, mal funcionamiento del dispositivo de observación o errores en el registro. En los datos de los ejemplos 1.4 y 1.5 se pueden observar datos atípicos. En el ejemplo 1.4 podemos notar caídas en los índices del DowJones y en el ejemplo 1.4 podemos notar una fuerte caída en el porcentaje de cambio de diario ocurrido el 19 de octubre de 1987. 1.4 Componentes de una serie de tiempo El análisis clásico de las series de tiempo se basa en la suposición de que los valores que toma la variable de observación es la consecuencia de tres componentes, cuya actuación conjunta da como resultado los valores medidos, estos componentes son: Componente de tendencia. Se puede definir como un cambio a largo plazo que se produce en la relación al nivel medio, o el cambio a largo plazo de la media. La tendencia se identifica con un movimiento suave de la serie a largo plazo. Componente estacional. Muchas series de tiempo presentan cierta periodicidad o dicho de otro modo, variación de cierto período (semestral, mensual, etc.). Por ejemplo las Ventas al Detalle en Puerto Rico aumentan por los meses de noviembre y diciembre por las festividades navideñas. Estos efectos son fáciles de entender y se pueden medir explícitamente o incluso se pueden eliminar de la serie de datos, a este proceso se le llama desestacionalización de la serie. Componente aleatoria. Esta componente no responde a ningún patrón de comportamiento, sino que es el resultado de factores fortuitos o aleatorios que inciden de forma aislada en una serie de tiempo. De los tres componentes anteriores los dos primeros son componentes determinísticos, mientras que la última es aleatoria. Los modelos que se utilizan con más frecuencia son: Modelo aditivo: \\(X_t=T_t+E_t+\\epsilon_t\\) Modelos multiplicativos: Puro: \\(X_t = T_t\\times E_t\\times\\epsilon_t\\) Mixto: \\(X_t = T_t\\times E_t+\\epsilon_t\\) La elección de uno de estos modelos se hará de manera que el modelo seleccionado sea capaz de agrupar las principales características observadas en el gráfico de la serie en estudio. 1.4.1 El Modelo Aditivo de Componentes de Series de Tiempo Dada una serie \\(X_t, t=1,\\ldots,n\\), el Modelo Aditivo de Componentes consiste en asumir que \\(X_t\\) se puede descomponer en tres componentes: \\[\\begin{equation} X_t = T_t+E_t+\\epsilon_t \\tag{1.8} \\end{equation}\\] donde \\(T_t\\) es la componente de tendencia, \\(E_t\\) es la componente estacional y \\(\\epsilon_t\\) es la componente aleatoria o de errores. Las componentes \\(T_t\\) y \\(E_t\\) son funciones de \\(t\\) determinísticas. Su evolución es perfectamente predecible. Este modelo es apropiado cuando la magnitud de la fluctuaciones estacionales de la serie no varía al hacerlo la tendencia. La componente \\(T_t\\) en algunos casos también puede ser una componente estacional, pero de baja frecuencia, o, equivalentemente, una componente con período muy grande. Por ejemplo, en una serie diaria, \\(E_t\\) puede tener período 30 días, y \\(T_t\\) período 360 días. En la Figura se muestra la idea de la descomposición. Al superponer las series en los gráficos (a), (b) y (c) se obtiene la serie en el gráfico (d). Figura 1.9: Modelo aditivo de series de tiempo Asumiendo el modelo aditivo, el análisis de series de tiempo consiste en modelar y estimar \\(T_t\\) y \\(E_t\\) y luego extraerlas de \\(X_t\\) para obtener \\(\\hat{\\epsilon}_t = X_t - \\hat{T}_t - \\hat{E}_t\\). La serie \\(\\hat{\\epsilon}_t\\) se modela y estima para finalmente reconstruir \\(X_t\\), \\(\\hat{X}_t = \\hat{T}_t+\\hat{E}_t+\\hat{\\epsilon}_t\\), y poder realizar el pronóstico \\(\\hat{X}_{t+h}=\\hat{T}_{t+h}+\\hat{E}_{t+h}+\\hat{\\epsilon}_{t+h}\\), utilizando la información disponible \\(X_t,\\ldots,X_n\\) con \\(h=1,2,\\ldots,m\\). Sin embargo, puede suceder que la serie \\(\\hat{\\epsilon}_t\\) sea incorrelacionada, es decir, \\(Corr(\\hat{\\epsilon}_t,\\hat{\\epsilon}_{t+s}) = 0\\), para \\(s\\neq0\\). En este caso \\(\\hat{\\epsilon}_{t+h}=0\\) para todo \\(h&gt;0\\). En R podemos descomponer una serie de tiempo usando la función stl() o la función decompose(). Retomando la serie de beneficios trimestrales de las acciones de Johnson y Johnson (Ejemplo 1.1) podemos observar la descomposición de la misma. En la parte superior de la gráfica se observa la serie original, en el gráfico siguiente la estacionalidad, en el tercero la tendencia y en el gráfico inferior los residuales. plot(decompose(jj, type = &quot;additive&quot;, filter = NULL)) Figura 1.10: Descomposición aditiva de la serie Johnson y Johnson La función stl() es más sofisticada que decompose(), la misma usa la descomposición de estacionalidad y tendencia de Loess (Seasonal and Trend decomposition using Loess) el cual es un método robusto y versátil para la descomposición de series de tiempo. El método STL fue desarrollado por Cleveland et al. (1990). A continuación mostramos la misma serie de beneficios de acciones de Johnson y Johnson usando esta función. plot(stl(jj,s.window=&quot;periodic&quot;), col=&quot;blue&quot;, main=&quot;Descomposicion de la serie Johnson y Johnson&quot;) Figura 1.11: Descomposición de la serie Johnson y Johnson usando la descomposición de Loess (STL) 1.4.2 El Modelo Multiplicativo de Componentes de Series de Tiempo Dada una serie de tiempo \\(X_t,t=1,\\ldots,n\\), el Modelo Multiplicativo de Componentes consiste en asumir que \\(X_t\\) se puede descomponer de una de las siguientes maneras: Puro: \\[\\begin{equation} X_t = T_t\\times E_t\\times\\epsilon_t \\tag{1.9} \\end{equation}\\] Mixto: \\[\\begin{equation} X_t = T_t\\times E_t+\\epsilon_t \\tag{1.10} \\end{equation}\\] donde \\(T_t\\) es la componente de tendencia, \\(E_t\\) es la componente estacional y \\(\\epsilon_t\\) es la componente aleatoria o de errores. Estos modelos son apropiados cuando la magnitud de las fluctuaciones estacionales de la serie crece y decrece proporcionalmente con los crecimientos y decrecimientos de la tendencia respectivamente. Usamos la misma función decompose() para realizar la descomposición multiplicativa de la serie de tiempo, para ello en ‘type’ cambiamos “additive” por “multiplicative” plot(decompose(jj, type = &quot;multiplicative&quot;, filter = NULL)) Figura 1.12: Descomposición multiplicativa de la serie Johnson y Johnson "],
["caracteristicas-de-series-de-tiempo.html", "Capítulo 2 Características de series de tiempo 2.1 Medidas de dependencia para series de tiempo 2.2 Estimación de la Tendencia 2.3 Estimación de la tendencia por regresión clásica", " Capítulo 2 Características de series de tiempo El objetivo primario en el análisis de Series de Tiempo es desarrollar modelos matemáticos que provean una descripción apropiada para los datos muestrales, como los vistos en los ejemplos del capítulo anterior. Así, lo primero que hacemos es utilizar la definición 1.7, para tener un soporte estadístico. En este capítulo daremos algunas definiciones que serán de uso general en todo el resto del libro, también sedescribiran algunos métodos para el análisis exploratorio de las series de tiempo 2.1 Medidas de dependencia para series de tiempo Definición 2.1 Un proceso estocástico es una familia de variables aleatorias indexadas \\(x(\\omega,t)\\) ó \\(x_t(\\omega)\\) donde \\(t\\) pertenece a un conjunto de índices \\(T\\) y \\(\\omega\\) pertenece a un espacio muestral \\(\\Omega\\). Si \\(t=t^*\\) fijo, \\(x(\\omega,t^*)\\) es una variable aleatoria. Si \\(\\omega=\\omega^*\\) fijo, \\(x(\\omega^*,t)\\) es una función de \\(t\\), y se llama una realización del proceso. Una serie de tiempo es la realización de un proceso estocástico. Una descripción completa de una serie de tiempo, observada como una colección de \\(n\\) variables aleatorias en puntos de tiempo enteros arbitrarios \\(t_1,t_2,\\ldots,t_n\\), para cada entero positivo \\(n\\), es proporcionada por la función de distribución conjunta, evaluada como la probabilidad de que los valores de la serie sean conjuntamente menor que \\(n\\) constantes \\(c_1,c_2,\\ldots,c_n\\), esto es \\[\\begin{equation} F(c_1,c_2,\\ldots,c_n)=P(x_{t_1}\\leq c_1,x_{t_2}\\leq c_2,\\ldots,x_{t_n}\\leq c_n). \\tag{2.1} \\end{equation}\\] Desafortunadamente, la función de distribución multidimensional usualmente no se puede escribir fácilmente a menos que las variables aleatorias tengan distribución normal conjunta, en cuyo caso, la ecuación (2.1) llega a ser la distribución normal multivariada usual. Un caso particular en la cual la función de distribución multidimensional es fácil de escribir, será en el caso de variables aleatorias normal estándar independientes e idénticamente distribuidas, para lo cual la función de distribución se puede expresar como el producto de las distribuciones marginales, es decir, \\[\\begin{equation} F(c_1,c_2,\\ldots,c_n)=\\prod_{t_1}^{n}\\Phi(c_t) \\tag{2.2} \\end{equation}\\] donde \\[\\begin{equation} \\Phi(x)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x}\\mathbb{E}xp\\left\\{-\\frac{z^2}{2}\\right\\}dz\\tag{2.3} \\end{equation}\\] es la función de distribución normal estándar acumulada. Aunque la función de distribución multidimensional describa los datos completamente, esto es un instrumento poco manejable para mostrar y analizar datos de series de tiempo. La función de distribución (2.1) debe ser evaluada como una función de \\(n\\) argumentos, entonces cualquier graficación de las correspondientes funciones de densidad multivariante es prácticamente imposible. La función de distribución unidimensional \\[F_t(x)=P\\{x_t\\leq x\\}\\] o la correspondiente función de densidad unidimensional \\[f_t(x)=\\frac{\\partial F_t(x)}{\\partial x},\\] cuando existen, a menudo son más útiles para determinar si una coordenada en particular de la serie de tiempo tiene una función de densidad conocida, como la distribución normal (gaussiana), por ejemplo. Definición 2.2 La función de media es definida como \\[\\begin{equation} \\mu_{xt}=\\mathbb{E}(x_t)=\\int_{-\\infty}^{\\infty}xf_t(x)dx, \\tag{2.4} \\end{equation}\\] en caso de que exista, donde \\(\\mathbb{E}\\) denota el operador usual de esperanza. Cuando no haya confusión sobre a que serie de tiempo nos referimos, escribiremos \\(\\mu_{xt}\\) como \\(\\mu_t\\). Lo importante de comprender sobre \\(\\mu_t\\) consiste en que es una media teórica para la serie de tiempo en un punto particular, donde la media se asume o calcula sobre todos los posibles eventos que podrían haber producido \\(x_t\\). Definición 2.3 La función de autocovarianza es definida como producto del segundo momento \\[\\begin{equation} \\gamma_x(s,t)=\\mathbb{E}[(x_s-\\mu_s)(x_t-\\mu_t)], \\tag{2.5} \\end{equation}\\] para todo \\(t\\) y \\(s\\). cuando no haya confusión en la existencia sobre a que serie nos referimos, escribiremos \\(\\gamma_x(s,t)=\\gamma(s,t)\\). Note que \\(\\gamma_x(s,t)=\\gamma_x(t,s)\\) para todo los puntos \\(s\\) y \\(t\\). La función de autocovarianza mide la dependencia lineal entre dos puntos de la misma serie en diferentes tiempos. La autocovarianza (2.5) es el promedio de los productos cruzados relacionado con la densidad conjunta \\(F(x_s,x_t)\\). Es claro que, para \\(s=t\\), la autocovarianza se reduce a la varianza (en el caso finito), dado que \\[\\begin{equation} \\gamma_x(t,t)=\\mathbb{E}[(x_t-\\mu_t)^2] \\tag{2.6} \\end{equation}\\] Otro función de medida de tendencia importante es la función de autocorrelación. Definición 2.4 La función de autocorrelación (ACF) (ACF, siglas en ingles: Autocorrelation Function) se define como \\[\\begin{equation} \\rho(s,t)=\\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s)\\gamma(t,t)}} \\tag{2.7} \\end{equation}\\] La \\(ACF\\) mide la predictibilidad lineal de una serie de tiempo en tiempo \\(t\\), digamos \\(x_t\\) usando solo el valor \\(x_s\\). Es fácil de demostrar que \\(-1\\leq\\rho(s,t)\\leq1\\) usando la desigualdad de Cauchy-Schwarz1 Si podemos predecir \\(x_t\\) exactamente de \\(x_s\\) a través de la relación lineal \\(x_t=\\beta_0+\\beta_1x_s\\) entonces la correlación será 1 cuando \\(\\beta_1&gt;0\\) y \\(-1\\) cuando \\(\\beta_1&lt;0\\). Definición 2.5 La función de covarianza cruzada entre dos series \\(x_t\\) y \\(y_t\\) se define como \\[\\begin{equation} \\gamma_{xy}(s,t)=\\mathbb{E}[(x_s-\\mu_{xs})(y_t-\\mu_{yt})] \\tag{2.8} \\end{equation}\\] Definición 2.6 La función de correlación cruzada (CCF) (CCF, siglas en ingles: Cross Correlation Function) es definida como \\[\\begin{equation} \\rho_{xy}(s,t)=\\frac{\\gamma_{xy}(s,t)}{\\sqrt{\\gamma_x(s,s)\\gamma_y(t,t)}} \\tag{2.9} \\end{equation}\\] Las definiciones anteriores de funciones de media y varianza son completamente generales. Aunque nosotros no hayamos hecho ninguna suposición especial sobre el comportamiento de las series de tiempo, muchos de los ejemplos precedentes han insinuado que puede existir una especie de regularidad en el comportamiento de las mismas. Introducimos la noción de regularidad que usa el concepto de estacionaridad, que ya hemos introducido empíricamente en el apartado 1.2.1 “Clasificación de las series de tiempo” Formalmente tenemos las siguientes definiciones de estacionaridad Definición 2.7 Una serie de tiempo estrictamente estacionaria es una serie para la cual el comportamiento probabilístico de cada sucesión de valores \\[\\{x_{t_1},x_{t_2},\\ldots,x_{t_k}\\}\\] es idéntico a la serie trasladada en el tiempo \\[\\{x_{t_1+h},x_{t_2+h},\\ldots,x_{t_k+h}\\}\\] Esto es, \\[\\begin{equation} P[X_{t_1}\\leq c_1,\\ldots,x_{t_k}\\leq c_k] = P[X_{t_1+h}\\leq c_1,\\ldots,x_{t_k+h}\\leq c_k] \\tag{2.10} \\end{equation}\\] para todo \\(k=1,2,\\ldots\\), todo puntos de tiempos \\(t_1,t_2,\\ldots,t_k\\) y números \\(c_1,c_2,\\ldots,c_k\\) y todo salto \\(h=\\pm0,\\pm1,\\pm2,\\ldots\\). Esta definición de estacionaridad es muy fuerte para la mayoría de las aplicaciones prácticas. Por ello necesitamos una versión menos fuerte que imponga menos condiciones sobre las distribuciones de probabilidad, ya que si observamos bien la ecuación (2.10), lo que nos dice la misma es que todas las posibles distribuciones de probabilidad deben ser iguales, lo que como ya indicamos en la práctica es muy difícil de compriobar aún para conjuntos de datos sencillos. La siguiente versión de estacionaridad solo impone condiciones sobre los dos primeros momentos de la serie Definición 2.8 Una serie de tiempo débilmente estacionaria \\(x_t\\), es un proceso de varianza finita tal que la función de media \\(\\mu_t\\) es constante y no depende del tiempo \\(t\\), la función de covarianza \\(\\gamma(t,s)\\) depende solo de las diferencias de \\(s\\) y \\(t\\), \\(|t-s|\\). Por consiguiente, usaremos el término estacionaridad para referirnos a estcionaridad débil; si un proceso es estacinario en el sentido estricto usaremos el término estrictamente estacionario. Nota. 1) Si una serie de tiempo es estrictamente estacionaria, entonces todos las funciones de distribución multivariadas para subconjuntos de variables deben coincidir con sus contrapartes en el conjunto trasladado, para todos los valores del parámetro \\(h\\). Por ejemplo para \\(k=1\\) La ecuación (2.10) implica que \\[\\begin{equation} P\\{x_s\\leq c\\}=P\\{x_t\\leq c\\} \\tag{2.11} \\end{equation}\\] para cada puntos \\(s\\) y \\(t\\). Esta declaración implica, por ejemplo, que si la probabilidad de un valor de una serie de tiempo muestreada cada hora es negativa a la 1:00a.m, la probabilidad a la 10:00a.m. es la misma. Además, si la función de media, \\(\\mu_t\\) de la serie \\(x_t\\) existe, (2.11) implica que \\(\\mu_s=\\mu_t\\) para todo \\(s\\) y \\(t\\), y por consiguiente \\(\\mu_t\\) debe ser constante. Cuando \\(k=2\\), podemos escribir la ecuación (2.10) como \\[\\begin{equation} P\\{x_s\\leq c_1,x_t\\leq c_2\\}=P\\{x_{s+h}\\leq c_1,x_{t+h}\\leq c_2\\} \\tag{2.12} \\end{equation}\\] para cada par de puntos \\(s\\) y \\(t\\) y salto \\(h\\). Entonces, si la función de varianza del proceso existe, (2.12) implica que la función de autocovarianza de la serie \\(x_t\\) satisface \\(\\gamma(s,t)=\\gamma(s+h,t+h)\\) para todos \\(s\\) y \\(t\\) y salto \\(h\\). Podemos interpretar este resultado diciendo que la función de autocovarianza del proceso depende sólo de las diferencias de tiempo entre \\(s\\) y \\(t\\), y no del tiempo actual. Es claro de la definición 2.7 de serie estrictamente estacionaria, que una serie de tiempo estrictamente estacionaria con varianza finita, también es una serie estacionaria. El recíproco no es cierto a menos que impongamos condicionaes adicionales. Un importante caso donde estacionaridad implica estricta estacionaridad es el caso de series de tiempo gaussianas. Ya que la función de media \\(\\mathbb{E}(x_t)=\\mu_t\\) de una serie de tiempo estacionaria es independiente del tiempo \\(t\\), escribimos \\[\\begin{equation} \\mu_t=\\mu \\tag{2.13} \\end{equation}\\] Debido a que la función de covarianza de una serie de tiempo estacionaria, \\(\\gamma(s,t)\\) en tiempos \\(s\\) y \\(t\\) depende sólo de la diferencia \\(|s-t|\\), podemos simplificar la notación. Sea \\(s=t+h\\), donde \\(h\\) representa el tiempo de traslación o salto, entonces \\[\\begin{eqnarray} \\gamma(s,t)&amp;=&amp;\\mathbb{E}[(x_{t+h}-\\mu)(x_t-\\mu)]\\\\ \\nonumber &amp;=&amp;\\mathbb{E}[(x_h-\\mu)(x_0-\\mu)]\\\\ &amp;=&amp;\\gamma(h,0) \\nonumber \\tag{2.14} \\end{eqnarray}\\] no depende del argumento de tiempo \\(t\\); asumiendo que \\(\\text{Var}(x_t)=\\gamma(0,0)&lt;\\infty\\). De ahora en adelante, por conveniencia, prescindiremos del segundo argumento de \\(\\gamma(h,0)\\), es decir, la función de covarianza se denotará \\(\\gamma(h)\\). Definición 2.9 La función de autocovarianza de una serie de tiempo estacionaria se escribirá como \\[\\begin{equation} \\gamma(h)=\\mathbb{E}[(x_{t+h}-\\mu)(x_t-\\mu)] \\tag{2.15} \\end{equation}\\] Definición 2.10 La función de autocorrelación (ACF) de una serie de tiempo estacionaria será escrita, usando (2.7) como \\[\\begin{equation} \\rho(h)=\\frac{\\gamma(t+h,t)}{\\sqrt{\\gamma(t+h,t+h)\\gamma(t,t)}}=\\frac{\\gamma(h)}{\\gamma(0)} \\tag{2.16} \\end{equation}\\] La desigualdad de Cauchy-Schwartz muestra nuevamente que \\(-1\\leq\\rho(h)\\leq1\\) para todo \\(h\\). ** Propiedades de la función de covarianza** Para el valor en \\(h=0\\), la función de autocovarianza \\[\\begin{equation} \\gamma(0)=\\mathbb{E}[(x_t-\\mu)^2] \\tag{2.17} \\end{equation}\\] es la varianza de la serie de tiempo; note que la desigualdad de Cauchy-Schwartz implica que \\(|\\gamma(h)|\\leq\\gamma(0)\\). La autocovarianza de una serie estacionaria es simétrica respecto al origen, esto es \\[\\begin{equation} \\gamma(h)=\\gamma(-h) \\tag{2.18} \\end{equation}\\] para todo \\(h\\). Esta propiedad se debe a que trasladar la serie por \\(h\\) significa que \\[\\begin{eqnarray*} \\gamma(h)&amp;=&amp;\\gamma(t+h-t)\\\\ &amp;=&amp;\\mathbb{E}[(x_{t+h}-\\mu)(x_t-\\mu)]\\\\ &amp;=&amp;\\mathbb{E}[(x_t-\\mu)(x_{t+h}-\\mu)]\\\\ &amp;=&amp;\\gamma(t-(t+h))\\\\ &amp;=&amp;\\gamma(-h) \\end{eqnarray*}\\] lo cual muestra como usar la notación para demostrar el resultado. Definición 2.11 Dos series de tiempo \\(x_t\\) y \\(x_s\\) se dice que son conjuntamente estacionarias si cada una de ellas es estacionaria y la función de correlación cruzada \\[\\begin{equation} \\gamma_{xy}(h)=\\mathbb{E}[(x_{t+h}-\\mu_x)(y_t-\\mu_y)] \\tag{2.19} \\end{equation}\\] es una función sólo del salto \\(h\\). Definición 2.12 La función de correlación cruzada (CCF) de dos series conjuntamente estacionarias \\(x_t\\) y \\(y_t\\) se define como \\[\\begin{equation} \\rho_{xy}(h)=\\frac{\\gamma_{xy}(h)}{\\sqrt{\\gamma_x(0)\\gamma_y(0)}} \\tag{2.20} \\end{equation}\\] De nuevo, tenemos el resultado \\(-1\\leq\\rho_{xy}(h)\\leq1\\) lo cual nos permite comparar los valores extremos -1 y 1 cuando vemos la relación entre \\(x_{t+h}\\) y \\(y_t\\). La función de correlación cruzada satisface \\[\\begin{equation} \\rho_{xy}(h)=\\rho_{yx}(-h) \\tag{2.21} \\end{equation}\\] lo cual se puede demostrar de manera similar que para (2.18). Ejemplo 2.1 (Estacionaridad conjunta) Considere las series \\(x_t\\) y \\(y_t\\) formadas por las sumas y diferencias de dos valores sucesivos de un ruido blanco respectivamente, esto es \\[x_t=w_t+w_{t-1}\\] y \\[y_t=w_t-w_{t-1}\\] donde \\(w_t\\) son variables aleatorias independientes con media cero y varianza \\(\\sigma_w^2\\). Es fácil demostrar que \\(\\gamma_x(0)=\\gamma_y(0)=2\\sigma_w^2\\) y \\(\\gamma_x(1)=\\gamma_x(-1)=\\sigma_w^2\\), \\(\\gamma_y(1)=\\gamma_y(-1)=-\\sigma_w^2\\). También \\[\\begin{eqnarray*} \\gamma_{xy}(1)&amp;=&amp;\\mathbb{E}[(x_{t+1}-0)(y_t-0)]\\\\ &amp;=&amp;\\mathbb{E}[(w_{t+1}+w_t)(w_t-w_{t-1})]\\\\ &amp;=&amp;\\sigma_w^2 \\end{eqnarray*}\\] porque solo uno de los productos es distinto de cero.\\ Similarmente, \\(\\gamma_{xy}(0)=0,\\gamma_{xy}(-1)=-\\sigma_w^2\\). Usando (), obtenemos \\[\\rho_{xy}(h)=\\begin{cases}0,&amp;h=0\\\\ 1/2,&amp;h=1\\\\ -1/2,&amp;h=-1\\\\ 0,&amp;|h|\\geq2\\end{cases}.\\] Claramente, las funciones de autocovarianza y correlación cruzada dependen solo del salto \\(h\\), por lo tanto las series son conjuntamente estacionarias. El concepto de estacionaridad débil forma la base para muchos de los análisis realizados con series de tiempo. Las propiedades fundamentales de la media (2.13) y la función de covarianza (2.15) son satisfechas por muchos modelos teóricos que aparecen para generar realizaciones muestrales apropiadas. Definición 2.13 Un proceso lineal \\(x_t\\) se define como una combinación lineal de variables aleatorias de ruido blanco \\(w_t\\), y está dado por \\[\\begin{equation} x_t=\\mu+\\sum_{j=-\\infty}^{\\infty}\\psi_jw_{t-j} \\tag{2.22} \\end{equation}\\] donde los coeficientes satisfacen \\[\\begin{equation} \\sum_{j=-\\infty}^{\\infty}|\\psi_j|&lt;\\infty \\tag{2.23} \\end{equation}\\] Para un proceso lineal, podemos demostrar que la función de autocovarianza está dada por \\[\\begin{equation} \\gamma(h)=\\sigma_w^2\\sum_{j=-\\infty}^{\\infty}\\psi_{j+h}\\psi_j \\tag{2.24} \\end{equation}\\] para todo \\(h\\geq0\\); recuerde que \\(\\gamma(-h)=\\gamma(h)\\). Finalmente como mencionamos anteriormente, un caso importante en el cual una serie débilmente estacionaria es también estrictamente estacionaria es la serie normal o gaussiana. Definición 2.14 Un proceso \\(\\{x_t\\}\\), se dice que es un proceso gaussiano si el \\(k\\)-ésimo vector dimensional \\(\\hat{x}=(x_{t_1},x_{t_2},\\ldots,x_{t_k})\\), para cada conjunto de puntos \\(t_1,t_2,\\ldots,t_k\\) y cada entero positivo \\(k\\) tiene distribución normal multivariada. Definiendo \\(k\\times1\\) vector de medias \\(\\hat{\\mu}=(\\mu_{t_1},\\mu_{t_2},\\ldots,\\mu_{t_k})&#39;\\) y la \\(k\\times k\\) matriz de covarianza positiva como \\(\\Gamma=\\{\\gamma(t_i,t_j);i,j=1,\\ldots,k\\}\\), la función de densidad normal multivariada se puede escribir como \\[\\begin{equation} f(\\hat{x})=(2\\pi)^{-k/2}|\\Gamma|^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\hat{x}-\\hat{\\mu})&#39;\\Gamma^{-1}(\\hat{x}-\\hat{\\mu})\\right\\} \\tag{2.25} \\end{equation}\\] donde \\(|\\cdot|\\) denota el determinante. Esta distribución forma la base para resolver problemas que envuelven inferencia estadística para series de tiempo. Si una serie de tiempo gaussiana \\(\\{x_t\\}\\) es débilmente estacionaria, entonces \\(\\mu_t=\\mu\\) y \\(\\gamma(t_i,t_j)=\\gamma(|t_i-t_j|)\\), de modo que el vector \\(\\hat{\\mu}\\) y la matriz \\(\\Gamma\\) son independientes del tiempo. Este hecho implica que todas las distribuciones finitas, (2.25) de la serie \\(\\{x_t\\}\\) dependen sólo del salto de tiempo y no del tiempo actual, y por consiguiente la serie debe ser estrictamente estacionaria. 2.2 Estimación de la Tendencia En esta sección introducimos la estimación de la tendencia. En esencia, existen dos métodos para estimar la tendencia y la componente estacional de una serie de tiempo: Método paramétrico: Se basa en Proponer modelos paramétricos para expresar la relación que guardan la tendencia y la componente estacional con el tiempo. Ajustar dichos modelos a la serie de tiempo (por ejemplo, a través del método de mínimos cuadrados). Aislar la tendencia y la componente estacional por medio de los modelos ajustados. Método no paramétrico: Se basa en Asumir “suavidad” en la relación que guardan la tendencia y la componente estacional con el tiempo. Aislar la tendencia y la componente estacional a través de la suavización del gráfico de la serie (aplicando, por ejemplo, filtros de promedios móviles). Hay otros métodos que no consideraremos en este libro, por ejemplo, wavelets. En ocasiones la expresión “suavizar una serie” es equivalente a “extracción de la tendencia de una serie”, y ambas equivalen a la estimación de la tendencia. A continuación presentamos una lista de posibles modelos para la tendencia \\(T_t\\): Lineal \\[\\begin{equation} T_t=\\beta_0+\\beta_1t \\tag{2.26} \\end{equation}\\] Cuadrático \\[\\begin{equation} T_t=\\beta_0+\\beta_1t+\\beta_2t^2 \\tag{2.27} \\end{equation}\\] Cúbico \\[\\begin{equation} T_t=\\beta_0+\\beta_1t+\\beta_2t^2+\\beta_3t^3 \\tag{2.28} \\end{equation}\\] Exponencial \\[\\begin{equation} T_t=\\exp(\\beta_0+\\beta_1t) \\tag{2.29} \\end{equation}\\] Logístico \\[\\begin{equation} T_t=\\frac{\\beta_2}{1+\\beta_1\\exp(-\\beta_0t)} \\tag{2.30} \\end{equation}\\] En la tendencia cuadrática podemos observar: Si \\(\\beta_1,\\beta_2&gt;0\\), \\(T_t\\) es monótona creciente. Si \\(\\beta_1,\\beta_2&lt;0\\), \\(T_t\\) es monótona decreciente. Si \\(\\beta_1&gt;0\\) y \\(\\beta_2&lt;0\\), \\(T_t\\) es cóncava. Si \\(\\beta_1&lt;0\\) y \\(\\beta_2&gt;0\\), \\(T_t\\) es convexa. Otro modelo propuesto para la tendencia es el dado por la siguiente definición. Definición 2.15 El modelo Logarítmico Lineal o Log-Lineal se define como \\[\\begin{equation} \\ln X_t = \\beta_0+\\beta_1t + \\epsilon_t \\tag{2.31} \\end{equation}\\] El modelo anterior corresponde a un modelo con tendencia lineal para el logaritmo de \\(X_t\\). En (2.31) al tomar exponencial se tiene \\(X_t = \\exp(\\beta_0+\\beta_1t + \\epsilon_t)\\), que es similar al modelo con tendencia exponencial (2.29). Sin embargo, son modelos diferentes y se estiman por métodos diferentes. Para la estimación de los parámetros \\(\\beta_0,\\beta_1,\\beta_2\\) en los modelos lineales (2.26), (2.27), (2.28) y (2.31) utilizaremos el método de mínimos cuadrados clásico (MCC). En este método los parámetros estimados son aquellos que producen el valor mínimo de la suma de errores cuadrados. Para los modelos (2.29) y (2.30) se usa el método de mínimos cuadrados no lineales, que también minimiza la suma de errores cuadrados. El modelo Log-Lineal (2.31) es equivalente, algebráicamente, a \\[X_t = \\exp(\\beta_0 + \\beta_1t + \\epsilon_t).\\] Sin embargo, este último modelo es no lineal y no coincide con el modelo exponencial,(2.29), \\(X_t = \\exp(\\beta_0+\\beta_1t)+\\epsilon_t\\). Es posible estimar por mínimos cuadrados ordinarios el modelo Log-Lineal y utilizar los parámetros estimados \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\) como valores iniciales en la estimación del modelo exponencial por mínimos cuadrados no lineales. Pero los parámetros estimados en ambos modelos no necesariamente coinciden. Aunque la serie tenga una componente estacional \\(E_t\\), \\(X_t = T_t + E_t + \\epsilon_t\\), solamente consideramos un modelo de regresión entre \\(X_t\\) y \\(T_t\\), tal que \\(X_t = T_t + \\eta_t\\), donde \\(\\eta_t\\) es el término de error, de forma que \\(\\eta_t=E_t+\\epsilon_t\\). Por ejemplo, En el caso lineal \\(T_t = \\beta_0 + \\beta_1t\\), ajustamos el modelo de regresión lineal: \\(X_t = \\beta_0 + \\beta_1t + \\eta_t\\). En el caso cuadrático \\(T_t = \\beta_0 +\\beta_1t+\\beta_2t^2\\), ajustamos el modelo de regresión cuadrático \\(X_t = \\beta_0+\\beta_1t+\\beta_2t^2 +\\eta_t\\). Nótese que en este caso hay que definir una variable explicativa adicional \\(t^2\\). En general, para que datos de series de tiempo sean estacionarias, es necesario hacer un promedio de productos en el tiempo. Como para datos de serie de tiempo es importante medir la dependencia entre los valores de la serie; al menos, debemos ser capaces de estimar las autocorrelaciones con precisión. Será difícil medir la dependencia de estos valores si la estructura de dependencia no es regular o si cambia en el tiempo. De ahí, que para realizar cualquier análisis estadístico significativo de datos de series de tiempo, será crucial que las funciones de media y autocovarianza satisfagan las condiciones de estacionaridad dadas en la Definición 2.8. A menudo, este no es el caso, y en esta sección daremos algunos métodos para lidiar con los efectos de no-estacionaridad sobre las propiedades estacionarias de las series a estudiar. Quizás la forma más fácil de trabajar con series no-estacionarias es el modelo de tendencia estacionaria donde el proceso tiene comportamiento estacionario alrededor de una tendencia. Podemos escribir este tipo de modelos como \\[\\begin{equation} X_t=T_t+Y_t \\tag{2.32} \\end{equation}\\] donde \\(X_t\\) son las observaciones, \\(T_t\\) denota la tendencia y \\(Y_t\\) es un proceso estacionario. Por lo general, una tendencia fuerte \\(T_t\\) puede oscurecer el comportamiento del proceso estacionario \\(Y_t\\), como veremos en ejemplos posteriores. De aquí, será una ventaja el que podamos remover la tendencia como un primer paso para un análisis exploratorio de los datos. Los pasos envuelven obtener un estimador razonable del componente de tendencia, llamémoslo \\(\\hat{T}_t\\) y entonces trabajar con el residual \\[\\begin{equation} \\hat{Y}_t=X_t-\\hat{T}_t. \\tag{2.33} \\end{equation}\\] El primer paso en el análisis de cualquier tipo de serie es un gráfico de los datos. Si existe alguna aparente discontinuidad en la serie, tal como un cambio súbito en el nivel de la serie, esto puede darnos una idea para el análisis de la serie, un primer paso sería dividir la serie en segmentos homogéneos. Si existen observaciones o datos “outliers”, estos deben ser estudiados con cuidado para verificar si existe alguna justificación para descartar estas observaciones, como por ejemplo si una observación ha sido registrada de algún otro proceso por error. La inspección del gráfico también podría sugerir la representación de los datos como una realización de un proceso, como el modelo clásico de descomposición dado por (1.8). Si la componente estacional y la componente aleatoria o ruido parecen incrementarse con el nivel del proceso entonces una transformación preliminar de los datos es a menudo usada para hacer que los datos transformados sean compatibles con el modelo (1.8). En esta sección discutiremos algunas técnicas para identificar y eliminar las componentes en (1.8). Nuestro objetivo es estimar y extraer las componentes determinísticas \\(T_t\\) y \\(E_t\\) con la esperanza de que el residual o la componente aleatoria \\(\\epsilon_t\\) llegue a ser un proceso estacionario. Entonces podremos usar la teoría de tales procesos para hallar un modelo probabilístico satisfactorio para el proceso \\(\\epsilon_t\\), analizar sus propiedades y usarlo en conjunto con \\(T_t\\) y \\(E_t\\) para hacer pronósticos y control de \\(X_t\\). Los dos enfoques para la eliminación de las componentes de tendencia y estacional son: Estimación de \\(T_t\\) y \\(E_t\\) en el modelo (1.8), Diferencia de los datos \\(X_t\\). Ilustraremos ambos enfoque con varios ejemplos 2.2.1 Estimación de la tendencia en ausencia de estacionalidad Si tenemos una serie de tiempo para la cual está ausente la componente estacional \\(E_t\\) el modelo (1.8) llega ser \\[\\begin{equation} X_t = T_t + \\epsilon_t,\\quad t=1,\\ldots,n \\tag{2.34} \\end{equation}\\] donde, sin perdida de generalidad, podemos suponer que \\(\\mathbb{E}(\\epsilon_t)=0\\). A continuación vamos a describir tres métodos para estimar la tendencia \\(T_t\\). Método T1: Estimación de \\(T_t\\) por mínimos cuadrados. El objetivo de este método es intentar ajustar una familia paramétrica de funciones como las vistas en las ecuaciones (2.26) a (??), a los datos eligiendo los parámetros que minimicen \\(\\sum_t(X_t-T_t)^2\\). Esto es, asumiendo que \\(\\mathbb{E}(\\epsilon_t)=0\\), se tiene \\[\\mathbb{E}(X_t)=T_t=f(t)\\] Una suposición común es que la función \\(f\\) depende de ciertos parámetros (desconocidos) \\(\\beta_1,\\ldots,\\beta_p\\), es decir, \\[\\begin{equation} f(t)=f(t;\\beta_1,\\ldots,\\beta_p) \\tag{2.35} \\end{equation}\\] Sin embargo, el tipo de función es conocida. Los parámetros \\(\\beta_1,\\ldots,\\beta_p\\) serán estimados a partir de una realización \\(x_t\\) de la variable aleatoria \\(X_t\\). La aproximación por estimación de mínimos cuadrados \\(\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_p\\) debe satisfacer \\[\\begin{equation} \\sum_t(x_t-f(t;\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_p))^2 = \\min_{\\beta_1,\\ldots,\\beta_p}\\sum_t(x_t-f(t;\\beta_1,\\ldots,\\beta_p))^2 \\tag{2.36} \\end{equation}\\] cuya solución, si existe, es un problema numérico. El valor \\(\\hat{x}_t=f(t;\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_p)\\) servirá como una predicción de futuros valores \\(x_t\\). Las diferencias observadas \\(x_t-\\hat{x}_t\\) son llamadas residuales. Ellas contienen información sobre la bondad de ajuste del modelo a los datos. Ejemplo 2.2 El archivo “USPOP.txt” contiene la información de la población de Estados Unidos de América desde 1780 hasta 1980 segun el censo poblacional cada 10 años. En el gráfico podemos observar que no existe estacionalidad, por lo que podemos aplicar el método descrito para ajustar la tendencia. uspop=ts(scan(&quot;data/USPOP.txt&quot;),frequency=1/10,start=1790) pop=window(uspop,start=1790) plot(pop,type=&quot;o&quot;,ylab=&quot;Poblacion (millones)&quot;) Podemos notar del gráfico que la tendencia es creciente y parece tener un comportamiento cuadrático, por lo que ajustando una función de la forma (2.27) para la población de los datos USPOP para \\(1790\\leq t\\leq1980\\) nos da los parámetros estimados \\[\\hat{a}_0=2.101\\times10^{10};\\quad \\hat{a}_1=-2.338\\times10^{7}; \\hat{a}_2=6.506\\times10^{3}\\] En el gráfico siguiente se puede observar la curva ajustada y los datos originales. Los valores estimados del proceso de ruido \\(\\epsilon_t, 1790\\leq t\\leq1980\\), son los residuales obtenidos por sustracción de \\(\\hat{T}_t=\\hat{a}_0+\\hat{a}_1t+\\hat{a}_2t^2\\) de la serie \\(X_t\\). La componente de tendencia \\(T_t\\) nos proporciona un predictor natural de los valores futuros de \\(X_t\\). Por ejemplo si deseamos estimar \\(T_{1990}\\) por su valor medio, obtenemos \\[T_{1990} = 2.4853\\times10^8\\] para la población de EE.UU en 1990. Sin embargo si los residuales \\(\\hat{\\epsilon}_t\\) están altamente correlacionados podemos ser capaces de usar esos valores para dar una mejor estimación de \\(T_{1990}\\) y por consiguiente de \\(X_{1990}\\). x=time(pop) reg=lm(pop~x+I(x^2),na.action=NULL) summary(reg) ## ## Call: ## lm(formula = pop ~ x + I(x^2), na.action = NULL) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6947521 -358167 436285 1481410 3391761 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.10e+10 6.59e+08 31.9 &lt;2e-16 *** ## x -2.34e+07 6.98e+05 -33.5 &lt;2e-16 *** ## I(x^2) 6.51e+03 1.85e+02 35.2 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2770000 on 18 degrees of freedom ## Multiple R-squared: 0.999, Adjusted R-squared: 0.999 ## F-statistic: 8.05e+03 on 2 and 18 DF, p-value: &lt;2e-16 plot(pop,type=&quot;o&quot;,xlab=&quot;Años&quot;,ylab=&quot;Poblacion (millones)&quot;) lines(reg$fitted.values,col=&quot;red&quot;) Ejemplo 2.3 El archivo “Population-North-Rhine-Westphalia.txt” contiene la población de la región North-Rhine-Westphalia (Alemania) en millónes cada 5 años desde 1935 hasta 1980. Observando el gráfico podemos suponer que la tendencia se puede ajustar por el modelo cúbico (2.28), esto es \\[T_t=\\beta_0+\\beta_1t+\\beta_2t^2+\\beta_3t^3\\] El código en R para el gráfico y el ajuste es NRWpop=read.table(&quot;data/Population-North-Rhine-Westphalia.txt&quot;, header = TRUE) knitr::kable(head(NRWpop,booktabs=TRUE, caption=&quot;Población (en millones) de North-Rhine-Westphalia, Alemania, 1935-1980&quot;)) Year Population 1935 11772 1940 12059 1945 11200 1950 12926 1955 14442 1960 15694 plot(NRWpop, type = &quot;b&quot;,col=&quot;blue&quot;,xlab = &quot;Años&quot;,ylab = &quot;Población (millones)&quot;) # Modelo cúbico t=NRWpop[,1] pob=NRWpop[,2] modelo=lm(pob~t+I(t^2)+I(t^3),na.action = NULL) summary(modelo) ## ## Call: ## lm(formula = pob ~ t + I(t^2) + I(t^3), na.action = NULL) ## ## Residuals: ## Min 1Q Median 3Q Max ## -813.0 -199.2 67.1 275.6 493.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.11e+09 5.10e+08 4.13 0.0061 ** ## t -3.23e+06 7.81e+05 -4.14 0.0061 ** ## I(t^2) 1.65e+03 3.99e+02 4.14 0.0061 ** ## I(t^3) -2.81e-01 6.79e-02 -4.14 0.0061 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 472 on 6 degrees of freedom ## Multiple R-squared: 0.974, Adjusted R-squared: 0.962 ## F-statistic: 76.2 on 3 and 6 DF, p-value: 3.63e-05 lines(t,modelo$fitted.values,col=&quot;red&quot;) La curva punteada en azul corresponde a los datos originales, la curva en rojo corresponde al ajuste mediante el modelo cúbico. Método T2: Suavizado por medio de un promedio móvil. Sea \\(q\\) un entero no negativo y consideremos un promedio móvil de la forma \\[\\begin{equation} W_t = \\frac{1}{2q+1}\\sum_{j=-q}^{q}X_{t+j} \\tag{2.37} \\end{equation}\\] de un proceso \\(\\{X_t\\}\\) definido por (2.34). Entonces para \\(q+1\\leq t\\leq n-q\\), \\[\\begin{eqnarray} W_t &amp;=&amp; \\frac{1}{2q+1}\\sum_{j=-q}^qT_{t+j}+\\frac{1}{2q+1}\\sum_{j=-q}^q\\epsilon_{t+j}\\\\ \\nonumber &amp;\\simeq&amp; T_t \\tag{2.38} \\end{eqnarray}\\] suponiendo que \\(T_t\\) es aproximadamente lineal sobre el intervalo \\([t-q,t+q]\\) y que el promedio del término de error sobre este intervalo es cercano a cero. El promedio móvil entonces nos provee con el estimador \\[\\begin{equation} \\hat{T}_t = \\frac{1}{2q+1}\\sum_{j=-q}^qX_{t+j},\\quad q+1\\leq t\\leq n-q. \\tag{2.39} \\end{equation}\\] Dado que \\(X_t\\) es no observado para \\(t\\leq0\\) o \\(t\\geq n\\) no podemos usar (2.39) para \\(t\\leq q\\) o \\(t&gt;n-q\\). Una forma de resolver este problema es haciendo \\(X_t=X_1\\) para \\(t&lt;1\\) y \\(X_t=X_n\\) para \\(t&gt;n\\). A continuación presentamos un ejemplo Ejemplo 2.4 El gráfico siguiente muestra las huelgas ocurridas en EE.UU, de 1951 a 1980, según la Oficina de Estadísticas Laborales del Departamento de Trabajo de los EE.UU. A estos datos le aplicamos un promedio móvil de 5 puntos, la Figura muestra la serie suavizada y el término de error estimado \\(\\hat{\\epsilon}_t = X_t - \\hat{T}_t\\) se muestra en la Figura . Como era de esperarse ellos no presentan una tendencia clara. Las instrucciones en R para el suavizado y los gráficos son los siguientes: H=read.table(&quot;data/Huelgas.txt&quot;) # Proemdio móvil por medio de la función &quot;filter&quot; W=filter(H[,2],sides=2,rep(1/5,5)) # Residuales de X_t y=H[,2]-W # Graficos par(mfrow=c(3,1)) plot(H,xlab=&quot;años&quot;,ylab=&quot;Huelgas&quot;,type=&#39;b&#39;, main = &quot;Huelgas en EE.UU., años 1951-1980&quot;) plot(H[,1],W,xlab=&quot;años&quot;,ylab=&quot;Huelgas&quot;,type=&#39;b&#39;, main = &quot;Promedio móvil de 5 puntos para los datos de Huelga&quot;) plot(H[,1],y,xlab=&quot;años&quot;,ylab=&quot;Residuales&quot;,type=&#39;b&#39;, main = &quot;Residuales e_t=X_t-T_t&quot;) Para cada valor fijo \\(a\\in[0,1]\\), el promedio móvil de un lado \\(\\hat{T}_t, t=1,\\ldots,n\\), definido por la recursión \\[\\begin{equation} \\hat{T}_t = aX_t+(1-a)\\hat{T}_t,\\quad t=2,\\ldots,n \\tag{2.40} \\end{equation}\\] y \\[\\hat{T}_1=X_1,\\] se puede calcular usando la opción sides=1 en la función filter de R. Es usual pensar como aplicación de la ecuación (2.40) como un suavizado exponencial, dado que se sigue de la recursión que para \\(t\\leq2, \\hat{T}_t=\\sum_{j=0}^{t-2}a(1-a)^jX_{t-j}+(1-a)^{t-1}X_1\\), es un promedio móvil con peso de \\(X_t,X_{t-1},\\ldots\\), con pesos decreciendo exponencialmente (excepto para el último término). Es útil pensar en \\(\\{\\hat{T}_t\\}\\) en (filter) como un proceso obtenido de \\(\\{X_t\\}\\) por aplicación de un operador lineal o filtro lineal \\(\\hat{T}_t=\\sum_{j=-\\infty}^{\\infty}a_jX_{t+j}\\) con pesos \\(a_j=(2q+1)^{-1},-q\\leq j\\leq q\\), y \\(a_j=0,|j|&gt;q\\). Este filtro particular es un filtro de “paso-bajo” ya que toma los datos \\(\\{X_t\\}\\) y remueve la componente de rápida fluctuación (o de alta frecuencia) \\(\\{\\hat{\\epsilon}_t\\}\\), para dejar el término de la tendencia estimada de lenta variación \\(\\{\\hat{T}_t\\}\\). Método T3: Diferenciación para generar datos estacionarios. En lugar de intentar remover el ruido por suavizado como en el Método T2, ahora intentaremos eliminar la tendencia por diferenciación. Definamos primero el operador diferencia \\(\\nabla\\) por \\[\\begin{equation} \\nabla x_t = x_t-x_{t-1}=(1-B)x_t, \\tag{2.41} \\end{equation}\\] donde \\(B\\) es el operador de desplazamiento hacia atrás (backward shift operator en inglés), \\[\\begin{equation} Bx_t=x_{t-1}. \\tag{2.42} \\end{equation}\\] Las potencias de los operadores \\(B\\) y \\(\\nabla\\) se definen de manera obvia, esto es, \\(B^j(x_t)=x_{t-j}\\) y \\(\\nabla^j(x_t)=\\nabla(\\nabla^{j-1}(x_t)),j\\geq1\\) con \\(\\nabla^0(x_t)=x_t\\). Los polinomios en \\(B\\) y \\(\\nabla\\) se manipulan de la misma manera que las funciones polinómicas de variables reales. Por ejemplo \\[\\begin{eqnarray*} \\nabla^2x_t &amp;=&amp; \\nabla(\\nabla x_t) = (1-B)(1-B)x_t = (1-2B+B^2)x_t \\\\ &amp;=&amp; x_t-2x_{t-1}+x_{t-2}. \\end{eqnarray*}\\] Si el operador \\(\\nabla\\) se aplica a una función con tendencia lineal \\(T_t=at+b\\), entonces obtenemos la función constante \\(\\nabla T_t=a\\). De la misma manera cada tendencia polinomial de grado \\(k\\) se puede reducir a una constante por aplicación del operador \\(\\nabla^k\\). Iniciando entonces con el modelo \\(X_t=T_t+\\epsilon_t\\), donde \\(T_t=\\sum_{j=0}^ka_jt^j\\) y \\(\\epsilon_t\\) es estacionario con media cero, obtenemos \\[\\nabla^kX_t = k!a_k+\\nabla^k\\epsilon_t,\\] un proceso estacionario con media \\(k!a_k\\). Esta consideración sugiere la posibilidad, dada una sucesión \\(\\{X_t\\}\\) de datos, de aplicar el operador \\(\\nabla\\) repetidamente hasta conseguir una sucesión \\(\\{\\nabla^kX_t\\}\\) la cual puede ser apropiadamente modelada como una realización de un proceso estacionario. Se encuentra a menudo en la práctica que el orden \\(k\\) de diferenciación es bastante pequeño, frecuentemente uno o dos.2 Ejemplo 2.5 Aplicando esta técnica al ejemplo 2.2 de población de los EE.UU, hallamos que dos operaciones de diferenciación son suficientes para producir una serie sin aparente tendencia. Los datos diferenciados se muestran en la Figura. Note que la magnitud de las fluctuaciones en \\(\\nabla^2X_n\\) se incrementa con el valor de \\(n\\). Este efecto se puede suprimir tomando primero logaritmo natural, \\(y_n=\\ln X_n\\) y entonces aplicando el operador \\(\\nabla^2\\) a la serie \\(\\{y_n\\}\\). Las instrucciones en R son las siguientes Dx=diff(uspop,difference=2) plot(Dx,type=&quot;b&quot;,xlab=&quot;Año&quot;, ylab=&quot;Diferencias&quot;) 2.2.2 Estimación de la tendencia y la estacionalidad Los métodos descritos para estimar y remover la tendencia pueden ser adaptados de manera natural para estimar tanto la tendencia como la estacionalidad en el modelo general \\[\\begin{equation} X_t = T_t + E_t + \\epsilon_t \\end{equation}\\] donde \\(\\mathbb{E}(\\epsilon_t)=0, E_{t+d}=E_t\\) y \\(\\sum_{j=1}^dE_t=0\\). Ilustraremos estos métodos con referencia al siguiente ejemplo de accidentes. El archivo “Accidentes3.txt” muestra el número de accidentes mortales de automóviles mensual ocurridos en EE.UU., entre los años 1973 y 1978. En la tabla siguiente se muestran los datos X&lt;-read.table(&quot;data/Accidentes3.txt&quot;, header = TRUE) Mes X1973 X1974 X1975 X1976 X1977 X1978 Ene 9007 7750 8162 7717 7792 7836 Feb 8106 6981 7306 7461 6957 6892 Mar 8928 8038 8124 7776 7726 7791 Abr 9137 8422 7870 7925 8106 8129 May 10017 8714 9387 8634 8890 9115 Jun 10826 9512 9556 8945 9299 9434 En la figura podemos observar que los datos presentan claramente una componente estacional con periodo \\(d=12\\). Será conveniente para el primer método indexar los datos por mes y año. Entonces \\(X_{j,k}, j=1,\\ldots,12, k=1,\\ldots,6\\) denotará el número de muertes accidentales reportados para el \\(j\\)-ésimo mes del \\(k\\)-ésimo año, \\((1972+k)\\). En otras palabras, definimos \\[X_{j,k}=X_{j+12(k-1)},\\quad j=1,\\ldots,12; k=1,\\ldots,6.\\] Método E1: Método de la tendencia pequeña. Si la tendencia es pequeña (como en los datos de accidentes) no es irrazonable suponer que el término de la tendencia es constante, digamos \\(T_k\\) para el año \\(k\\). Dado que \\(\\sum_{j=1}^{12}E_j=0\\), nos lleva al estimador insesgado natural para la tendencia \\[\\begin{equation} \\hat{T}_k = \\frac{1}{12}\\sum_{j=1}^{12}X_{j,k}, \\tag{2.43} \\end{equation}\\] mientras que para la estacionalidad \\(E_j, j=1,\\ldots,12\\) tenemos el estimador \\[\\begin{equation} \\hat{E}_j = \\frac{1}{6}\\sum_{k=1}^6(X_{j,k}-\\hat{T}_k), \\tag{2.44} \\end{equation}\\] el cual automáticamente satisface el requisito de que \\(\\sum_{j=1}^{12}\\hat{E}_j=0\\). El término de error estimado para el mes \\(j\\) del año \\(k\\) es por supuesto \\[\\begin{equation} \\hat{\\epsilon}_{j,k} = X_{j,k}-\\hat{T}_k-\\hat{E}_j, \\quad j=1,\\ldots,12; k=1,\\ldots,6. \\tag{2.45} \\end{equation}\\] La generalización de (2.43) a (2.45) para datos con estacionalidad con un periodo distinto de 12 es bastante sencillo de realizar, simplemente cambiamos 12 por el correspondiente valor de \\(d\\). Así, en general, si tenemos \\(n\\) años (meses, semanas, días, etc.) y estacionalidad con periodo \\(d\\), los estimadores seran: Para la tendencia \\(T_k\\): \\[\\begin{equation} \\hat{T}_k=\\frac{1}{d}\\sum_{j=1}^dX_{j,k} \\tag{2.46} \\end{equation}\\] Para la estacionalidad \\(E_j\\): \\[\\begin{equation} \\hat{E}_j=\\frac{1}{n}\\sum_{k=1}^n(X_{j,k}-\\hat{T}_k),\\quad j=1,\\ldots,d \\tag{2.47} \\end{equation}\\] Para el error \\[\\begin{equation} \\hat{\\epsilon}_{j,k}=X_{j,k}-\\hat{T}_k-\\hat{E}_j,\\quad k=1,\\ldots,n; j=1,\\ldots,d. \\tag{2.48} \\end{equation}\\] Las Figuras siguientes muestran respectivamente las observaciones con la tendencia removida \\(X_{j,k}-\\hat{T}_k\\), la componente estacional estimada \\(\\hat{E}_j\\) y las observaciones con la tendencia y la estacionalidad removida \\(\\hat{\\epsilon}_{j,k}=X_{j,k}-\\hat{T}_k-\\hat{E}_j\\). En la última no se observa una aparente tendencia o estacionalidad. # Estimacion de la tendencia Tk=numeric(n*d) for(k in 1:n) { for(j in 1:d) { Tk[(k-1)*d+j]=Tk[(k-1)*d+j]+(1/12)*X[j,k+1] } } # Grafico con la tendencia removida plot(V-Tk,type = &quot;l&quot;,xlab = &quot;Meses&quot;,ylab = &quot;Num. de accidentes&quot;, main = &quot;Accidentes mortales mensuales con la tendencia T_k removida&quot;) # Estimacion de la estacionalidad Ej=numeric(n*d) for(j in 1:d) { aux=0 for(k in 1:n) { aux=aux+(X[j,k+1]-Tk[(k-1)*d+j]) } for(k in 1:n) { Ej[(k-1)*d+j]=(1/n)*aux } } # Grafico de la estacionalidad plot(Ej,type = &quot;l&quot;,xlab = &quot;Meses&quot;,ylab = &quot;Num. de accidentes&quot;, main = &quot;Estacionalidad de los accidentes mortales mensuales&quot;) # Estimacion del error error=V-Tk-Ej # Grafico del error estimado plot(error,type = &quot;l&quot;,xlab = &quot;Meses&quot;,ylab = &quot;Error estimado&quot;, main = &quot;Error estimado de los accidentes mortales&quot;) grid(col = &quot;darkgray&quot;) Método E2: Estimación por promedio móvil. La siguiente técnica es preferible al Método E1 ya que no se basa en la suposición de que \\(T_t\\) es casi constante sobre cada ciclo estacional. Suponga que tenemos las observaciones \\(\\{x_1,\\ldots,x_n\\}\\). Se estima primero la tendencia aplicando un filtro de promedio móvil especialmente elegido para eliminar la componente estacional y para amortiguar el ruido. Si el periodo \\(d\\) es par, digamos \\(d=2q\\), entonces usamos \\[\\begin{equation} \\hat{T}_t = (0.5x_{t-q} + x_{t-q+1} + \\cdots + x_{t+q-1} + 0.5x_{t+q})/d, q&lt;t\\leq n-q. \\tag{2.49} \\end{equation}\\] Si el periodo es impar, digamos \\(d=2q+1\\), entonces usamos el promedio móvil simple (2.39). La Figura~ muestra la tendencia estimada \\(\\hat{T}_t\\) para los datos de accidentes mortales obtenido de (2.49). También muestra la tendencia constante a trozos obtenida por el Método S1. El segundo paso, es estimar la componente estacional. Para cada \\(k=1,\\ldots,d\\), calculamos el promedio \\(w_k\\) de las desviaciones \\(\\{(X_{k+jd}-\\hat{T}_{k+jd}):q&lt;k+jd\\leq n-q\\}\\). Dado que este promedio de desviaciones no necesariamente suma cero, estimamos la componente estacional \\(E_k\\) como \\[\\begin{equation} \\hat{E}_k = w_k -\\frac{1}{d}\\sum_{i=1}^dw_i,\\quad i=1,\\ldots,d, \\tag{2.50} \\end{equation}\\] y \\(\\hat{E}_k=\\hat{E}_{k-d},k&gt;d\\). Los datos sin la componente estacional se definen entonces como la serie original con la componente estacional removida, es decir, \\[\\begin{equation} d_t = X_t-\\hat{E}_t,\\quad t=1,\\ldots,n. \\tag{2.51} \\end{equation}\\] Finalmente, reestimamos la tendencia de \\(\\{d_t\\}\\) aplicando un filtro de promedio móvil como se describió para los datos no estacionales o fijando un polinomio a la serie \\(\\{d_t\\}\\). El término del ruido estimado llega a ser entonces \\[\\hat{\\epsilon}_t = X_t - \\hat{E}_t - \\hat{E}_t, \\quad t=1,\\ldots,n.\\] Los resultados de aplicar los Métodos S1 y S2 a los datos de accidentes mortales son casi iguales, dado que en este caso la constante a trozos y el promedio móvil de \\(T_t\\) están razonablemente cercanos. Una comparación de los valores estimados de \\(E_k, k=1,\\ldots,12\\), obtenido con ambos métodos se muestra en la Tabla~ k 1 2 3 4 5 6 7 8 9 10 11 12 \\(\\hat{E}_t(S1)\\) -7434 -1504 -724 -523 338 808 1665 961 -87 197 -321 -67 \\(\\hat{E}_t(S2)\\) -804 -1522 -737 -526 343 746 1680 987 -109 258 -259 -57 Componentes estacional estimadas para los datos de accidentes mortales Método E3: Diferenciación a paso \\(\\mathbf{d}\\). La técnica de diferenciación la cual aplicamos antes a datos no estacionales se pueden adaptar para lidiar con el caso estacional de periodo \\(d\\) introduciendo el operador de diferencia de paso \\(d\\) \\(\\nabla_d\\) definido por \\[\\begin{equation} \\nabla_dX_t = X_t-X_{t-d} = (1-B^d)X_t. \\tag{2.52} \\end{equation}\\] Este operador no debe confundirse con el operador \\(\\nabla^d = (1-B)^d\\) definido por (). Aplicando el operador \\(\\nabla_d\\) al modelo \\[X_t = T_t + E_t + \\epsilon_t,\\] donde \\(\\{E_t\\}\\) tiene periodo \\(d\\), obtenemos \\[\\nabla_dX_t = T_t-T_{t-d} + \\epsilon_t-\\epsilon_{t-d},\\] lo cual nos da una descomposición de la diferencia \\(\\nabla_dX_t\\) en una componente de tendencia \\((T_t-T_{t-d})\\) y un término de ruido \\((\\epsilon_t-\\epsilon_{t-d})\\). La tendencia \\((T_t-T_{t-d})\\) se puede eliminar usando los métodos ya descritos, por ejemplo, aplicando alguna potencia del operador \\(\\nabla\\). La figura siguiente muestra el resultado de aplicar el operador \\(\\nabla_{12}\\) a los datos de accidentes mortales. # Operador Nabla_d, usamos la funcion diff con lag=12 NdX=diff(V,lag=12) plot(NdX,type = &quot;l&quot;) La componente estacional evidente en la Figura~ está ausente en la Figura de \\(\\nabla_{12}X_t,13\\leq t\\leq72\\). Sin embargo todavía parece haber una tendencia decreciente. Si ahora aplicamos el operador \\(\\nabla\\) a \\(\\nabla_{12}X_t\\) y graficamos las diferencias \\(\\nabla\\nabla_{12}X_t,t=14,\\ldots,72\\) obtenemos el gráfico mostrado en la Figura~, los cuales no tienen una aparente tendencia o componente estacional. DNdX=diff(NdX) plot(DNdX,type = &quot;l&quot;) 2.3 Estimación de la tendencia por regresión clásica Los modelos de regresión son importantes para modelos en el dominio de tiempo y de frecuencia que discutiremos posteriormente. La idea principal depende de poder expresar una serie respuesta \\(X_t\\) como una combinación lineal de entradas \\(z_{t_1},z_{t_2},\\ldots,z_{t_q}\\). La estimación de los coeficientes \\(\\beta_1,\\beta_2,\\ldots,\\beta_q\\) de la combinación por mínimos cuadrados proporciona un método para modelar \\(X_t\\) en términos de las entradas. 2.3.1 Regresión Clásica Supongamos que tenemos \\(X_t\\), para \\(t=1,2,\\ldots,n\\) influenciada por una colección de series independientes \\(z_{t_1},z_{t_2},\\ldots,z_{t_q}\\), donde consideraremos primero que las entradas son fijas y conocidas. Podemos expresar esta relación como \\[\\begin{equation} X_t=\\beta_1z_{t_1}+\\beta_2z_{t_2}+\\cdots+\\beta_qz_{t_q}+w_t \\tag{2.53} \\end{equation}\\] donde \\(\\beta_1,\\beta_2,\\ldots,\\beta_q\\) son los coeficientes de regresión fijos y desconocidos, \\(\\{w_t\\}\\) es un error aleatorio o un proceso de ruido consistente de variables normales iid con media cero y varianza \\(\\sigma_w^2\\). El modelo lineal descrito en (2.53) se puede escribir de forma más general por medio de definir los vectores columna \\(\\mathbf{z}_t=(z_{t_1},z_{t_2},\\ldots,z_{t_q})^t\\) y \\(\\mathbf{\\beta}=(\\beta_1,\\beta_2,\\ldots,\\beta_q)^t\\) donde \\(t\\) denota la traspuesta, así (2.53) será \\[\\begin{equation} X_t=\\mathbf{\\beta}^t\\mathbf{z}_t+w_t \\tag{2.54} \\end{equation}\\] donde \\(w_t\\sim iid(0,\\sigma_w^2)\\). Es natural considerar la estimación de los coeficientes del vector \\(\\mathbf{\\beta}\\) minimizando la suma residual de cuadrados \\[\\begin{equation} RSS=\\sum_{t=1}^{n}(X_t-\\mathbf{\\beta}^t\\mathbf{z}_t)^2 \\tag{2.55} \\end{equation}\\] con respecto a \\(\\beta_1,\\beta_2,\\ldots,\\beta_q\\). Minimizando RSS obtenemos el estimador común de mínimos cuadrados. Esta minimización se puede hacer por diferenciación de (2.55) con respecto al vector \\(\\mathbf{\\beta}\\) o usando las propiedades de proyección. En la notación anterior, obtenemos la ecuación normal \\[\\begin{equation} \\left(\\sum_{t=1}^{n}\\mathbf{z}_t\\mathbf{z}_t^t\\right)\\hat{\\mathbf{\\beta}}=\\sum_{t=1}^{n}\\mathbf{z}_tX_t \\tag{2.56} \\end{equation}\\] Definiendo la matriz \\(Z=(\\mathbf{z}_1,\\mathbf{z}_2,\\ldots,\\mathbf{z}_n)^t\\) como una matriz \\(n\\times q\\) compuesta de \\(n\\) muestras de las variables de entradas y el vector observado \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)^t\\) se puede hacer una simplificación de la notación. Esta identificación nos lleva a \\[\\begin{equation} (Z^tZ)\\hat{\\mathbf{\\beta}}=Z^t\\mathbf{x} \\tag{2.57} \\end{equation}\\] y la solución es \\[\\begin{equation} \\hat{\\mathbf{\\beta}}=(Z^tZ)^{-1}Z^t\\mathbf{x}, \\tag{2.58} \\end{equation}\\] cuando la matriz \\(Z^tZ\\) es de rango \\(q\\). El residual minimizado de suma de cuadrados (2.55) tiene la forma matricial equivalente \\[\\begin{eqnarray} RSS&amp;=&amp;(\\mathbf{x}-Z\\hat{\\mathbf{\\beta}})^t(\\mathbf{x}-Z\\hat{\\mathbf{\\beta}})\\\\ \\nonumber &amp;=&amp;\\mathbf{x}^t\\mathbf{x}-\\hat{\\mathbf{\\beta}}^tZ^t\\mathbf{x}\\\\ \\nonumber &amp;=&amp;\\mathbf{x}^t\\mathbf{x}-\\mathbf{x}^tZ(Z^tZ)^{-1}Z^t\\mathbf{x}. \\tag{2.59} \\end{eqnarray}\\] El estimador común de mínimos cuadrados es insesgado, esto es, \\(\\mathbb{E}(\\hat{\\mathbf{\\beta}})=\\mathbf{\\beta}\\), y tiene la menor varianza de todos los estimadores insesgados lineales. Si los errores \\(w_t\\) son normalmente distribuidos (Gaussianos), \\(\\hat{\\mathbf{\\beta}}\\) es también el estimador de máxima verosimilitud para \\(\\mathbf{\\beta}\\) y es normalmente distribuido con \\[\\begin{eqnarray} \\text{cov}(\\hat{\\mathbf{\\beta}})&amp;=&amp;\\sigma_w^2\\left(\\sum_{t=1}^{n}\\mathbf{z}_t\\mathbf{z}_t^t\\right)^{-1}\\\\ \\nonumber &amp;=&amp;\\sigma_w^2(Z^tZ)^{-1}\\\\ \\nonumber &amp;=&amp;\\sigma_w^2C, \\tag{2.60} \\end{eqnarray}\\] donde \\[\\begin{equation} C=(Z^tZ)^{-1}. \\tag{2.61} \\end{equation}\\] Un estimador insesgado para la varianza \\(\\sigma_w^2\\) es \\[\\begin{equation} s_w^2=\\frac{RSS}{n-q} \\tag{2.62} \\end{equation}\\] contrastado con el estimador de máxima verosimilitud \\(\\hat{\\sigma}_w^2=RSS/n\\) el cual tiene divisor \\(n\\). Bajo la suposición de que \\(s_w^2\\) tiene distribución proporcional a una variable aleatoria chi-cuadrado con \\(n-q\\) grados de libertad, \\(\\chi_{n-q}^2\\), e independiente de \\(\\hat{\\beta}\\), se sigue que \\[\\begin{equation} t_{n-q}=\\frac{(\\hat{\\beta}_i-\\beta_i)}{s_w\\sqrt{c_{ii}}} \\tag{2.63} \\end{equation}\\] tiene distribución \\(t\\)-de Student con \\(n-q\\) grados de libertad; \\(c_{ii}\\) denota el \\(i\\)-ésimo elemento de la diagonal de \\(C\\), como se definió en (2.61). Hay varios modelos que podemos utilizar de manera de seleccionar el mejor subconjunto de variables independientes. Suponga que tenemos un modelo que sólo considera un subconjunto \\(q_1&lt;q\\) de variables independientes \\(\\mathbf{z}_{1t}=(z_{t_1},z_{t_2},\\ldots,z_{t_q1})^t\\) que influencian a la variable \\(X_t\\), así el modelo \\[\\begin{equation} X_t=\\mathbf{\\beta}_1^t\\mathbf{z}_{1t}+w_t \\tag{2.64} \\end{equation}\\] llega a ser la hipótesis nula, donde \\(\\mathbf{\\beta}_1=(\\beta_1,\\beta_2,\\ldots,\\beta_{q1})^t\\) es un subconjunto de los coeficientes de las \\(q\\) variables originales. Podemos contrastar el modelo reducido (2.64) contra el modelo completo (2.54) comparando el residual de la suma de cuadrados bajo los dos modelos usando el estadístico \\(F\\) \\[\\begin{equation} F_{q-q1,n-q}=\\frac{RSS_1-RSS}{RSS}\\frac{n-q}{q-q1} \\tag{2.65} \\end{equation}\\] el cual tiene distribución \\(F\\) con \\(q-q1\\) y \\(n-q\\) grados de libertad cuando (2.65) es el modelo correcto. La información envuelta en la prueba se resume en una tabla de Análisis de Varianza (ANOVA) como la mostrada en la Tabla siguiente para este caso particular. La diferencia en el numerador es llamada regresión de la suma de cuadrados. Fuente g.l Suma de cuadrados Medias Cuadrados \\(z_{t,q_1+1},\\ldots,z_{t,q}\\) \\(q-q_1\\) \\(SS_{reg}=RSS_1-RSS\\) \\(MS_{reg}=SS_{reg}/(q-q_1)\\) Error \\(n-q\\) \\(RSS\\) \\(s_e^2=RSS/(n-q)\\) Total \\(n-q_1\\) \\(RSS_1\\) En términos de la Tabla, por convención escribimos el estadístico \\(F\\) dado en (2.65) como el radio de dos medias cuadrados, obteniéndose \\[\\begin{equation} F_{q-q1,n-q}=\\frac{M S_{reg}}{s_w^2}. \\tag{2.66} \\end{equation}\\] Un caso de especial interés es para \\(q_1=1\\) y \\(z_{1t}=1\\), así el modelo en (2.64) es \\[X_t=\\beta_1+w_t\\] y la proporción de variación explicada por las otras variables es \\[\\begin{equation} R_{xz}^2=\\frac{RSS_0-RSS}{RSS_0}, \\tag{2.67} \\end{equation}\\] donde la suma residual de cuadrados bajo el modelo reducido dada por \\[\\begin{equation} RSS_0=\\sum_{t=1}^{n}(X_t-\\bar{X})^2 \\tag{2.68} \\end{equation}\\] es precisamente la suma de desviaciones al cuadrado de la media \\(\\bar{X}\\). La medida \\(R_{xz}^2\\) es la correlación múltiple cuadrado entre \\(X_t\\) y \\(z_{t2},z_{t3},\\ldots,z_{tq}\\). Las técnicas discutidas se pueden usar para hacer comparación entre varios modelos. Estas pruebas han sido usadas en el pasado en una manera gradual, donde las variables son añadidas o suprimidas cuando los valores de la prueba \\(F\\) exceden o fallan en exceder algunos niveles predeterminados. El procedimiento, llamado regresión múltiple por pasos, es útil para conseguir un conjunto de variables que sea de utilidad. Una manera alternativa es enfocándose en un procedimiento para selección del modelo que no sea secuencial, sino simplemente evaluar cada modelo en base a sus propios méritos. Suponga que consideramos un modelo de regresión con \\(k\\) coeficientes y denotemos el estimador de máxima verosimilitud para la varianza como \\[\\begin{equation} \\hat{\\sigma}_k^2=\\frac{RSS_k}{n} \\tag{2.69} \\end{equation}\\] donde \\(RSS_k\\) denota la suma residual de cuadrados bajo el modelo con \\(k\\) coeficientes de regresión. Entonces, Akaike (1969, 1973, 1974) sugirió medir la bondad del ajuste para este modelo en particular equilibrando el error del ajuste contra el número de parámetros en el modelo; definiendo lo siguiente Definición 2.16 (Criterio de Información de Akaike (AIC)) El Criterio de Información de Akaike se define como \\[\\begin{equation} AIC=\\ln\\hat{\\sigma}_k^2+\\frac{n+2k}{n} \\tag{2.70} \\end{equation}\\] donde \\(\\hat{\\sigma}_k^2\\) está dado por (2.69) y \\(k\\) es el número de parámetros en el modelo El criterio de información de Akaike (AIC) es una medida de la calidad relativa de un modelo estadístico, para un conjunto dado de datos. Como tal, el AIC proporciona un medio para la selección del modelo. El valor de \\(k\\) que minimiza \\(AIC\\) especifica el mejor modelo. La idea es que la minimización de \\(\\hat{\\sigma}_k^2\\) sea razonablemente objetiva, excepto que decrezca monótonamente cuando \\(k\\) crece. Por lo tanto, debemos penalizar la variación del error por un término proporcional al número de parámetros. La elección del término de penalización dado por (2.70) no es único. Note que la desigualdad de Cauchy-Schwartz implica \\(|\\gamma(s,t)|^2\\leq\\gamma(s,s)\\gamma(t,t)\\).}.↩ Esto depende del hecho de que muchas funciones pueden ser aproximadas bastante bien, en un intervalo de longitud finita, por un polinomio de grado razonablemente bajo.↩ "],
["modelos-de-series-de-tiempo.html", "Capítulo 3 Modelos de series de tiempo 3.1 Modelos Estocásticos 3.2 Modelos lineales", " Capítulo 3 Modelos de series de tiempo Como indicamos en el capítulo anterior el objetivo principal en el análisis de series de tiempo es desarrollar modelos matemáticos que provean una descripción apropiada para los datos muestrales. Recordando las definiciones 1.7 y 2.1 podemos describir los modelos generales útiles para la descripción de series de tiempo 3.1 Modelos Estocásticos 3.1.1 Procesos Estocásticos De la definición de procesos estocásticos (Definición 2.1), las variables aleatorias de la familia (medibles para todo \\(t\\in T\\)) son funciones de la forma \\[x(\\omega,t):\\Omega\\times T\\to\\mathbb{R}\\] Para \\(T=\\mathbb{N}\\), tenemos un proceso en tiempo discreto y para \\(T\\subset\\mathbb{R}\\) tenemos un proceso en tiempo continuo. En lo que respecta a este libro, consideraremos como subconjunto de índices \\(T=(0,\\infty)\\). Como ya indicamos, usaremos la notación \\(X_t\\) para denotar la realización de un proceso estocástico \\(x_t(\\omega*)\\) cuando no haya lugar a confución. De esta forma, adoptaremos sin pérdida de generalidad, el conjunto de índices habitual de las series de tiempo en el ámbito de las finanzas y economía \\(I=(1,T)\\). De lo anterior, se tiene que los procesos estocásticos suelen ser descritos mediante su distribución conjunta de probabilidades, de manera que la relación que existe entre una realización y un proceso estocástico es análoga a la existente entre la muestra y la población en el análisis estadístico clásico. 3.1.2 Momentos, Varianza, Covarianza y Correlación Definición 3.1 El valor esperado y varianza de un proceso estocástico están dados por \\[\\begin{equation} \\mathbb{E}(x_t)=\\int_{\\Omega}x(\\omega,t)dP(\\omega),\\quad t\\in[0,T] \\tag{3.1} \\end{equation}\\] y \\[\\begin{equation} Var(x_t)=\\mathbb{E}(x_t-\\mathbb{E}(x_t))^2,\\quad t\\in[0,T] \\tag{3.2} \\end{equation}\\] siempre que las integrales existan y sean finitas. Definición 3.2 El \\(k\\)-ésimo momento de \\(x_t\\), con \\(k\\geq1\\), se define como \\(\\mathbb{E}(x_t^k)\\) para todo \\(t\\in[0,t]\\). Definición 3.3 La función de covarianza del proceso para dos instantes de tiempo \\(t\\) y \\(s\\) está dada por \\[\\gamma(t,s)=Cov(x_t,x_s)=\\mathbb{E}[(x_t-\\mathbb{E}(x_t))(x_s-\\mathbb{E}(x_s))]\\] La cantidad \\(x_t-x_s\\) es llamada el proceso de incrementos desde \\(s\\) a \\(t\\), con \\(s&lt;t\\). 3.1.3 Variación de un proceso Sea \\(P_n=\\{0=t_0&lt;t_1&lt;\\cdots&lt;t_i&lt;\\cdots&lt;t_n=t\\}\\) una partición cualquiera del intervalos \\([0,t]\\) en \\(n\\) subintervalos y denotemos por \\[||P_n||=\\max\\{j=0,1,\\ldots,n-1(t_{j+1}-t_j)\\}\\] el tamaño de paso máximo de discretización de la partición \\(P_n\\). Definición 3.4 La variación del proceso \\(x\\) se define como \\[\\begin{equation} V_t(x)=p-\\lim_{||P_n||}\\sum_{k=0}^{n-1}|x_{t_{k+1}}-x_{t_k}| \\tag{3.3} \\end{equation}\\] Si \\(x\\) es diferenciable, entonces \\(V_t(x)=\\int_0^t|x&#39;(u)|du\\). Si \\(V_t(X)&lt;\\infty\\), entonces decimos que \\(x\\) es de variación acotada en \\([0,t]\\). Si es cierto para todo \\(t\\geq0\\), entonces decimos que \\(x\\) tiene variación acotada. Definición 3.5 La variación cuadrática de un proceso estocástico \\(x\\), denotada por \\([x,x]_t\\), se define como \\[\\begin{equation} [x,x]_t = p-\\lim_{||P_n||}\\sum_{k=0}^{n-1}|x_{t_{k+1}}-x_{t_k}|^2 \\tag{3.4} \\end{equation}\\] Para procesos estocásticos con trayectorias continua, el límite existe, y en dicho caso usamos la notación \\(\\langle x,x\\rangle_t\\) y podemos definirla alternativamente como \\[\\begin{equation} \\langle x,x\\rangle_t = p-\\lim_{n\\to\\infty}\\sum_{k=1}^{2^n}\\left(x_{\\min(t,k/2^n)} - x_{\\min(t,(k-1)/2^n)}\\right)^2 \\tag{3.5} \\end{equation}\\] Si \\(x\\) es continuo y tiene variación acotada cuadrática finita, entonces su variación total es infinita. Note que \\(V_t(x)\\) y \\([x,x]_t\\) son también procesos estocásticos. 3.1.4 Martingalas En teoría de probabilidad, un proceso estocástico de tipo martingala (galicismo de martingale) es todo proceso caracterizado por no tener deriva. Este tipo de procesos estocásticos reciben su nombre de la estrategia de la martingala, un método de apuestas que tuvo cierta fama en el siglo XVIII. La estrategia de la martingala consiste en volver a apostar por el total perdido al momento de incurrir en una pérdida en un juego de azar,. En la nueva apuesta, el jugador tiene la posibilidad de recobrar todas sus pérdidas, por lo que podría parecer que a largo plazo la esperanza de ganancia con esta estrategia se mantienen constantes y a favor del jugador. De hecho, estadísticamente es así: el capital medio del jugador (esto es, el dinero que el jugador tiene a su disposición para jugar) se mantiene constante. El problema reside en que, al incurrir en sucesivas pérdidas, el jugador que siga la estrategia de la martingala se ve obligado a apostar de nuevo cantidades cada vez mayores (las pérdidas acumuladas), que tienden a crecer exponencialmente. Al cabo de unos pocos ciclos de apuestas, el jugador, cuyos recursos son habitualmente muy inferiores a los de la banca, se ve arruinado al ser incapaz de apostar de nuevo por el total de sus pérdidas. Evitar jugadores que intenten seguir la estratega de la martingala es de todos modos una de las razones por las que los casinos actuales establecen límites máximos de apuesta. La estrategia de la martingala se popularizó en el siglo XVIII con fama de ser una estrategia ingenua y propia de mentes simples, puesto que aunque en apariencia es infalible, sin embargo, está abocada a arruinar al jugador. Recibe el nombre de los habitantes de la localidad francesa de Martigues (martingales en francés), situada en las cercanías de Marsella, que por aquel entonces tenían fama de ser ingenuos y simplones. El concepto de la martingala en la teoría de probabilidades fue introducido por Paul Pierre Lévy, y una gran parte del desarrollo original de la teoría la realizó Joseph Leo Doob. Parte de la motivación para ese esfuerzo era demostrar la inexistencia de estrategias de juego infalibles. El concepto fue inmediatamente aplicado al análisis de procesos bursátiles. Uno de los resultados más importantes de la matemática financiera es, precisamente, que un mercado perfecto sin posibilidades de arbitraje es una martingala. Definición 3.6 Sea \\((\\omega,\\mathcal{F},P)\\) un espacio de probabilidad. Una filtración \\(\\{\\mathcal{F}_t,t\\geq0\\}\\) es una familia creciente de sub-\\(\\sigma\\)-álgebras de \\(\\mathcal{F}\\) indexadas por \\(t\\geq0\\); es decir, para cada \\(s,t&gt;0\\) tal que \\(s&lt;t\\), se tiene \\(\\mathcal{F}_s\\subset\\mathcal{F}_t\\) con \\(\\mathcal{F}_0=\\{\\Omega,\\emptyset\\}\\). Para cada proceso estocástico \\(\\{x_t\\}_{t\\geq0}\\) y para cada \\(t\\), podemos asociar una \\(\\sigma\\)-álgebra denotada por \\(\\mathcal{F}_t=\\sigma\\{x_s:0\\leq s\\leq t\\}\\), y que además es la \\(\\sigma\\)-álgebra generada por \\(x\\); es decir, la \\(\\sigma\\)-álgebra más pequeña (minimal) de \\(\\mathcal{F}\\) que hace a \\(x(s,\\omega)\\) medible para cada \\(0\\leq s\\leq t\\). Definición 3.7 Dado un proceso estocástico \\(\\{X_t\\}_{t\\geq0}\\) y una filtración \\(\\{\\mathcal{F}_t, t\\geq0\\}\\) (no necesariamente la que genera \\(X\\)), el proceso \\(X\\) se denomina adaptado a \\(\\{\\mathcal{F}_t, t\\geq0\\}\\) (\\(\\mathcal{F}_t\\)-adaptado) si para cada \\(t\\geq0\\), \\(X(t)\\) es \\(\\mathcal{F}_t\\)-medible. En otras palabras \\(X=\\{X_t\\}_{t\\geq0}\\) es \\(\\mathcal{F}_t\\)-adaptado cuando el valor de \\(X_t\\) en el tiempo \\(t\\) solo depende de la información contenida en la realización hasta el instante \\(t\\). Dado un espacio de probabilidad \\((\\Omega,\\mathcal{F},P)\\) y una filtración \\(\\{\\mathcal{F}_t,t\\geq0\\}\\), entonces definimos el espacio de probabilidad filtrado a la cuaterna \\((\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\geq0},P)\\). Definición 3.8 Sea \\((\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\geq0},P)\\) un espacio de probabilidad filtrado. Un proceso \\(X_t\\) con \\(t\\in T\\), \\(T\\subseteq\\mathcal{R}\\) un conjunto de índices, es una martingala relativo a la filtración \\(\\{\\mathcal{F}_t,t\\geq0\\}\\), si \\(X_t\\) es adaptado a la filtración \\(\\{\\mathcal{F}_t,t\\geq0\\}\\) \\(X_t\\) es integrable, es decir, \\(\\mathbb{E}|X_t|&lt;\\infty\\), Para cualesquieras \\(s\\) y \\(t\\) con \\(s&lt;t\\), \\(\\mathbb{E}(X_t|\\mathcal{F}_s)=X_s\\) c.s. Decimos que el proceso es una submartingala si \\[\\mathbb{E}(X_t|\\mathcal{F}_s)\\geq X_s \\text{ c.s.}\\] Decimos que es una supermartingala si \\[\\mathbb{E}(X_t|\\mathcal{F}_s)\\leq X_s \\text{ c.s.}\\] Ejemplo 3.1 Sean \\(X_0,X_1,\\ldots,X_n\\) variables aleatorias iid tal que \\(\\mathbb{E}(X_1)=\\mu\\) y sean \\[\\begin{eqnarray*} M_0 &amp;=&amp; X_0 \\\\ M_1 &amp;=&amp; X_0+X_1 \\\\ \\vdots &amp; &amp; \\vdots \\\\ M_n &amp;=&amp; X_0+X_1+\\cdots+X_n \\end{eqnarray*}\\] La sucesión de variables aleatorias \\(M_n\\) se llama paseo aleatorio y es una supermartingala si \\(\\mu\\leq0\\), una martingala si \\(\\mu=0\\) y una submartingala si \\(\\mu\\geq0\\). Es fácil demostrarlo, sencillamente usamos el hecho de que \\[M_{n+1}=M_n+X_{n+1}\\] y que \\(M_n\\) y \\(X_{n+1}\\) son independientes. Podemos generar tal proceso en R. n=100 mu=0 sigma=1 X=rnorm(n,mu,sigma) M=cumsum(X) plot(M,type = &quot;l&quot;,xlab = &quot;t&quot;,ylab = &quot;M_n&quot;) Ejemplo 3.2 (Precio de acciones) Sean \\(Y_0,Y_1,\\ldots,Y_n\\) variables aleatorias independientes y positivas. Supongamos que una acción tiene precio \\(M_0\\) a tiempo \\(t=0\\). Un modelo común para modelar el precio de la acción en tiempo \\(t=n\\) es \\[M_{n+1}=M_nY_n\\] donde \\((Y_n-1)\\times100\\) representa (en porcentaje) la variabilidad de la acción. Usando las propiedades de esperanza condicional (Apéndice), es muy sencillo demostrar que \\[\\mathbb{E}(M_{n+1}|M_0,\\ldots,M_n)=M_n\\mathbb{E}(Y_n)\\] En particular, si \\(Y_1,\\ldots,Y_n\\) son idénticamente distribuidas con \\(\\mathbb{E}(Y_1)=\\mu\\) tenemos que \\(M_n\\) es Una martingala si \\(\\mu=1\\) Una submartingala si \\(\\mu&gt;1\\) Una supermartingala si \\(\\mu&lt;1\\). Dos modelos bien conocidos de lo anterior son Modelo Black-Scholes discreto. Sean \\(Y_1,\\ldots,Y_n\\) definidas por \\[Y_n=e^{Z_n}\\] donde \\(Z_1,\\ldots,Z_n\\) son variables aleatorias independientes normales \\(N(\\mu,\\sigma^2)\\). Modelo Binomial. Sean \\(Y_1,\\ldots,Y_n\\) definidas por \\[P(Y_i=(1+t)e^{-r})=p\\quad\\text{ y }\\quad P(Y_i=(1+t)^{-1}e^{-r})=1-p\\] La constante \\(r\\) es la tasa de interés y los factores \\((1+t)\\) y \\((1+t)^{-1}\\) modelan las variaciones del mercado y garantizan que el precio tiene la forma \\(M_0(1+t)^ye^{-nr}\\), con \\(|y|\\leq n\\). La volatilidad del precio está asociada a \\(p\\). Definición 3.9 Una variable aleatoria \\(X\\) es cuadrado integrable si \\(\\mathbb{E}(X^2)&lt;\\infty\\). Un proceso estocástico \\(X_t\\) en el intervalo \\([0,T]\\), donde \\(T\\) puede ser infinito, es cuadrado integrable si \\[\\begin{equation} \\sup_{t\\in[0,T]}\\mathbb{E}(X_t^2)&lt;\\infty \\tag{3.6} \\end{equation}\\] es decir, si sus segundos momentos son acotados. Definición 3.10 Un proceso estocástico \\(X_t, 0\\leq t\\leq T\\) se dice que es uniformemente integrable si \\[\\mathbb{E}(|X_t|\\mathbf{1}_{\\{|X_t|&gt;n\\}})\\] converge a 0 cuando \\(n\\to\\infty\\) uniformemente en \\(t\\). 3.1.5 Propiedad de Markov La propiedad de Markov establece que si conocemos el estado actual de un proceso estocástico, entonces el comportamiento futuro de dicho proceso es independiente de su pasado. Un proceso \\(X_t\\) tiene la propiedad de Markov si la distribución condicional del proceso \\(X_t\\) dado el proceos en el instante \\(X_t=x\\), no depende de los valores pasados. Definición 3.11 \\(X\\) es un proceso de Markov si para cualquier \\(t\\) y \\(s&gt;0\\), \\[P(X_{t+s}\\leq y|\\mathcal{F}_t) = P(X_{t+s}\\leq y|X_t) \\text{ c.s.}\\] donde \\(\\mathcal{F}_t\\) es la \\(\\sigma\\)-álgebra generada por el proceso hasta el tiempo \\(t\\). Definición 3.12 La función de transición de probabilidad de un proceso \\(X\\) se define como \\[P(y,t,x,s) = P(X_y\\leq y|X_s\\leq x)\\] la función de distribución condicional del proceso en el instante \\(t\\), dado que éste está en el punto \\(x\\) en el instante \\(s&lt;t\\). La propiedad de Markov implica una expresión que resulta muy útil en términos de la esperanza condicional por la \\(\\sigma\\)-álgebra de eventos, la cual es válida tanto para procesos en tiempo discreto como en tiempo continuo. Las definiciones y propiedades anteriores son temas de estudio de gran importancia y con una amplia teoría matemática que está fuera del alcance de este libro, pero lo que hemos descrito es suficiente para el objetivo del mismo. 3.2 Modelos lineales Los modelos lineales proporcionan un enfoque natural que permite analizar el comportamiento de los procesos estocásticos o series de tiempo y en especial a lo referente a finanzas y economía. En esta sección discutiremos la estructura de dependencia, autocorrelación, modelización y predicción de los modelos lineales teóricos, con los correspondientes comandos en R para generar y nalaizar dichos procesos. 3.2.1 Proceso de Ruido Blanco Definición 3.13 Un proceso \\(\\{w_t\\}\\) se denomina ruido blanco (white noise) de media 0 y varianza \\(\\sigma^2\\) si satisface \\[\\begin{eqnarray*} \\mathbb{E}(w_t) &amp;=&amp; 0,\\quad Var(w_t)=\\sigma_w^2&lt;\\infty \\\\ Cov(w_t,w_{t-k}) &amp;=&amp; 0, \\forall k\\neq0 \\end{eqnarray*}\\] Las series de tiempo generadas de esta manera son muy usadas como modelos para ruido en aplicaciones de ingeniería. La designación “blanco” se origina de la analogía con la luz blanca e indica que todos los posibles períodos de oscilación están presentes con igual intensidad. En particular, una sucesión de variables aleatorias iid con media 0 y varianza \\(\\sigma_w^2\\) representa un caso especial de un proceso de ruido blanco. Este proceso lo denotaremos por \\(w_t\\sim WN(0,\\sigma_w^2)\\). Un muy usado ruido blanco es el ruido blanco gaussiano, donde las \\(w_t\\) son variables aleatorias normales o gaussianas con media 0 y varianza \\(\\sigma_w^2\\) y denotadas como \\(w_t\\sim iidN(0,\\sigma_w^2)\\). La función de media de un ruido blanco es trivial, es decir \\[\\mu_w=\\mathbb{E}(w_t)=0.\\] Calculemos la función de autocovarianza de \\(w_t\\) \\[\\begin{eqnarray*} \\gamma_w(s,t) &amp;=&amp; \\mathbb{E}[(w_s-\\mu_s)(w_t-\\mu_t)] \\\\ &amp;=&amp; \\mathbb{E}[w_sw_t] \\\\ &amp;=&amp; \\begin{cases} \\sigma_w^2, &amp;\\text{ si }s=t \\\\ 0, &amp;\\text{ si }s\\neq t \\end{cases} \\end{eqnarray*}\\] La última igualdad se sigue del hecho de que \\(w_s\\) y \\(w_t\\) son no-correlacionados para \\(s\\neq t\\) por lo que \\(\\mathbb{E}(w_sw_t) = \\mathbb{E}(w_s)\\mathbb{E}(w_t)=0\\). Ejemplo 3.3 (Estacionaridad de un ruido blanco) La función de autocovarianza de un ruido blanco es fácil de evaluar como \\[\\gamma_w(h) = \\mathbb{E}(w_{t+h}w_t) = \\begin{cases} \\sigma_w^2,&amp;\\text{ si }h=0\\\\ 0,&amp;\\text{ si }h\\neq0 \\end{cases}\\] donde \\(\\sigma_w^2\\) es la varianza del ruido blanco. Esto significa que la serie es débilmente estacionaria o estacionaria. Si las variables de ruido blanco también son gaussianas, el proceso es estrictamente estacionario, como se pueder ver evaluando (2.10) usando la relación (2.2). #----------------------------------------- # Ruidos blancos #----------------------------------------- # Uniforme [0,1] wu=runif(500,0,1) # Gaussiano wn=rnorm(500,0,1) # Graficos par(mfrow=c(2,1)) plot(wu,type = &quot;l&quot;,xlab = &quot;Num. de observaciones&quot;, main = &quot;Ruido blanco uniforme en [0,1]&quot;) plot(wn,type = &quot;l&quot;,xlab = &quot;Num. de observaciones&quot;, main = &quot;Ruido blanco gaussiano&quot;) # Funciones de autocovarianza (ACF) acf(wu) acf(wn) Ejemplo 3.4 Podemos reemplazar las series de ruido blanco \\(w_t\\) por un promedio móvil que suavice la serie. Por ejemplo, consideremos la serie \\(w_t\\) en la ecuación ( ) y reemplacémosla por un promedio móvil de 3 puntos, dado por \\[\\begin{equation} v_t = \\frac{1}{3}(w_{t-1}+w_t+w_{t+1}) \\tag{3.7} \\end{equation}\\] lo cual nos da una serie suavizada. Tomando la serie del ejemplo anterior y usando la función ‘filter’ de R se obtienen los gráficos siguientes: #------------------------------------------ # Promedio movil #------------------------------------------ # Uniforme vu=filter(wu,sides = 2,rep(1/3,3)) par(mfrow=c(2,1),mar=c(3,4,3,2))# plot.ts(wu,xlab=&quot; &quot;,ylab=&quot;Ruido blanco unif.&quot;) plot.ts(vu,ylim=c(0,1),ylab=&quot;Promedio móvil&quot;) # Gaussiano vn=filter(wn,sides = 2,rep(1/3,3)) par(mfrow=c(2,1),mar=c(3,4,3,2)) plot.ts(wn,xlab=&quot; &quot;,ylab=&quot;Ruido blanco gauss.&quot;) plot.ts(vn,ylim=c(-3,3),ylab=&quot;Promedio móvil&quot;) En la parte superior de cada uno se observan los ruidos blancos y en la parte inferior los respectivos promedios móviles. Podemos notar que las series de promedio móvil suavizan el comportamiento de las series originales, si tomamos más puntos en el promedio mayor será el suavizado. Ejemplo 3.5 (Función de media de un promedio móvil) Si \\(w_t\\) denota una serie de ruido blanco, entonces \\(\\mu_{wt}=\\mathbb{E}(w_t)=0\\) para todo \\(t\\). Luego para el promedio móvil de 3 puntos se tiene \\[\\mu_{wt} = \\mathbb{E}(v_t) = \\frac{1}{3}\\mathbb{E}(w_{t-1}+w_t+w_{t+1}) = \\frac{1}{3}(\\mathbb{E}(w_{t-1})+\\mathbb{E}(w_t)+\\mathbb{E}(w_{t+1}))=0.\\] Ejemplo 3.6 (Autocovarianza de un promedio móvil) Consideremos el promedio móvil de 3 puntos del ejemplo anterior y calculemos su función de autocovarianza \\[\\begin{eqnarray*} \\gamma_v(s,t) &amp;=&amp; \\mathbb{E}[(v_s-\\mu_s)(v_t-\\mu_t)] \\\\ &amp;=&amp; \\mathbb{E}[(v_s-o)(v_t-0)] \\\\ &amp;=&amp; \\frac{1}{9}\\mathbb{E}[(w_{s-1}+w_s+w_{s+1})(w_{t-1}+w_t+w_{t+1})] \\end{eqnarray*}\\] Consideremos \\(s-t=h\\), para \\(h=0,\\pm1,\\pm2,\\ldots\\). Entonces, tenemos para \\(h=0\\) \\[\\begin{eqnarray*} \\gamma_v(t,t) &amp;=&amp; \\frac{1}{9}\\mathbb{E}[(w_{t-1}+w_t+w_{t+1})(w_{t-1}+w_t+w_{t+1})] \\\\ &amp;=&amp; \\frac{1}{9}[\\mathbb{E}(w_{t-1}w_{t-1})+\\mathbb{E}(w_tw_t)+\\mathbb{E}(w_{t+1}w_{t+1})] \\\\ &amp;=&amp; \\frac{3}{9} \\end{eqnarray*}\\] Para \\(h=1\\), tenemos \\[\\begin{eqnarray*} \\gamma_v(t+1,t) &amp;=&amp; \\frac{1}{9}\\mathbb{E}[(w_t+w_{t+1}+w_{t+2})(w_{t-1}+w_t+w_{t+1})] \\\\ &amp;=&amp; \\frac{1}{9}[\\mathbb{E}(w_tw_t)+\\mathbb{E}(w_{t+1}w_{t+1})] \\\\ &amp;=&amp; \\frac{2}{9} \\end{eqnarray*}\\] Usando el hecho de que \\(\\mathbb{E}(w_tw_s)=0\\) si \\(s\\neq t\\). Cálculos similares nos dan \\(\\gamma_v(t-1,t)=2/9, \\gamma_v(t+2,t)=\\gamma_v(t-2,t)=1/9\\) y 0 para \\(h\\geq3\\). Resumiendo se tiene \\[\\begin{equation} \\gamma_v(s,t) = \\begin{cases} 3/9, &amp;\\text{ si }s=t\\\\ 2/9, &amp;\\text{ si }|s-t|=1\\\\ 1/9, &amp;\\text{ si }|s-t|=2\\\\ 0, &amp;\\text{ si }|s-t|\\geq3 \\end{cases} \\tag{3.8} \\end{equation}\\] Ejemplo 3.7 (Estacionaridad de un promedio móvil) El proceso de promedio móvil usado en los ejemplos 3.4 y 3.5 es estacionario ya que podemos escribir la función de autocovarianza obtenida en (3.8) como \\[\\gamma_v(h) = \\begin{cases} 3/9, &amp;\\text{ si }h=0\\\\ 2/9, &amp;\\text{ si }h=\\pm1\\\\ 1/9, &amp;\\text{ si }h=\\pm2\\\\ 0, &amp;\\text{ si }|h|\\geq3 \\end{cases}\\] Ejemplo 3.8 Un modelo para analizar tendencias es el camino aleatorio con tendencia dado por \\[\\begin{equation} X_t = \\delta+X_{t-1}+w_t \\tag{3.9} \\end{equation}\\] para \\(t=1,2,\\ldots,\\) con condición inicial \\(X_0=0\\), y donde \\(w_t\\) es un ruido blanco. La constante \\(\\delta\\) es llamada tendencia, y cuando \\(\\delta=0\\), (3.9) es llamado simplemente camino aleatorio. El término camino aleatorio viene del hecho de que cuando \\(\\delta=0\\) el valor de la serie de tiempo en tiempo \\(t\\) es el valor de la serie de tiempo al tiempo \\(t-1\\) más un movimiento completamente aleatorio determinado por \\(w_t\\). La expresión (3.9) la podemos reescribir como una suma de variables de ruido blanco, esto es, \\[\\begin{equation} X_t = \\delta t+\\sum_{j=1}^Nw_j \\tag{3.10} \\end{equation}\\] para \\(t=1,2,\\ldots.\\) A continuación generaremos un camino aleatorio usando R set.seed(154) w=rnorm(500,0,1) X=cumsum(w) wd=w+0.2; Xd=cumsum(wd) plot.ts(Xd,ylim=c(-40,80)) lines(X,col=&quot;red&quot;) lines(0.2*(1:500),lty=&quot;dashed&quot;,col=&quot;blue&quot;) Figura 3.1: Gráficos de caminos aleatorios: con tendencia (negro), sin tendencia (rojo) "],
["modelos-ar.html", "Capítulo 4 Modelos AR 4.1 Modelo AR(1) 4.2 Modelo AR(2) 4.3 Procesos AR(p)", " Capítulo 4 Modelos AR Los modelos autoregresivos están basados en la idea de que el valor actual de la serie \\(x_t\\) se puede explicar como una función de \\(p\\) valores pasados \\(x_{t-1},x_{t-2},\\ldots,x_{t-p}\\) donde \\(p\\) determina el número de pasos en necesarios para predecir el valor actual. Una parte de las series de tiempo económicas y financieras suelen ser caracterizadas por los modelos autorregresivos. Entre los principales ejemplos de las finanzas tenemos valoración de precios y de dividendos, las tasas reales de cambio, tasas de interés y los diferenciales de tipos de interés (spreads). Definición 4.1 Un modelo autoregresivo de orden \\(p\\), abreviado \\(AR(p)\\) es de la forma \\[\\begin{equation} x_t=\\phi_1x_{t-1}+\\phi_2x_{t-2}+\\cdots+\\phi_px_{t-p}+w_t, \\tag{4.1} \\end{equation}\\] donde \\(x_t\\) es estacionario, \\(\\phi_1\\phi_2,\\ldots,\\phi_p\\) son constantes (\\(\\phi_p\\neq0\\)). A menos que se declare lo contrario, se asume que \\(w_t\\) es un ruido blanco gaussiano de media cero y varianza \\(\\sigma_w^2\\). La media de \\(x_t\\) en (4.1) es cero. Si la media \\(\\mu\\) de \\(x_t\\) no es cero, reemplazamos \\(x_t\\) por \\(x_t-\\mu\\) en (4.1), es decir \\[x_t-\\mu=\\phi_1(x_{t-1}-\\mu)+\\phi_2(x_{t-2}-\\mu)+\\cdots+\\phi_p(x_{t-p}-\\mu)+w_t\\] o escribimos \\[\\begin{equation} x_t=\\alpha+\\phi_1x_{t-1}+\\phi_2x_{t-2}+\\cdots+\\phi_px_{t-p}+w_t, \\tag{4.2} \\end{equation}\\] donde \\(\\alpha=\\mu(1-\\phi_1-\\phi_2-\\cdots-\\phi_p)\\). Note que (4.2) es similar al modelo de regresión dado en (2.53) y por consiguiente el término autoregresión. Sin embargo, se presentan algunas dificultades técnicas para la aplicación de este modelo, porque los regresores \\(x_{t-1},x_{t-2},\\ldots,x_{t-p}\\) son aleatorios, mientras que \\(x_t\\) se asume fijo. Una forma más útil se deriva de usar el siguiente operador de cambio dado por (2.42). Para escribir el modelo \\(AR(p)\\) como \\[\\begin{equation} (1-\\phi_1B-\\phi_2B^2-\\cdots-\\phi_pB^p)x_t=w_t \\tag{4.3} \\end{equation}\\] o más conciso como \\[\\begin{equation} \\phi(B)x_t=w_t. \\tag{4.4} \\end{equation}\\] Las propiedades de \\(\\phi(B)\\) son importantes para resolver (4.4). Esto nos lleva a la siguiente definición. Definición 4.2 El operador autoregresivo de orden \\(p\\) se define como \\[\\begin{equation} \\phi(B) = 1-\\phi_1B-\\phi_2B^2-\\cdots-\\phi_pB^p \\tag{4.5} \\end{equation}\\] 4.1 Modelo AR(1) Iniciaremos el estudio de los modelos \\(AR\\) considerando el modelo de primer orden \\(AR(1)\\), dado por \\(x_t=\\phi x_{t-1}+w_t\\). Iterando el operador de cambio \\(k\\) veces, obtenemos \\[\\begin{eqnarray*} x_t &amp;=&amp; \\phi x_{t-1}+w_t = \\phi(\\phi x_{t-2}+w_{t-1})+w_t \\\\ &amp;=&amp; \\phi^2x_{t-2}+\\phi w_{t-1}+w_t \\\\ &amp;\\vdots&amp; \\\\ &amp;=&amp; \\phi^kx_{t-k}+\\sum_{j=0}^{k-1}\\phi^jw_{t-j}. \\end{eqnarray*}\\] Este método sugiere que por iteración continua del operador de cambio, siempre que \\(|\\phi|&lt;1\\) y \\(x_t\\) sea estacionario, podemos representar un modelo \\(AR(1)\\) como un proceso lineal dado por3 \\[\\begin{equation} x_t \\sum_{j=0}^{\\infty}\\phi^jw_{t-j} \\tag{4.6} \\end{equation}\\] El proceso \\(AR(1)\\) definido en (4.6) es estacionario con media \\[\\mathbb{E}(x_t) = \\sum_{j=0}^{\\infty}\\phi^j\\mathbb{E}(w_{t-j})=0,\\] y la función de autocovarianza es \\[\\begin{eqnarray} \\gamma(h) &amp;=&amp; Cov(x_{t+h},x_t) \\nonumber \\\\ &amp;=&amp; \\mathbb{E}\\left[\\left(\\sum_{j=0}^{\\infty}\\phi^jw_{t+h-j}\\right) \\left(\\sum_{k=0}^{\\infty}\\phi^kw_{t-k}\\right)\\right] \\nonumber \\\\ &amp;=&amp; \\sigma_w^2\\sum_{j=0}^{\\infty}\\phi^j\\phi^{j+h} = \\sigma_w^2\\phi^h\\sum_{j=0}^{\\infty}\\phi^{2j} \\nonumber \\\\ &amp;=&amp; \\frac{\\sigma_w^2\\phi^h}{1-\\phi^2}, h&gt;0 \\tag{4.7} \\end{eqnarray}\\] Recuerde que \\(\\gamma(h)=\\gamma(-h)\\) de modo que basta presentar la función de autocovarianza para \\(h\\geq0\\). Si en (4.7), hacemos \\(h=0\\), obtenemos la varianza del proceso \\(AR(1)\\), siendo esta \\[Var(x_t)=\\frac{\\sigma_w^2}{1-\\phi^2},\\] asumiendo que \\(\\phi_1^2&lt;1\\). El requisito de que \\(\\phi_1^2&lt;1\\) resulta del hecho de que la varianza de una variable aleatoria es acotada y no negativa. Por consiguiente, la estacionaridad de un modelo \\(AR(1)\\) implica que \\(-1&lt;\\phi_1&lt;1\\). Pero si \\(-1&lt;\\phi_1&lt;1\\), entonces por (4.6) y la independencia de \\(\\{w_t\\}\\) se puede demostrar que la media y la varianza de \\(x_t\\) son finitas. Además, por la desigualdad de Cauchy-Schwartz todas las autocovarianzas de \\(x_t\\) son finitas. Por lo tanto, el modelo \\(AR(1)\\) es estacionario. En resumen, una condición necesaria y suficiente para que un proceos \\(AR(1)\\) sea estacionario es \\(|\\phi_1|&lt;1\\). De (4.7) la ACF de un modelo \\(AR(1)\\) es \\[\\begin{equation} \\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\phi^h, \\quad h&gt;0 \\tag{4.8} \\end{equation}\\] y \\(\\rho(h)\\) satisface la recursión \\[\\begin{equation} \\rho(h) = \\phi\\rho(h-1)\\text{, para }h=1,2,\\ldots. \\tag{4.9} \\end{equation}\\] Las ecuaciones (4.8) y (4.9) indican que la ACF de un modelo \\(AR(1)\\) estacionario tiene un decaimiento exponencial con tasa igual a \\(\\phi_1\\). Si \\(\\phi_1&gt;0\\) el decaimiento es constante. Si por el contrario, \\(\\phi_1&lt;0\\) entonces el decaimiento es compuesto y se presenta de forma alternante con tasa \\(\\phi_1^2\\). Para tener una idea de esto, consideremos los modelos autoregresivos de orden 1 simulados, para distintos valores de \\(\\phi_1\\). # Coeficientes phi phi1=0.9 phi2=-0.8 phi3=0.4 phi4=-0.5 # Ruido blanco gaussiano w=rnorm(100,0,1) # Series AR(1) ar1_1=filter(w,filter = phi1,method = &quot;recursive&quot;) ar1_2=filter(w,filter = phi2,method = &quot;recursive&quot;) ar1_3=filter(w,filter = phi3,method = &quot;recursive&quot;) ar1_4=filter(w,filter = phi4,method = &quot;recursive&quot;) # Graficos par(mfrow=c(2,2)) plot.ts(ar1_1, col=&quot;blue&quot;,type = &quot;l&quot;, main = &quot;AR(1) con phi=0.9&quot;,xlab=&quot;t&quot;,ylab=&quot;x_t&quot;) plot.ts(ar1_2, col=&quot;blue&quot;,type = &quot;l&quot;, main = &quot;AR(1) con phi=-0.8&quot;,xlab=&quot;t&quot;,ylab=&quot;x_t&quot;) plot.ts(ar1_3, col=&quot;blue&quot;,type = &quot;l&quot;, main = &quot;AR(1) con phi=0.4&quot;,xlab=&quot;t&quot;,ylab=&quot;x_t&quot;) plot.ts(ar1_4, col=&quot;blue&quot;,type = &quot;l&quot;, main = &quot;AR(1) con phi=-0.5&quot;,xlab=&quot;t&quot;,ylab=&quot;x_t&quot;) Figura 4.1: Simulaciones de procesos autoregresivos de orden 1, AR(1), para distintos valores de \\(phi_1\\) A continuación mostramos las funciones de autocovarianzas de las series AR(1) simuladas anteriormente par(mfrow=c(2,2)) acf(ar1_1,type = &quot;covariance&quot;, main=&quot;ACF de la Serie AR(1) con phi=0.9&quot;) acf(ar1_2,type = &quot;covariance&quot;, main=&quot;ACF de la Serie AR(1) con phi=-0.8&quot;) acf(ar1_3,type = &quot;covariance&quot;, main=&quot;ACF de la Serie AR(1) con phi=0.4&quot;) acf(ar1_4,type = &quot;covariance&quot;, main=&quot;ACF de la Serie AR(1) con phi=-0.5&quot;) Figura 4.2: Funciones de autocovarianzas para las series AR(1) simuladas 4.2 Modelo AR(2) Un proceso \\(AR(2)\\) tiene la forma general \\[\\begin{equation} x_t = \\alpha + \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + w_t \\tag{4.10} \\end{equation}\\] siendo \\(\\alpha = \\mu(1-\\phi_1-\\phi_2)\\), con \\(\\phi_2\\neq 0\\). Podemos calcular su función de media \\[\\begin{eqnarray*} \\mathbb{E}(x_t) &amp;=&amp; \\mathbb{E}(\\alpha + \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + w_t) \\\\ &amp;=&amp; \\alpha+\\phi_1\\mathbb{E}(x_{t-1})+\\phi_2\\mathbb{E}(x_{t-2}) \\end{eqnarray*}\\] Por estacionalidad, se tiene que \\(\\mathbb{E}(x_t)=\\mathbb{E}(x_{t-1})=\\mathbb{E}(x_{t-2})\\), luego \\[\\mathbb{E}(x_t)(1-\\phi_1-\\phi_2) = \\alpha\\] Así, \\(\\mathbb{E}(x_t) = \\frac{\\alpha}{1-\\phi_1-\\phi_2}\\), siempre que \\(\\phi_1+\\phi_2\\neq1\\). Usando \\(\\alpha=(1-\\phi_1-\\phi_2)\\mu\\) podemos reescribir el proceso \\(AR(2)\\) como \\[x_t-\\mu = \\phi_1(x_{t-1}-\\mu)+\\phi_2(x_{t-2}-\\mu)+w_t.\\] Multiplicando por \\(x_{t-h}-\\mu\\), tenemos \\[(x_{t-h}-\\mu)(x_t-\\mu) = \\phi_1(x_{t-h}-\\mu)(x_{t-1}-\\mu) + \\phi_2(x_{t-h}-\\mu)(x_{t-2}-\\mu) + (x_{t-h}-\\mu)w_t.\\] Tomando valor esperado y usando el hecho de que \\(\\mathbb{E}[(x_{t-h}-\\mu)w_t]=0\\), para \\(h&gt;0\\), obtenemos \\[\\begin{equation} \\gamma(h) = \\phi_1\\gamma(h-1)+\\phi_2\\gamma(h-2) \\text{, para }h&gt;0. \\tag{4.11} \\end{equation}\\] Este último resultado se conoce como la ecuación de momentos de un proceso estacionario \\(AR(2)\\). Dividiendo (4.11) por \\(\\gamma(0)\\), tenemos la propiedad \\[\\begin{equation} \\rho(h) = \\phi_1\\rho(h-1)+\\phi_2\\rho(h-2)\\text{, para }h&gt;0 \\tag{4.12} \\end{equation}\\] para la ACF de \\(x_t\\). En particular, para paso 1 (\\(h=1\\)) la ACF satisface \\[\\rho(1) = \\phi_1\\rho(0)+\\phi_2\\rho(-1) = \\phi_1+\\phi_2\\rho(1)\\] Por lo tanto, para un proceso \\(AR(2)\\) estacionario \\(x_t\\), tenemos \\[\\begin{eqnarray*} \\rho(0) &amp;=&amp; 1 \\\\ \\rho(1) &amp;=&amp; \\frac{\\phi_1}{1-\\phi_2} \\\\ \\rho(h) &amp;=&amp; \\phi_1\\rho(h-1)+\\phi_2\\rho(h-2),\\quad h\\geq2 \\end{eqnarray*}\\] El resultado de la ecuación (4.12) nos dice que la ACF de un proceso estacionario \\(AR(2)\\) satisface la ecuación en diferencias de segundo orden \\[\\begin{equation} (1-\\phi_1B-\\phi_2B^2)\\rho(h) = 0 \\tag{4.13} \\end{equation}\\] donde \\(B\\) es el operador definido en (2.42). La ecuación (4.13) determina las propiedades de la ACF de un proceso \\(AR(2)\\) estacionario. También determina el comportamiento de los pronósticos de \\(x_t\\). Correspondiendo a la ecuación en diferencias anterior, existe una ecuación polinómica de segundo orden \\[\\begin{equation} x^2-\\phi_1x-\\phi_2=0 \\tag{4.14} \\end{equation}\\] Las soluciones de esta ecuación son las raíces características de un proceso \\(AR(2)\\) y estas son \\[x=\\frac{\\phi_1\\pm\\sqrt{\\phi_1^2+4\\phi_2}}{2}\\] Denotamos las dos raíces por \\(r_1\\) y \\(r_2\\). Si ambos son reales, entonces la ecuación en diferencias de segundo orden la podemos factorizar como \\[(1-r_1B)(1-r_2B)\\] y el proceso \\(AR(2)\\) lo podemos considerar como un proceso \\(AR(1)\\) operando sobre otro proceso \\(AR(1)\\). La ACF de \\(x_t\\) es entonces una mezcla de dos decaimientos exponenciales. Pero si \\(\\phi_1^2+4\\phi_2&lt;0\\), entonces \\(r_1\\) y \\(r_2\\) son raíces complejas conjugadas, y el gráfico de la ACF de \\(x_t\\) mostrará un amortiguamiento de senos y cosenos. En aplicaciones financieras y económicas, las raíces caracteríticas complejas son importantes. Dan lugar al comportamiento de los ciclos económicos. Por lo tanto, es común que los modelos económicos de series de tiempo tengan raíces características de valor complejo. Para un proceso \\(AR(2)\\) dado por (4.10) con raíces características complejas, la longitud promedio de un ciclo estocástico es \\[k=\\frac{360°}{\\arccos(\\phi_1/2\\sqrt{-\\phi_2})},\\] donde el arcocoseno está expresado en grados. La figura siguiente muestra la ACF de 4 procesos estacionarios \\(AR(2)\\). Los procesos \\(AR(2)\\) mostrados son: \\(x_t=1.2x_{t-1}-0.35x_{t-2}+w_t\\) \\(x_t=0.6x_{t-1}-0.4x_{t-2}+w_t\\) \\(x_t=0.2x_{t-1}+0.35x_{t-2}+w_t\\) \\(x_t=-0.2x_{t-1}+0.35x_{t-2}+w_t\\) Figura 4.3: Cuatro procesos estacionarios AR(2) con distintos valores de phi1 y phi2 Figura 4.4: ACF de 4 procesos estacionarios AR(2) con distintos valores de phi1 y phi2 La serie (b) tiene raíces características complejas, en efecto \\[\\phi_1^2+4\\phi_2=(0.6)^2+4\\times(-0.4)-1.24&lt;0\\] Se puede notar que n el gráfico de la ACF que este exhibe un comportamiento de ondas de senos y cosenos. Los otros 3 procesos \\(AR(2)\\) tienen raíces características reales, por lo que las ACF decaen exponencialmente. La condición de estacionaridad de un proceso \\(AR(2)\\) es que los valores absolutos de sus raíces características sean menor que uno, esto es \\(|\\phi_1|&lt;1, |\\phi_2|&lt;1\\). Bajo esta condición, la ecuación recursiva (4.12) asegura que la ACF del proceso converge a cero cuando el salto \\(h\\) crece. Esta propiedad de convergencia es una condición necesaria para una serie de tiempo estacionaria. De hecho, la condición también aplica para un proceso \\(AR(1)\\) donde la ecuación polinómica es \\(x-\\phi_1=0\\). La raíz característica es \\(x=\\phi_1\\), la cual debe ser menor que uno en módulo para que \\(x_t\\) sea estacionario. Como mostramos antes, para un proceso estacionario \\(AR(1)\\) la ACF es \\(\\rho(h)=\\phi^h\\), (4.8). Así, la condición \\(|\\phi|&lt;1\\), asegura que \\(\\rho(h)=\\phi^h\\to0\\), cuando \\(h\\to\\infty\\). 4.3 Procesos AR(p) Los resultados de los procesos \\(AR(1)\\) y \\(AR(2)\\), los podemos generalizar a procesos \\(AR(p)\\). Así, la función de media del proceso \\(AR(p)\\) estacionario será \\[\\begin{equation} \\mathbb{E}(x_t) = \\frac{\\alpha}{1-\\phi_1-\\cdots-\\phi_p} \\tag{4.15} \\end{equation}\\] siempre que el denominador sea distinto de cero. La ecuación polinómica asociada al modelo es \\[\\begin{equation} x^p-\\phi_1x^{p-1}-\\phi_2x^{p-2}-\\cdots-\\phi_p=0 \\tag{4.16} \\end{equation}\\] la cual nos referimos como la ecuación característica del modelo. Si todas las raíces caractarísticas de esta ecuación son menores qye uno en módulo, esto es \\(|r_j|&lt;1\\), con \\(j=1,\\ldots,p\\), entonces la serie \\(x_t\\) es estacionaria. Para un proceso \\(AR(p)\\) estacionario, la ACF satisface la ecuación en diferencias \\[(1-\\phi_1B-\\phi_2B^2-\\cdots-\\phi_pB^p)\\rho(h)=0\\text{, para }h&gt;0.\\] El gráfico de la ACF de un proceso \\(AR(p)\\) estacionario mostrará una mezcla de ondas de senos y cosenos con decaimientos exponenciales dependiendo de la naturaleza de sus raíces características. Ejemplo 4.1 Consideremos el modelo \\(AR(3)\\) de la forma \\[x_t=0.0047+0.35x_{t-1}+0.18x_{t-2}-0.14x_{t-3}+w_t.\\] Reescribiendo el proceso como \\[x_t-0.35x_{t-1}-0.18x_{t-2}+0.14x_{t-3}=0.0047+w_t\\] obtenemos la correspondiente ecuación en diferencias de orden 3, \\[(1-0.35B-0.18B^2+0.14B^3)=0\\] la cual podemos factorizar como \\[(1+0.52B)(1-0.87B+0.27B^2)=0\\] El primer factor \\((1+0.52B)=0\\), muestra u ndecaimiento exponencial en la ACF. Veamos ahora el segundo factor \\((1-0.87B-(-0.27)B^2)=0\\), tenemos que \\(\\phi_1^2+4\\phi_2=(0.87)^2+4(-0.27)=-0.3231&lt;0\\). Por consiguiente la ACF mostrará un comportamiento en ondas de senos y cosenos. xt&lt;-arima.sim(list(order=c(3,0,0),ar=c(0.35,0.18,-0.14)),n=100) par(mfrow=c(2,1)) plot(xt,type=&quot;l&quot;,main=&quot;Proceso AR(3)&quot;) acf(xt, main=&quot;ACF para el proceso AR(3)&quot;) Note que \\(\\lim_{k\\to\\infty}\\mathbb{E}(x_t-\\sum_{j=0}^{\\infty}\\phi^jw_{t-j})^2 = \\lim_{k\\to\\infty}\\phi^{2k}\\mathbb{E}(x_{t-k}^2)=0\\), de modo que (4.6) existe en el sentido de media cuadrado.↩ "],
["referencias.html", "Referencias", " Referencias "]
]
