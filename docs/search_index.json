[
["index.html", "Series de Tiempo en R Ciencia de los Datos Financieros Prefacio ¿Por qué leer este libro? Estructura del libro Información sobre los programas y convenciones Prácticas interactivas con R Agradecimientos", " Series de Tiempo en R Ciencia de los Datos Financieros Synergy Vision 2022-11-12 Prefacio La versión en línea de este libro se comparte bajo la licencia Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ¿Por qué leer este libro? Estructura del libro Información sobre los programas y convenciones Este libro es posible gracias a una gran cantidad de desarrolladores que contribuyen en la construcción de herramientas para generar documentos enriquecidos e interactivos. En particular al autor de los paquetes Yihui Xie xie2015. Prácticas interactivas con R Vamos a utilizar el paquete Datacamp Tutorial que utiliza la librería en JavaScript Datacamp Light para crear ejercicios y prácticas con R. De esta forma el libro es completamente interactivo y con prácticas incluidas. De esta forma estamos creando una experiencia única de aprendizaje en línea. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImIgPC0gNSIsInNhbXBsZSI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5cblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGEiLCJzb2x1dGlvbiI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5hIDwtIDVcblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGFcbmEiLCJzY3QiOiJ0ZXN0X29iamVjdChcImFcIilcbnRlc3Rfb3V0cHV0X2NvbnRhaW5zKFwiYVwiLCBpbmNvcnJlY3RfbXNnID0gXCJBc2VnJnVhY3V0ZTtyYXRlIGRlIG1vc3RyYXIgZWwgdmFsb3IgZGUgYGFgLlwiKVxuc3VjY2Vzc19tc2coXCJFeGNlbGVudGUhXCIpIn0= Agradecimientos Synergy Vision, Caracas, Venezuela "],
["acerca-del-autor.html", "Acerca del Autor", " Acerca del Autor Este material es un esfuerzo de equipo en Synergy Vision, (http://synergy.vision/nosotros/). El propósito de este material es ofrecer una experiencia de aprendizaje distinta y enfocada en el estudiante. El propósito es que realmente aprenda y practique con mucha intensidad. La idea es cambiar el modelo de clases magistrales y ofrecer una experiencia más centrada en el estudiante y menos centrado en el profesor. Para los temas más técnicos y avanzados es necesario trabajar de la mano con el estudiante y asistirlo en el proceso de aprendizaje con prácticas guiadas, material en línea e interactivo, videos, evaluación contínua de brechas y entendimiento, entre otros, para procurar el dominio de la materia. Nuestro foco es la Ciencia de los Datos Financieros y para ello se desarrollará material sobre: Probabilidad y Estadística Matemática en R, Programación Científica en R, Mercados, Inversiones y Trading, Datos y Modelos Financieros en R, Renta Fija, Inmunización de Carteras de Renta Fija, Teoría de Riesgo en R, Finanzas Cuantitativas, Ingeniería Financiera, Procesos Estocásticos en R, Series de Tiempo en R, Ciencia de los Datos, Ciencia de los Datos Financieros, Simulación en R, Desarrollo de Aplicaciones Interactivas en R, Minería de Datos, Aprendizaje Estadístico, Estadística Multivariante, Riesgo de Crédito, Riesgo de Liquidez, Riesgo de Mercado, Riesgo Operacional, Riesgo de Cambio, Análisis Técnico, Inversión Visual, Finanzas, Finanzas Corporativas, Valoración, Teoría de Portafolio, entre otros. Nuestra cuenta de Twitter es (https://twitter.com/bysynergyvision) y nuestros repositorios están en GitHub (https://github.com/synergyvision). Somos Científicos de Datos Financieros "],
["introducción.html", "Capítulo 1 Introducción 1.1 Conceptos financieros básicos 1.2 Conceptos básicos 1.3 Ejemplos 1.4 Componentes de una serie de tiempo", " Capítulo 1 Introducción Las series de tiempo ya han desempeñado un papel importante en las primeras ciencias naturales. La astronomía babilónica utilizó series de tiempo de las posiciones relativas de estrellas y planetas para predecir eventos astronómicos. Las observaciones de los movimientos de los planetas formaron la base de las leyes que Johannes Kepler descubrió. El análisis de las series de tiempo ayuda a detectar las regularidades en las observaciones de una variable y a derivar “leyes” de ellas, y/o explotar toda la información incluida en esta variable para predecir mejor los desarrollos futuros. La idea metodológica básica detrás de estos procedimientos, que también eran válidos para los babilonios, es que es posible descomponer series de tiempos en un número finito de componentes independientes pero no directamente observables que se desarrollan regularmente y que por lo tanto pueden ser calculados de antemano. Para este procedimiento es necesario que existan diferentes factores independientes que incidan en la variable. A mediados del siglo XIX, este enfoque metodológico de la astronomía fue asumido por los economistas Charles Babbage y William Stanley Jevons. La descomposición en componentes no observados que dependen de diferentes factores causales, como suele emplearse en el análisis clásico de series de tiempo, fue desarrollada por Warren M. Persons (1919). Distinguía cuatro componentes diferentes: Desarrollo a largo plazo, tendencia, Componente cíclico con períodos de más de un año, el ciclo económico, Componente que contiene los altibajos dentro de un año, el ciclo estacional, y Componente que contiene todos los movimientos que no pertenecen ni a la tendencia ni al ciclo económico ni al componente estacional, el residual. Suponiendo que los diferentes factores no observables son independientes, su recubrimiento aditivo genera las series de tiempo que, sin embargo, sólo podemos observar en su conjunto. Para obtener información sobre el proceso de generación de datos, tenemos que hacer suposiciones sobre sus componentes no observados. El análisis clásico de series de tiempo supone que los componentes sistemáticos, es decir, la tendencia, el ciclo económico y el ciclo estacional, no están influenciados por perturbaciones estocásticas y, por lo tanto, pueden representarse mediante funciones determinísticas del tiempo. El impacto estocástico se limita a los residuos, que, por otra parte, no contienen movimientos sistemáticos. Por lo tanto, se modela como una serie de variables aleatorias independientes o no correlacionadas con esperanza cero y varianza constante, es decir, como un proceso aleatorio puro. Este enfoque cambió desde la presentación de los trabajos de George E. P. Box and Gwilym M. Jenkins, “Time Series Analysis: Forecasting and Control”, en los años 70 del siglo XX. Se abandonaron los procedimientos puramente descriptivos del análisis clásico de series de tiempo y, en su lugar, se han utilizado los resultados y métodos de la teoría de la probabilidad y las estadísticas matemáticas. Desde ese entonces, el análisis de series ha tenido un desarrollo creciente. Se han presentado una gran variedad de libros sobre este tópico, cada uno de ellos influenciado principalmente por la orientación de las series que se discuten en sus contenidos. Una gran parte de la literatura está dirigida a exponer los aspectos teóricos alrededor de las series de tiempo, siendo en muchos casos, rigurosamente desarrollados y descritos, sin embargo poco de ellos presentan implementaciones de las técnicas estudiadas y su compresión en ejemplos reales lo que a veces puede dificultar su comprensión en especial para aquellos que no posean una apropiada formación matemática. Los primeros intentos de estudiar el comportamiento de las series de tiempo financieras fueron realizados por profesionales financieros y periodistas en lugar de por académicos. De hecho, esto parece haberse convertido en una tradición de larga data, ya que, incluso hoy en día, gran parte de la investigación y el desarrollo empíricos todavía se originan en la propia industria financiera. Esto puede explicarse por el carácter práctico de los problemas, la necesidad de datos especializados y las posibles ventajas de dicho análisis. El primer y más conocido ejemplo de la investigación publicada sobre series de tiempo financieras es el legendario Charles Dow, como se expresa en sus editoriales en el Wall Street Times entre 1900 y 1902. Estos escritos formaron la base de la “teoría del Dow” e influyeron en lo que más tarde se conoció como análisis técnico y carisma. Aunque Dow no coleccionó y publicó sus editoriales por separado, esto fue hecho póstumamente por su seguidor Samuel Nelson (Nelson, 1902). Las ideas originales de Dow fueron posteriormente interpretadas y ampliadas por Hamilton (1922) y Rhea (1932). Estas ideas gozaron de cierto reconocimiento entre los académicos de la época: por ejemplo, Hamilton fue elegido miembro de la Royal Statistical Society. Aunque Dow y sus seguidores discutieron muchas de las ideas que encontramos en el análisis moderno de finanzas y series de tiempo, incluyendo estacionalidad, eficiencia del mercado, correlación entre rendimiento de activos e índices, diversificación e imprevisibilidad, no hicieron ningún esfuerzo serio para adoptar métodos estadísticos formales. La mayor parte del análisis empírico consistió en la interpretación minuciosa de gráficos detallados de las medias bursátiles sectoriales, formando así los famosos Índices Dow-Jones. Se argumentó que estos índices descuentan toda la información necesaria y proporcionan el mejor pronóstico de eventos futuros. Una idea fundamental, muy relevante para la teoría de los ciclos de Stanley Jevons y la metodología de descomposición de tendencias de la “curva Harvard A-B-C” de Warren Persons, fue que las variaciones de precios del mercado consistían en tres movimientos primarios: diarios, a medio y largo plazo. La investigación empírica más temprana que utiliza métodos estadísticos formales se remonta a los documentos de Working (1934), Cowles (1933,1944) y Cowles and Jones (1937). El trabajo centró la atención en una característica previamente señalada de los precios de las materias primas y las acciones: que se asemejan a la acumulación de cambios puramente aleatorios. Alfred Cowles 3rd, analista financiero cuantitativamente entrenado y fundador de Econometric Society and the Cowles Foundation, investigó la habilidad de los analistas de mercado y servicios financieros para predecir los futuros cambios de precios, encontrando que había pocas pruebas de que pudieran hacerlo. Cowles y Jones reportaron evidencia de correlación positiva entre sucesivas variaciones de precios, pero, como posteriormente Cowles (1960) comentó, esto fue probablemente debido a que tomaron promedios mensuales de precios diarios o semanales antes de computar los cambios: un fenómeno de “correlación espuria”, analizado por Working (1960). La previsibilidad de los cambios de precios se ha convertido desde entonces en un tema importante de la investigación financiera, pero, sorprendentemente, poco más se publicó hasta el estudio de Kendall (1953), en el que encontró que los cambios semanales en una amplia variedad de precios financieros no podían predecirse ni a partir de los cambios pasados en las series ni a partir de los cambios pasados en otras series de precios. Este parece haber sido el primer informe explícito de esta propiedad de los precios financieros a menudo citada, aunque la investigación sobre la previsibilidad de los precios sólo se vio impulsada por la publicación de los documentos de Roberts (1959) y Osborne (1959). El primero presenta un argumento en gran medida heurístico sobre por qué las sucesivas variaciones de precios deben ser independientes, mientras que el segundo desarrolla la proposición de que no se trata de cambios absolutos de precios, sino de cambios logarítmicos de precios independientes entre sí. Con la suposición auxiliar de que las propias modificaciones se distribuyen normalmente, esto implica que los precios se generan como movimiento Browniano. El análisis de series de tiempo desempeña un papel importante en el análisis requerido para el pronóstico de eventos futuros. Existen varias formas o métodos de calcular cual va a ser la tendencia del comportamiento del proceso en estudio. Un pronóstico es una predicción de algún evento o eventos futuros. Como sugirió Neils Bohr, hacer buenas predicciones no siempre es fácil. Los pronósticos famosamente “malos” incluyen lo siguiente del libro “Malas Predicciones”: “La población es de tamaño constante y se mantendrá hasta el fin de la humanidad.” La Enciclopedia, 1756. “1930 será un espléndido año de empleo.” Departamento de Trabajo de los EE.UU., pronóstico de Año Nuevo en 1929, justo antes de que el mercado se desplomara el 29 de octubre. “Las computadoras se multiplican a un ritmo rápido. Para el cambio de siglo habrá 220,000 en los EE.UU.” Wall Street Journal, 1966. Algunos ejemplos donde se puede utilizar y hacer precciones con series de tiempo: Dirección de Operaciones. Las organizaciones empresariales utilizan habitualmente las previsiones de ventas de productos o la demanda de servicios para programar la producción, controlar los inventarios, gestionar la cadena de suministro, determinar las necesidades de personal y planificar la capacidad. Las previsiones también pueden utilizarse para determinar la combinación de productos o servicios que deben ofrecerse y las ubicaciones en las que deben fabricarse los productos. Marketing. La previsión es importante en muchas decisiones de marketing. Las previsiones de respuesta de las ventas a los gastos publicitarios, las nuevas romociones o los cambios en las políticas de precios permiten a las empresas evaluar su eficacia, determinar si se están alcanzando los objetivos y realizar ajustes. Finanzas y Gestión de Riesgos. Los inversores en activos financieros están interesados en pronosticar los rendimientos de sus inversiones. Estos activos incluyen, pero no se limitan a acciones, bonos y materias primas; otras decisiones de inversión se pueden tomar en relación con las previsiones de tasas de inter?s, opciones y tipos de cambio. La gestión del riesgo financiero requiere previsiones de la volatilidad de la rentabilidad de los activos para que se puedan evaluar y asegurar los riesgos asociados a las carteras de inversion, y para que los derivados financieros puedan cotizarse adecuadamente. Economía. Los gobiernos, las instituciones financieras y las organizaciones de política requieren pronósticos de las principales variables económicas, como el producto interno bruto, el crecimiento demográfico, el desempleo, las tasas de interés, la inflación, el crecimiento del empleo, la producción y el consumo. Estas previsiones son parte integrante de la orientación de la política monetaria y fiscal, así como de los planes y decisiones presupuestarias adoptadas por los gobiernos. también son fundamentales en las decisiones de planificación estratégica tomadas por organizaciones empresariales e instituciones financieras. Control de Procesos Industriales. Las previsiones de los valores futuros de las características de calidad crítica de un proceso de producción pueden ayudar a determinar cuándo deben cambiarse las variables controlables importantes del proceso, o si el proceso debe detenerse y revisarse. Los esquemas de retroalimentación y control feedforward son ampliamente utilizados en el monitoreo y ajuste de procesos industriales, y las predicciones de la producción del proceso son una parte integral de estos esquemas. Demografía. Las previsiones de población por país y región se realizan de manera rutinaria, a menudo estratificadas por variables como el género, la edad y la raza. Los demógrafos también pronostican nacimientos, muertes y patrones migratorios de las poblaciones. Los gobiernos utilizan estas previsiones para planificar políticas y acciones de servicio social, como el gasto en atención médica, programas de jubilación y programas de lucha contra la pobreza. Muchas empresas utilizan pronósticos de poblaciones por grupos de edad para hacer planes estratégicos en relación con el desarrollo de nuevas líneas de productos o tipos de servicios que será ofrecido. 1.1 Conceptos financieros básicos La mayoría de los estudios financieros y econ?micos implican rendimiento, en lugar de precios de los activos. Existen dos buenas razones para ello. Primero, para los inversores medios, el rendimiento de un activo es un resumen completo y libre de escala de la oportunidad de inversión. Segundo, las series de rendimiento son más fáciles de manejar que las series de precios porque las primeras tienen propiedades estadísticas más atractivas. Sin embargo, existen varias definiciones de rendimiento de activos. Sea \\(P_t\\) el precio de un activo en tiempo \\(t\\). Discutiremos algunas definiciones de rendimiento que utilizaremos a lo largo del libro. Supongamos por el momento que el activo no paga dividendos. Definición 1.1 (Rendimiento simple de un periodo) Mantener el activo fijo durante un periodo a partir de tiempo \\(t-1\\) hasta tiempo \\(t\\) da lugar a una rentabilidad bruta simple \\[\\begin{equation} 1+R_t=\\frac{P_t}{P_{t-1}}\\quad\\text{ o }\\quad P_t=P_{t-1}(1+R_t) \\tag{1.1} \\end{equation}\\] El correspondiente rendimiento neto simple de un periodo o rendimiento simple es \\[\\begin{equation} R_t=\\frac{P_t}{P_{t-1}}-1=\\frac{P_t-P_{t-1}}{P_{t-1}} \\tag{1.2} \\end{equation}\\] Definición 1.2 (Rendimiento simple multiperiodo) Mantener el activo fijo durante \\(k\\) periodos entre los tiempos \\(t-k\\) y \\(t\\) da un rendimiento bruto simple de periodo \\(k\\) \\[\\begin{eqnarray*} 1+ R_t[k] &amp;=&amp; \\frac{P_t}{P_{t-k}} = \\frac{P_t}{P_{t-1}}\\times\\frac{P_{t-1}}{P_{t-2}}\\times \\cdots \\times\\frac{P_{t-k+1}}{P_{t-k}} \\\\ &amp;=&amp; (1+R_t)(1+R_{t-1})\\cdots(1+R_{t-k+1}) \\\\ &amp;=&amp; \\prod_{j=0}^{k-1}(1+R_{t-j}) \\end{eqnarray*}\\] De la definición, se tiene que la rentabilidad bruta simple de periodo \\(k\\) es el producto de las \\(k\\) rentabilidades brutas simples de un periodo. Esto se llama rendimiento compuesto. El rendimiento neto simple de periodo \\(k\\) es \\(R_t[k]=(P_t-P_{t-k})/P_{t-k}\\). En la práctica, el intervalo de tiempo real es importante para discutir y comparar los rendimientos (por ejemplo, rendimiento mensual o rendimiento anual). Si no se indica el intervalo de tiempo, se asume implícitamente que es de un año. Si el activo se mentuvo durante \\(k\\) años, entonces el rendimiento anualizado se define como \\[\\text{Anualizado}\\{R_t[k]\\} = \\left[\\prod_{j=0}^{k-1}(1+R_{t-j})\\right]^{1/k}-1.\\] Esta es una media geométrica de los \\(k\\) rendimientos brutos simple de un periodo y lo podemos calcular por \\[\\text{Anualizado}\\{R_t[k]\\} = \\exp\\left[\\frac{1}{k}\\sum_{j=0}^{k-1}\\ln(1+R_{t-j})\\right]-1.\\] Debido a que es más fácil calcular el promedio aritmético que la media geométrica y los rendimientos de un periodo tienden a ser pequeños, podemos utilizar el desarrollo de Taylor de primer orden para aproximar el rendimiento anualizado y obtener \\[\\begin{equation} \\text{Anualizado}\\{R_t[k]\\} \\approx \\frac{1}{k}\\sum_{j=0}^{k-1}R_{t-j} \\tag{1.3} \\end{equation}\\] Sin embargo, la exactitud de la aproximación en la ecuación (1.3) puede no ser suficiente en algunas aplicaciones. Otra definición útil es la de rendimiento compuesto continuo, pero antes de discutir tales definiciones, discutamos el efecto de la capitalización. Supongamos que la tasa de interés de un depósito bancario es del 10% anual y el depósito inicial es de \\(\\$1,00\\). Si el banco paga intereses una vez al año, entonces el valor neto del depósito se convierte en \\(\\$1,00(1+0.10)=\\$1,10\\) un año después. Si el banco paga intereses semestralmente, el tipo de interés a 6 meses es \\(10\\%/2=5\\%\\) y el valor del depósito será \\(\\$1,00(1+0.10/2)2=\\$1,1025\\) después del primer año. En general, si el banco para intereses \\(m\\) veces al año, entonces la tasa de interés para cada pago es \\(10\\%/m\\) y el valor neto del depósito se convierte en \\(\\$1.00(1+0.1/m)m\\) un año después. La tabla siguiente da los resultados para algunos intervalos de tiempo comúnmente usados. En particular, el valor neto se aproxima a \\(\\$1,1052\\), que se obtiene con \\(\\exp(0.1)\\) y se refiere al resultado de la capitalización continua. Tipo Número de pagos Tasa de interés por periodo Valor neto Anual 1 0.1 $1.10000 Semestral 2 0.05 $1.10250 Trimestral 4 0.025 $1.10831 Mensual 12 0.0083 $1.10471 Semanal 52 0.1/52 $1.10506 Diario 365 0.1/365 $1.10516 Continuo Inf. $1.10517 Tabla. Ilustración de los efectos de la combinación. El intervalo de tiempo es de 1 año y la tasa de interés es del 10% anual. En general, el valor liquidativo \\(A\\) de la capitalización continua es \\[\\begin{equation} A=C\\exp(r\\times n) \\tag{1.4} \\end{equation}\\] donde \\(r\\) es el tipo de interés anual, \\(C\\) es el capital inicial y \\(n\\) es el número de años. A partir de la ecuación (1.4), tenemos \\[\\begin{equation} C=A\\exp(-r\\times n) \\tag{1.5} \\end{equation}\\] el cual se refiere como el valor presente de un activo que vale \\(A\\) dolares \\(n\\) años a partir de ahora, asumiendo que la tasa de interés compuesta continua es \\(r\\) por año. Definición 1.3 (Rendimiento compuesto continuo) El logaritmo natural de rendimiento bruto simple de un activo se denomina rendimiento compuesto continuo o rendimiento logarítmico \\[\\begin{equation} r_t=\\ln(1+R_t) = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right) = p_t-o_{t-1} \\tag{1.6} \\end{equation}\\] donde \\(p_t=\\ln(P_t)\\). Los rendimientos compuestos continuos deisfrutan de algunas ventajas sobre los rendimientos netos simples \\(R_t\\). En primer lugar, consideremos los rendimientos multiperiodos. Tenemos \\[\\begin{eqnarray*} r_t[k] &amp;=&amp; \\ln(1+R_t[k]) = \\ln[(1+R_t)(1+R_{t-1})\\cdots(1+R_{t-k+1})]\\\\ &amp;=&amp; \\ln(1+R_t)+\\ln(1+R_{t_1})+\\cdots+\\ln(1+R_{t-k+1})\\\\ &amp;=&amp; r_t+r_{t-1}+\\cdots+r_{t-k+1}. \\end{eqnarray*}\\] Por lo tanto, el rendimiento multiperiodo compuesto continuo es simplemente la suma de los rendimientos compuesto continuo de un periodo involucrados. En segundo lugar, las propiedades estadísticas de los logaritmos de los rendimientos son más manejables. Definición 1.4 (Rentabilidad de la cartera) El rendimiento neto simple de una cartera de inversión compuesta por \\(N\\) activos es una media ponderada de los rendimientos netos simples de los activos en cuestión, en la que la ponderación de cada activo es el porcentaje del valor de la cartera invertido en ese activo. Sea \\(p\\) un portafolio que ponga peso con el activo \\(i\\), entonces el rendimineto simple \\(p\\) en el tiempo \\(t\\) es \\[R_{p,t} = \\sum_{i=1}^N\\omega_iR_{it}\\] donde \\(R_{it}\\) es el rendimiento simple del activo \\(i\\). Los rendimientos compuestos continuos de una cartera, sin embargo, no tienen la propiedad conveniente anterior. Si los rendimientos simples \\(R_t\\) son todos pequeños en magnitud, entonces tenemos \\[r_{p,t} \\approx \\sum_{i=1}^N\\omega_ir_{it}\\] donde \\(r_{p,t}\\) es el rendimiento compuesto continuo de la cartera en el momento \\(t\\). Esta aproximación se utiliza a menudo para estudiar los rendimientos de las carteras. Definición 1.5 (Pago de dividendos) Si un activo paga dividendos periódicamente, debemos modificar las definiciones de rendimientos de activos. Sea \\(D_t\\) el pago de dividendos de un activo entre los tiempos \\(t-1\\) y \\(t\\) y sea \\(P_t\\) el precio del activo al final del periodo \\(t\\). Entonces el dividendo no se incluye en \\(P_t\\). Entonces el rendimiento neto simple y el rendimiento compuesto continuo en el tiempo \\(t\\) están dados por \\[\\begin{eqnarray*} R_t &amp;=&amp; \\frac{P_t+D_t}{P_{t-1}}-1 \\\\ r_t &amp;=&amp; \\ln(P_t+D_t) - \\ln(P_{t-1}) \\end{eqnarray*}\\] Definición 1.6 (Exceso de rendimiento) El rendimiento excesivo de un activo en el momento \\(t\\) es la diferencia entre el rendimiento del activo y el rendimiento de algún activo de referencia. A menudo se considera que el activo de referencia no tiene riesgo, como una devolución de letras del Tesoro de EE.UU. a corto plazo. El exceso de rentabilidad simple y el logaritmo de exceso de rentabilidad de un activo se definen como \\[\\begin{equation} Z_t = R_t-R_{0t}; \\quad z_t=r_t-r_{0t} \\tag{1.7} \\end{equation}\\] donde \\(R_{0t}\\) y \\(r_{0t}\\) son los rendimientos simples y logarítmicos del activo de referencia, respectivamente. En la literatura financiera, el exceso de rentabilidad se considera como el pago de una cartera de arbitraje que va larga en un activo y corta en el activo de referencia sin inversión inicial neta. ** Resumen de la relación** Las relaciones entre el rendimiento simple \\(R_t\\) y el rendimiento compuesto continuo (o logarítmico) \\(r_t\\) son \\[r_t=\\ln(1+R_t), \\qquad R_t=e^{r_t}-1\\] La agregación temporal de los rendimientos produce \\[\\begin{eqnarray*} 1+R_t[k] &amp;=&amp; (1+R_t)(1+R_{t-1})\\cdots(1+R_{t-k+1}) \\\\ r_t[k] &amp;=&amp; r_t+r_{t-1}+\\cdots+r_{t-k+1} \\end{eqnarray*}\\] Si el tipo de interés compuesto continuo es por año, entonces la relación entre los valores presentes y futuros de un activo fijo es \\[A = C\\exp(r\\times n),\\qquad C=A\\exp(-r\\times n).\\] 1.2 Conceptos básicos Una serie tiempo es una secuencia de observaciones, medidos en determinados momentos del tiempo, ordenados cronológicamente y, espaciados entre sí de manera uniforme, así los datos usualmente son dependientes entre sí. El principal objetivo de una serie de tiempo es su análisis para hacer pronóstico. Formalmente se tiene la siguiente definición. Definición 1.7 Una serie de tiempo es un conjunto de observaciones \\(x_t\\), cada una registrada a un tiempo específico \\(t\\). Definición 1.8 Un modelo de series de tiempo para los datos observados \\(\\{x_t\\}\\) es una especificación de una distribución conjunta (o posiblemente solo de medias y covarianzas) de una sucesión de variables aleatorias \\(\\{X_t\\}\\) de las cuales \\(\\{x_t\\}\\) es una realización. A continuación presentaremos una serie de ejemplos que demuestran la utilidad y lo cotidiano de las series de tiempo, también se mostrarán los códigos en R para cargar los archivos de datos y graficar las respectivas series de tiempo. 1.3 Ejemplos Ejemplo 1.1 (Beneficios de acciones) Beneficios por acción trimestrales para la compañía Johnson y Johnson. Se tienen 84 trimestres iniciando el primer trimestre de 1960 hasta el último trimestre de 1980. Los métodos para analizar tales datos se verán en el Tema 3 usando técnicas de regresión. El archivo es “jj.txt”. Los comandos en R para cargar el archivo y graficar la serie de tiempo son los siguientes: jj=ts(scan(&quot;data/jj.txt&quot;),start=1960,freq=4) plot(jj, type=&quot;l&quot;,ylab=&quot;Beneficios por acción trimestrales&quot;) Figura 1.1: Beneficios por acción trimestrales para la compañía Johnson y Johnson Ejemplo 1.2 El archivo “ReservasInternacionales.xlsx”, contiene el registro mensual de Reservas Internacionales Venezolanas en millones de dólares ($), iniciando en el mes de enero de 1996 hasta el mes de diciembre de 2017 library(readxl) reservas &lt;- read_excel(&quot;data/ReservasInternacionales.xlsx&quot;) reservas=ts(reservas,start = 1996,frequency = 12) plot.ts(reservas[,2], xlab=&quot;Año&quot;,ylab=&quot;Monto&quot;, main=&quot;Reservas Internacionales de Venezuela (millones $)&quot;) Figura 1.2: Reservas Internacionales de Venezuela (millones $) 1996-2017 Ejemplo 1.3 El archivo “PreciosPetroleoVzla.xlsx” contiene el precio promedio mensual de venta para el petróleo venezolano (en dólares) desde enero 2006 hasta noviembre 2017 library(readxl) petroleo &lt;- read_excel(&quot;data/PreciosPetroleoVzla.xlsx&quot;) petroleo=ts(petroleo,start = 2006,frequency = 12) plot.ts(petroleo[,2], xlab=&quot;Año&quot;,ylab=&quot;Monto&quot;, main=&quot;Precio promedio del petróleo venezolano (en dolares $)&quot;) Figura 1.3: Precio promedio del petróleo venezolano (en dolares $) 2006-2017 Ejemplo 1.4 El archivo “IndiceDowJones.xlsx” contiene los valores histórico del Índice Dow-Jones desde enero de 1930 hasta octubre de 2017. En el archivo podems notar que desde enero de 1930 hasta diciembre de 1994, los registros son el promedio semanal, a partir de enero de 1995, los registros son diarios. La primera columa es la fecha, la segunda columna es el valor de apertura, la tercera columna el valor máximo, la cuarta el valor mínimo, la quinta el último valor del índice o valor de cierre y la sexta columna es el volumen de acciones. DJ=read_excel(&quot;data/IndiceDowJones.xlsx&quot;) DJ=ts(DJ) plot.ts(DJ[,-1], xlab=&quot;Días&quot;, main=&quot;Índice Dow-Jones desde enero 1930 hasta octubre 2017&quot;) Figura 1.4: Índice Dow-Jones desde enero 1930 hasta octubre 2017 Ejemplo 1.5 La figura siguiente muestra los porcentajes de cambio diario de la Bolsa de Valores de New York desde el 2 de febrero de 1984 hasta el 31 de diciembre de 1991. Como se ve hay una caída fuerte, esta ocurrió el 19 de octubre de 1987 en \\(t=938\\). El archivo de datos es “nyse.txt”. NYSE=ts(scan(&quot;data/nyse.txt&quot;)) plot(NYSE,xlab=&quot;Tiempo&quot;,ylab=&quot;Porcentaje de cambio, NYSE&quot;) Figura 1.5: Porcentaje de cambio de la bolsa de New York Ejemplo 1.6 La evolución del EURIBOR es algo que fluctúa a diario. Se entiende por EURIBOR (Euro Interbank Offered Rate) el tipo de interés, promovido por el Instituto Europeo de Mercados Monetarios (EMMI), consistente en la media aritmética simple de los valores diarios con días de mercado para operaciones de depósitos en euros a plazo de uno/tres/seis/doce meses y referido al día quince del mes anterior al comienzo de cada período de interés o al día siguiente hábil si aquel no lo fuese, calculado a partir del ofertado por una muestra de Bancos para operaciones entre entidades de similar calificación. A continuación mostramos dos series del EURIBOR. La primera es la evolución histórica anual del EURIBOR desde su implantación en 1999 hasta 2018, los datos se corresponden al mes de enero de cada año. La segunda es la evolución mensual desde enero de 2007 hasta marzo de 2018. EURIBORa&lt;-read_excel(&quot;data/EURIBOR-anual.xlsx&quot;) plot(EURIBORa$Año,EURIBORa$Índice,type=&quot;l&quot;, col = &quot;blue&quot;, xlab = &quot;Periodo&quot;, main=&quot;Serie EURIBOR anual (1999-2018)&quot;) grid(col = &quot;gray&quot;) Figura 1.6: Evolución anual del EURIBOR (1999-2018) EURIBORm&lt;-read_excel(&quot;data/EURIBOR-mensual.xlsx&quot;) EURIts&lt;-ts(EURIBORm[,2],start = 2007, frequency = 12) plot.ts(EURIts,xlab = &quot;Periodo&quot;, col = &quot;blue&quot;, main=&quot;Serie EURIBOR mensual (enero 2007- marzo 2018)&quot;) grid(col = &quot;gray&quot;) Figura 1.7: Evolución mensual del EURIBOR (2007-2018) Ejemplo 1.7 El archivo Cambio-EUR-USD.xlsx contiene el histórico de la cotización dolar estadounidense versus el euro desde el 01/05/2017 hasta el 26/04/2018. En la primera columna se muestra la fecha, la segunda columna el precio de apertura, la tercera el precio de cierre, la cuarta la diferencia en %, la quinta el precio máximo del día, la sexta el precio mínimo y la utlima el volumen de transacciones. A continuación presentamos los gráficos de apertura, cierre, máximo y mínimo. Cambio&lt;-read_excel(&quot;data/Cambio-EUR-USD.xlsx&quot;) par(mfrow=c(3,2)) plot(Cambio$Fecha,Cambio$Apertura, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Cotizacion&quot;, main = &quot;Apertura&quot;) plot(Cambio$Fecha,Cambio$Cierre, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Cotizacion&quot;, main = &quot;Cierre&quot;) plot(Cambio$Fecha,Cambio$Máximo, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Cotizacion&quot;, main = &quot;Máximo&quot;) plot(Cambio$Fecha,Cambio$Mínimo, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Cotizacion&quot;, main = &quot;Mínimo&quot;) plot(Cambio$Fecha,Cambio$`Dif.%`, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Porcentaje&quot;, main = &quot;Diferencia (apetura-cierre) %&quot;) plot(Cambio$Fecha,Cambio$Volumen, col=&quot;blue&quot;, type = &quot;l&quot;, xlab = &quot;Periodo&quot;, ylab = &quot;Monto&quot;, main = &quot;Volumen&quot;) Figura 1.8: Histórico de cambio del USD vs. EUR (01/05/2017 al 26/04/2018) 1.3.1 Clasificación de las series de tiempo Como se ha mostrado en los ejemplos anteriores, hay una amplia variedad de series de tiempo que pueden clasificarse en varias categorías desde varios puntos de vista. Series de tiempo continuas y discretas. Los datos registrados continuamente, por ejemplo, por un dispositivo analógico, se denominan series de tiempo continuas. Por otra parte, los datos observados en ciertos intervalos de tiempo, como la presión atmosférica medida cada hora, se denominan series de tiempo discretas. Existen dos tipos de series de tiempo discretas: una en la que las observaciones de los datos se realizan a intervalos de igual espaciamiento y otra en la que las observaciones de los datos se realizan a intervalos de espaciamiento desigual. Aunque las series de tiempo mostradas en los ejemplos anteriores están conectadas continuamente por líneas sólidas, todas ellas son series de tiempo discretas. A partir de ahora en este libro, consideramos sólo series de tiempo discretas registradas a intervalos igualmente espaciados, porque las series de tiempo que analizamos en ordenadores digitales son generalmente series de tiempo discretas. Series de tiempo univariadas y multivariadas. Las series de tiempo que consisten en una sola observación en cada punto temporal, como se muestran en los ejemplos 1.1, 1.2, 1.3 y 1.5, se denominan series de tiempo univariadas. Por otra parte, las series de tiempo que se obtienen grabando simultáneamente dos o más fenómenos como los ilustrados en el ejemplo 1.4 se denominan series de tiempo multivariadas. Sin embargo, puede ser difícil distinguir entre series de tiempo univariadas y multivariadas desde su naturaleza; más bien, la distinción se hace desde el punto de vista del analista y por varios otros factores, como la restricción de la medición y los conocimientos empíricos o teóricos sobre el tema. Desde el punto de vista del modelado estadístico, la selección de variables en sí misma es un problema importante en el análisis de series de tiempo. Series de tiempo estacionarias y no estacionarias. Una serie de tiempo es un registro de un fenómeno que varía irregularmente con el tiempo. En el análisis de series de tiempo, las series de tiempo de variación irregular se expresan generalmente mediante modelos estocásticos. En algunos casos, un fenómeno aleatorio puede ser considerado como la realización de un modelo estocástico con una estructura de variación temporal. Estas series de tiempo se denominan series de tiempo estacinarias. El ejemplo 1.5 es un ejemplo típico de una serie de tiempo estacionaria. Por otra parte, si la estructura estoc?stica de una serie de tiempo cambia con el tiempo, se denomina serie de tiempo no estacionaria. Como ejemplos típicos de series de tiempo no estacionarias, considere la serie en los ejemplos 1.1 a 1.4 . Se puede observar que los valores medios cambian a lo largo del tiempo. Series de tiempo gaussianas y no gaussianas. Cuando una distribución de una serie de tiempo sigue una distribución normal, la serie de tiempo se denomina serie de tiempo gaussiana; de lo contrario, se denomina serie de tiempo no gausiana. La mayoría de los modelos considerados en este libro son modelos gaussianos, asumiendo que las series de tiempo siguen distribuciones gaussianas. Al igual que en el caso del ejemplo 1.3, el patrón de las series de tiempo es a veces asimétrico, de modo que la distribución marginal no puede considerarse gaussiana. Incluso en tal situación, podemos obtener una serie de tiempo gaussiana aproximada mediante una transformación de datos apropiada. Series de tiempo lineales y no lineales. Una serie de tiempo expresable como la salida de un modelo lineal se denomina serie de tiempo lineal. Por el contrario, la salida de un modelo no lineal se denomina serie de tiempo no lineal. Datos faltantes y valores atípicos. En el modelado de series de tiempo de problemas del mundo real, a veces necesitamos tratar con observaciones faltante y valores atípicos. Algunos valores de las series de tiempo que no se han registrado por algunas razones se denominan observaciones que faltan en las series de tiempo. Los valores atípicos (observaciones exteriores) pueden ocurrir debido al comportamiento extraordinario del objeto, mal funcionamiento del dispositivo de observación o errores en el registro. En los datos de los ejemplos 1.4 y 1.5 se pueden observar datos atípicos. En el ejemplo 1.4 podemos notar caídas en los índices del DowJones y en el ejemplo 1.4 podemos notar una fuerte caída en el porcentaje de cambio de diario ocurrido el 19 de octubre de 1987. 1.4 Componentes de una serie de tiempo El análisis clásico de las series de tiempo se basa en la suposición de que los valores que toma la variable de observación es la consecuencia de tres componentes, cuya actuación conjunta da como resultado los valores medidos, estos componentes son: Componente de tendencia. Se puede definir como un cambio a largo plazo que se produce en la relación al nivel medio, o el cambio a largo plazo de la media. La tendencia se identifica con un movimiento suave de la serie a largo plazo. Componente estacional. Muchas series de tiempo presentan cierta periodicidad o dicho de otro modo, variación de cierto período (semestral, mensual, etc.). Por ejemplo las Ventas al Detalle en Puerto Rico aumentan por los meses de noviembre y diciembre por las festividades navideñas. Estos efectos son fáciles de entender y se pueden medir explícitamente o incluso se pueden eliminar de la serie de datos, a este proceso se le llama desestacionalización de la serie. Componente aleatoria. Esta componente no responde a ningún patrón de comportamiento, sino que es el resultado de factores fortuitos o aleatorios que inciden de forma aislada en una serie de tiempo. De los tres componentes anteriores los dos primeros son componentes determinísticos, mientras que la última es aleatoria. Los modelos que se utilizan con más frecuencia son: Modelo aditivo: \\(X_t=T_t+E_t+\\epsilon_t\\) Modelos multiplicativos: Puro: \\(X_t = T_t\\times E_t\\times\\epsilon_t\\) Mixto: \\(X_t = T_t\\times E_t+\\epsilon_t\\) La elección de uno de estos modelos se hará de manera que el modelo seleccionado sea capaz de agrupar las principales características observadas en el gráfico de la serie en estudio. 1.4.1 El Modelo Aditivo de Componentes de Series de Tiempo Dada una serie \\(X_t, t=1,\\ldots,n\\), el Modelo Aditivo de Componentes consiste en asumir que \\(X_t\\) se puede descomponer en tres componentes: \\[\\begin{equation} X_t = T_t+E_t+\\epsilon_t \\tag{1.8} \\end{equation}\\] donde \\(T_t\\) es la componente de tendencia, \\(E_t\\) es la componente estacional y \\(\\epsilon_t\\) es la componente aleatoria o de errores. Las componentes \\(T_t\\) y \\(E_t\\) son funciones de \\(t\\) determinísticas. Su evolución es perfectamente predecible. Este modelo es apropiado cuando la magnitud de la fluctuaciones estacionales de la serie no varía al hacerlo la tendencia. La componente \\(T_t\\) en algunos casos también puede ser una componente estacional, pero de baja frecuencia, o, equivalentemente, una componente con período muy grande. Por ejemplo, en una serie diaria, \\(E_t\\) puede tener período 30 días, y \\(T_t\\) período 360 días. En la Figura se muestra la idea de la descomposición. Al superponer las series en los gráficos (a), (b) y (c) se obtiene la serie en el gráfico (d). Figura 1.9: Modelo aditivo de series de tiempo Asumiendo el modelo aditivo, el análisis de series de tiempo consiste en modelar y estimar \\(T_t\\) y \\(E_t\\) y luego extraerlas de \\(X_t\\) para obtener \\(\\hat{\\epsilon}_t = X_t - \\hat{T}_t - \\hat{E}_t\\). La serie \\(\\hat{\\epsilon}_t\\) se modela y estima para finalmente reconstruir \\(X_t\\), \\(\\hat{X}_t = \\hat{T}_t+\\hat{E}_t+\\hat{\\epsilon}_t\\), y poder realizar el pronóstico \\(\\hat{X}_{t+h}=\\hat{T}_{t+h}+\\hat{E}_{t+h}+\\hat{\\epsilon}_{t+h}\\), utilizando la información disponible \\(X_t,\\ldots,X_n\\) con \\(h=1,2,\\ldots,m\\). Sin embargo, puede suceder que la serie \\(\\hat{\\epsilon}_t\\) sea incorrelacionada, es decir, \\(Corr(\\hat{\\epsilon}_t,\\hat{\\epsilon}_{t+s}) = 0\\), para \\(s\\neq0\\). En este caso \\(\\hat{\\epsilon}_{t+h}=0\\) para todo \\(h&gt;0\\). En R podemos descomponer una serie de tiempo usando la función stl() o la función decompose(). Retomando la serie de beneficios trimestrales de las acciones de Johnson y Johnson (Ejemplo 1.1) podemos observar la descomposición de la misma. En la parte superior de la gráfica se observa la serie original, en el gráfico siguiente la estacionalidad, en el tercero la tendencia y en el gráfico inferior los residuales. plot(decompose(jj, type = &quot;additive&quot;, filter = NULL)) Figura 1.10: Descomposición aditiva de la serie Johnson y Johnson La función stl() es más sofisticada que decompose(), la misma usa la descomposición de estacionalidad y tendencia de Loess (Seasonal and Trend decomposition using Loess) el cual es un método robusto y versátil para la descomposición de series de tiempo. El método STL fue desarrollado por Cleveland et al. (1990). A continuación mostramos la misma serie de beneficios de acciones de Johnson y Johnson usando esta función. plot(stl(jj,s.window=&quot;periodic&quot;), col=&quot;blue&quot;, main=&quot;Descomposicion de la serie Johnson y Johnson&quot;) Figura 1.11: Descomposición de la serie Johnson y Johnson usando la descomposición de Loess (STL) 1.4.2 El Modelo Multiplicativo de Componentes de Series de Tiempo Dada una serie de tiempo \\(X_t,t=1,\\ldots,n\\), el Modelo Multiplicativo de Componentes consiste en asumir que \\(X_t\\) se puede descomponer de una de las siguientes maneras: Puro: \\[\\begin{equation} X_t = T_t\\times E_t\\times\\epsilon_t \\tag{1.9} \\end{equation}\\] Mixto: \\[\\begin{equation} X_t = T_t\\times E_t+\\epsilon_t \\tag{1.10} \\end{equation}\\] donde \\(T_t\\) es la componente de tendencia, \\(E_t\\) es la componente estacional y \\(\\epsilon_t\\) es la componente aleatoria o de errores. Estos modelos son apropiados cuando la magnitud de las fluctuaciones estacionales de la serie crece y decrece proporcionalmente con los crecimientos y decrecimientos de la tendencia respectivamente. Usamos la misma función decompose() para realizar la descomposición multiplicativa de la serie de tiempo, para ello en ‘type’ cambiamos “additive” por “multiplicative” plot(decompose(jj, type = &quot;multiplicative&quot;, filter = NULL)) Figura 1.12: Descomposición multiplicativa de la serie Johnson y Johnson "],
["características-de-series-de-tiempo.html", "Capítulo 2 Características de series de tiempo 2.1 Medidas de dependencia para series de tiempo 2.2 Estimación de la Tendencia 2.3 Estimación de la tendencia por regresión clásica", " Capítulo 2 Características de series de tiempo El objetivo primario en el análisis de Series de Tiempo es desarrollar modelos matemáticos que provean una descripción apropiada para los datos muestrales, como los vistos en los ejemplos del capítulo anterior. Así, lo primero que hacemos es utilizar la definición 1.7, para tener un soporte estadístico. En este capítulo daremos algunas definiciones que serán de uso general en todo el resto del libro, también se describiran algunos métodos para el análisis exploratorio de las series de tiempo 2.1 Medidas de dependencia para series de tiempo Definición 2.1 Un proceso estocástico es una familia de variables aleatorias indexadas \\(x(\\omega,t)\\) ó \\(x_t(\\omega)\\) donde \\(t\\) pertenece a un conjunto de índices \\(T\\) y \\(\\omega\\) pertenece a un espacio muestral \\(\\Omega\\). Si \\(t=t^*\\) fijo, \\(x(\\omega,t^*)\\) es una variable aleatoria. Si \\(\\omega=\\omega^*\\) fijo, \\(x(\\omega^*,t)\\) es una función de \\(t\\), y se llama una realización del proceso. Una serie de tiempo es la realización de un proceso estocástico. Una descripción completa de una serie de tiempo, observada como una colección de \\(n\\) variables aleatorias en puntos de tiempo enteros arbitrarios \\(t_1,t_2,\\ldots,t_n\\), para cada entero positivo \\(n\\), es proporcionada por la función de distribución conjunta, evaluada como la probabilidad de que los valores de la serie sean conjuntamente menor que \\(n\\) constantes \\(c_1,c_2,\\ldots,c_n\\), esto es \\[\\begin{equation} F(c_1,c_2,\\ldots,c_n)=P(x_{t_1}\\leq c_1,x_{t_2}\\leq c_2,\\ldots,x_{t_n}\\leq c_n). \\tag{2.1} \\end{equation}\\] Desafortunadamente, la función de distribución multidimensional usualmente no se puede escribir fácilmente a menos que las variables aleatorias tengan distribución normal conjunta, en cuyo caso, la ecuación (2.1) llega a ser la distribución normal multivariada usual. Un caso particular en la cual la función de distribución multidimensional es fácil de escribir, será en el caso de variables aleatorias normal estándar independientes e idénticamente distribuidas, para lo cual la función de distribución se puede expresar como el producto de las distribuciones marginales, es decir, \\[\\begin{equation} F(c_1,c_2,\\ldots,c_n)=\\prod_{t_1}^{n}\\Phi(c_t) \\tag{2.2} \\end{equation}\\] donde \\[\\begin{equation} \\Phi(x)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x}\\mathbb{E}xp\\left\\{-\\frac{z^2}{2}\\right\\}dz\\tag{2.3} \\end{equation}\\] es la función de distribución normal estándar acumulada. Aunque la función de distribución multidimensional describa los datos completamente, esto es un instrumento poco manejable para mostrar y analizar datos de series de tiempo. La función de distribución (2.1) debe ser evaluada como una función de \\(n\\) argumentos, entonces cualquier graficación de las correspondientes funciones de densidad multivariante es prácticamente imposible. La función de distribución unidimensional \\[F_t(x)=P\\{x_t\\leq x\\}\\] o la correspondiente función de densidad unidimensional \\[f_t(x)=\\frac{\\partial F_t(x)}{\\partial x},\\] cuando existen, a menudo son más útiles para determinar si una coordenada en particular de la serie de tiempo tiene una función de densidad conocida, como la distribución normal (gaussiana), por ejemplo. Definición 2.2 La función de media es definida como \\[\\begin{equation} \\mu_{xt}=\\mathbb{E}(x_t)=\\int_{-\\infty}^{\\infty}xf_t(x)dx, \\tag{2.4} \\end{equation}\\] en caso de que exista, donde \\(\\mathbb{E}\\) denota el operador usual de esperanza. Cuando no haya confusión sobre a que serie de tiempo nos referimos, escribiremos \\(\\mu_{xt}\\) como \\(\\mu_t\\). Lo importante de comprender sobre \\(\\mu_t\\) consiste en que es una media teórica para la serie de tiempo en un punto particular, donde la media se asume o calcula sobre todos los posibles eventos que podrían haber producido \\(x_t\\). Definición 2.3 La función de autocovarianza es definida como producto del segundo momento \\[\\begin{equation} \\gamma_x(s,t)=\\mathbb{E}[(x_s-\\mu_s)(x_t-\\mu_t)], \\tag{2.5} \\end{equation}\\] para todo \\(t\\) y \\(s\\). cuando no haya confusión en la existencia sobre a que serie nos referimos, escribiremos \\(\\gamma_x(s,t)=\\gamma(s,t)\\). Note que \\(\\gamma_x(s,t)=\\gamma_x(t,s)\\) para todo los puntos \\(s\\) y \\(t\\). La función de autocovarianza mide la dependencia lineal entre dos puntos de la misma serie en diferentes tiempos. La autocovarianza (2.5) es el promedio de los productos cruzados relacionado con la densidad conjunta \\(F(x_s,x_t)\\). Es claro que, para \\(s=t\\), la autocovarianza se reduce a la varianza (en el caso finito), dado que \\[\\begin{equation} \\gamma_x(t,t)=\\mathbb{E}[(x_t-\\mu_t)^2] \\tag{2.6} \\end{equation}\\] Otro función de medida de tendencia importante es la función de autocorrelación. Definición 2.4 La función de autocorrelación (ACF) (ACF, siglas en ingles: Autocorrelation Function) se define como \\[\\begin{equation} \\rho(s,t)=\\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s)\\gamma(t,t)}} \\tag{2.7} \\end{equation}\\] La \\(ACF\\) mide la predictibilidad lineal de una serie de tiempo en tiempo \\(t\\), digamos \\(x_t\\) usando solo el valor \\(x_s\\). Es fácil de demostrar que \\(-1\\leq\\rho(s,t)\\leq1\\) usando la desigualdad de Cauchy-Schwarz 1 Si podemos predecir \\(x_t\\) exactamente de \\(x_s\\) a través de la relación lineal \\(x_t=\\beta_0+\\beta_1x_s\\) entonces la correlación será 1 cuando \\(\\beta_1&gt;0\\) y \\(-1\\) cuando \\(\\beta_1&lt;0\\). Definición 2.5 La función de covarianza cruzada entre dos series \\(x_t\\) y \\(y_t\\) se define como \\[\\begin{equation} \\gamma_{xy}(s,t)=\\mathbb{E}[(x_s-\\mu_{xs})(y_t-\\mu_{yt})] \\tag{2.8} \\end{equation}\\] Definición 2.6 La función de correlación cruzada (CCF) (CCF, siglas en ingles: Cross Correlation Function) es definida como \\[\\begin{equation} \\rho_{xy}(s,t)=\\frac{\\gamma_{xy}(s,t)}{\\sqrt{\\gamma_x(s,s)\\gamma_y(t,t)}} \\tag{2.9} \\end{equation}\\] Las definiciones anteriores de funciones de media y varianza son completamente generales. Aunque nosotros no hayamos hecho ninguna suposición especial sobre el comportamiento de las series de tiempo, muchos de los ejemplos precedentes han insinuado que puede existir una especie de regularidad en el comportamiento de las mismas. Introducimos la noción de regularidad que usa el concepto de estacionaridad, que ya hemos introducido empíricamente en el apartado 1.2.1 “Clasificación de las series de tiempo” Formalmente tenemos las siguientes definiciones de estacionaridad Definición 2.7 Una serie de tiempo estrictamente estacionaria es una serie para la cual el comportamiento probabilístico de cada sucesión de valores \\[\\{x_{t_1},x_{t_2},\\ldots,x_{t_k}\\}\\] es idéntico a la serie trasladada en el tiempo \\[\\{x_{t_1+h},x_{t_2+h},\\ldots,x_{t_k+h}\\}\\] Esto es, \\[\\begin{equation} P[X_{t_1}\\leq c_1,\\ldots,x_{t_k}\\leq c_k] = P[X_{t_1+h}\\leq c_1,\\ldots,x_{t_k+h}\\leq c_k] \\tag{2.10} \\end{equation}\\] para todo \\(k=1,2,\\ldots\\), todo puntos de tiempos \\(t_1,t_2,\\ldots,t_k\\) y números \\(c_1,c_2,\\ldots,c_k\\) y todo salto \\(h=\\pm0,\\pm1,\\pm2,\\ldots\\). Esta definición de estacionaridad es muy fuerte para la mayoría de las aplicaciones prácticas. Por ello necesitamos una versión menos fuerte que imponga menos condiciones sobre las distribuciones de probabilidad, ya que si observamos bien la ecuación (2.10), lo que nos dice la misma es que todas las posibles distribuciones de probabilidad deben ser iguales, lo que como ya indicamos en la práctica es muy difícil de comprobar aún para conjuntos de datos sencillos. La siguiente versión de estacionaridad solo impone condiciones sobre los dos primeros momentos de la serie Definición 2.8 Una serie de tiempo débilmente estacionaria \\(x_t\\), es un proceso de varianza finita tal que la función de media \\(\\mu_t\\) es constante y no depende del tiempo \\(t\\), la función de covarianza \\(\\gamma(t,s)\\) depende solo de las diferencias de \\(s\\) y \\(t\\), \\(|t-s|\\). Por consiguiente, usaremos el término estacionaridad para referirnos a estacionaridad débil; si un proceso es estacionario en el sentido estricto usaremos el término estrictamente estacionario. Nota. 1) Si una serie de tiempo es estrictamente estacionaria, entonces todos las funciones de distribución multivariadas para subconjuntos de variables deben coincidir con sus contrapartes en el conjunto trasladado, para todos los valores del parámetro \\(h\\). Por ejemplo para \\(k=1\\) La ecuación (2.10) implica que \\[\\begin{equation} P\\{x_s\\leq c\\}=P\\{x_t\\leq c\\} \\tag{2.11} \\end{equation}\\] para cada puntos \\(s\\) y \\(t\\). Esta declaración implica, por ejemplo, que si la probabilidad de un valor de una serie de tiempo muestreada cada hora es negativa a la 1:00a.m, la probabilidad a la 10:00a.m. es la misma. Además, si la función de media, \\(\\mu_t\\) de la serie \\(x_t\\) existe, (2.11) implica que \\(\\mu_s=\\mu_t\\) para todo \\(s\\) y \\(t\\), y por consiguiente \\(\\mu_t\\) debe ser constante. Cuando \\(k=2\\), podemos escribir la ecuación (2.10) como \\[\\begin{equation} P\\{x_s\\leq c_1,x_t\\leq c_2\\}=P\\{x_{s+h}\\leq c_1,x_{t+h}\\leq c_2\\} \\tag{2.12} \\end{equation}\\] para cada par de puntos \\(s\\) y \\(t\\) y salto \\(h\\). Entonces, si la función de varianza del proceso existe, (2.12) implica que la función de autocovarianza de la serie \\(x_t\\) satisface \\(\\gamma(s,t)=\\gamma(s+h,t+h)\\) para todos \\(s\\) y \\(t\\) y salto \\(h\\). Podemos interpretar este resultado diciendo que la función de autocovarianza del proceso depende sólo de las diferencias de tiempo entre \\(s\\) y \\(t\\), y no del tiempo actual. Es claro de la definición 2.7 de serie estrictamente estacionaria, que una serie de tiempo estrictamente estacionaria con varianza finita, también es una serie estacionaria. El recíproco no es cierto a menos que impongamos condicionaes adicionales. Un importante caso donde estacionaridad implica estricta estacionaridad es el caso de series de tiempo gaussianas. Ya que la función de media \\(\\mathbb{E}(x_t)=\\mu_t\\) de una serie de tiempo estacionaria es independiente del tiempo \\(t\\), escribimos \\[\\begin{equation} \\mu_t=\\mu \\tag{2.13} \\end{equation}\\] Debido a que la función de covarianza de una serie de tiempo estacionaria, \\(\\gamma(s,t)\\) en tiempos \\(s\\) y \\(t\\) depende sólo de la diferencia \\(|s-t|\\), podemos simplificar la notación. Sea \\(s=t+h\\), donde \\(h\\) representa el tiempo de traslación o salto, entonces \\[\\begin{eqnarray} \\gamma(s,t)&amp;=&amp;\\mathbb{E}[(x_{t+h}-\\mu)(x_t-\\mu)]\\\\ \\nonumber &amp;=&amp;\\mathbb{E}[(x_h-\\mu)(x_0-\\mu)]\\\\ &amp;=&amp;\\gamma(h,0) \\nonumber \\tag{2.14} \\end{eqnarray}\\] no depende del argumento de tiempo \\(t\\); asumiendo que \\(\\text{Var}(x_t)=\\gamma(0,0)&lt;\\infty\\). De ahora en adelante, por conveniencia, prescindiremos del segundo argumento de \\(\\gamma(h,0)\\), es decir, la función de covarianza se denotará \\(\\gamma(h)\\). Definición 2.9 La función de autocovarianza de una serie de tiempo estacionaria se escribirá como \\[\\begin{equation} \\gamma(h)=\\mathbb{E}[(x_{t+h}-\\mu)(x_t-\\mu)] \\tag{2.15} \\end{equation}\\] Definición 2.10 La función de autocorrelación (ACF) de una serie de tiempo estacionaria será escrita, usando (2.7) como \\[\\begin{equation} \\rho(h)=\\frac{\\gamma(t+h,t)}{\\sqrt{\\gamma(t+h,t+h)\\gamma(t,t)}}=\\frac{\\gamma(h)}{\\gamma(0)} \\tag{2.16} \\end{equation}\\] La desigualdad de Cauchy-Schwartz muestra nuevamente que \\(-1\\leq\\rho(h)\\leq1\\) para todo \\(h\\). ** Propiedades de la función de covarianza** Para el valor en \\(h=0\\), la función de autocovarianza \\[\\begin{equation} \\gamma(0)=\\mathbb{E}[(x_t-\\mu)^2] \\tag{2.17} \\end{equation}\\] es la varianza de la serie de tiempo; note que la desigualdad de Cauchy-Schwartz implica que \\(|\\gamma(h)|\\leq\\gamma(0)\\). La autocovarianza de una serie estacionaria es simétrica respecto al origen, esto es \\[\\begin{equation} \\gamma(h)=\\gamma(-h) \\tag{2.18} \\end{equation}\\] para todo \\(h\\). Esta propiedad se debe a que trasladar la serie por \\(h\\) significa que \\[\\begin{eqnarray*} \\gamma(h)&amp;=&amp;\\gamma(t+h-t)\\\\ &amp;=&amp;\\mathbb{E}[(x_{t+h}-\\mu)(x_t-\\mu)]\\\\ &amp;=&amp;\\mathbb{E}[(x_t-\\mu)(x_{t+h}-\\mu)]\\\\ &amp;=&amp;\\gamma(t-(t+h))\\\\ &amp;=&amp;\\gamma(-h) \\end{eqnarray*}\\] lo cual muestra como usar la notación para demostrar el resultado. Definición 2.11 Dos series de tiempo \\(x_t\\) y \\(x_s\\) se dice que son conjuntamente estacionarias si cada una de ellas es estacionaria y la función de correlación cruzada \\[\\begin{equation} \\gamma_{xy}(h)=\\mathbb{E}[(x_{t+h}-\\mu_x)(y_t-\\mu_y)] \\tag{2.19} \\end{equation}\\] es una función sólo del salto \\(h\\). Definición 2.12 La función de correlación cruzada (CCF) de dos series conjuntamente estacionarias \\(x_t\\) y \\(y_t\\) se define como \\[\\begin{equation} \\rho_{xy}(h)=\\frac{\\gamma_{xy}(h)}{\\sqrt{\\gamma_x(0)\\gamma_y(0)}} \\tag{2.20} \\end{equation}\\] De nuevo, tenemos el resultado \\(-1\\leq\\rho_{xy}(h)\\leq1\\) lo cual nos permite comparar los valores extremos -1 y 1 cuando vemos la relación entre \\(x_{t+h}\\) y \\(y_t\\). La función de correlación cruzada satisface \\[\\begin{equation} \\rho_{xy}(h)=\\rho_{yx}(-h) \\tag{2.21} \\end{equation}\\] lo cual se puede demostrar de manera similar que para (2.18). Ejemplo 2.1 (Estacionaridad conjunta) Considere las series \\(x_t\\) y \\(y_t\\) formadas por las sumas y diferencias de dos valores sucesivos de un ruido blanco respectivamente, esto es \\[x_t=w_t+w_{t-1}\\] y \\[y_t=w_t-w_{t-1}\\] donde \\(w_t\\) son variables aleatorias independientes con media cero y varianza \\(\\sigma_w^2\\). Es fácil demostrar que \\(\\gamma_x(0)=\\gamma_y(0)=2\\sigma_w^2\\) y \\(\\gamma_x(1)=\\gamma_x(-1)=\\sigma_w^2\\), \\(\\gamma_y(1)=\\gamma_y(-1)=-\\sigma_w^2\\). También \\[\\begin{eqnarray*} \\gamma_{xy}(1)&amp;=&amp;\\mathbb{E}[(x_{t+1}-0)(y_t-0)]\\\\ &amp;=&amp;\\mathbb{E}[(w_{t+1}+w_t)(w_t-w_{t-1})]\\\\ &amp;=&amp;\\sigma_w^2 \\end{eqnarray*}\\] porque solo uno de los productos es distinto de cero.\\ Similarmente, \\(\\gamma_{xy}(0)=0,\\gamma_{xy}(-1)=-\\sigma_w^2\\). Usando (), obtenemos \\[\\rho_{xy}(h)=\\begin{cases}0,&amp;h=0\\\\ 1/2,&amp;h=1\\\\ -1/2,&amp;h=-1\\\\ 0,&amp;|h|\\geq2\\end{cases}.\\] Claramente, las funciones de autocovarianza y correlación cruzada dependen solo del salto \\(h\\), por lo tanto las series son conjuntamente estacionarias. El concepto de estacionaridad débil forma la base para muchos de los análisis realizados con series de tiempo. Las propiedades fundamentales de la media (2.13) y la función de covarianza (2.15) son satisfechas por muchos modelos teóricos que aparecen para generar realizaciones muestrales apropiadas. Definición 2.13 Un proceso lineal \\(x_t\\) se define como una combinación lineal de variables aleatorias de ruido blanco \\(w_t\\), y está dado por \\[\\begin{equation} x_t=\\mu+\\sum_{j=-\\infty}^{\\infty}\\psi_jw_{t-j} \\tag{2.22} \\end{equation}\\] donde los coeficientes satisfacen \\[\\begin{equation} \\sum_{j=-\\infty}^{\\infty}|\\psi_j|&lt;\\infty \\tag{2.23} \\end{equation}\\] Para un proceso lineal, podemos demostrar que la función de autocovarianza está dada por \\[\\begin{equation} \\gamma(h)=\\sigma_w^2\\sum_{j=-\\infty}^{\\infty}\\psi_{j+h}\\psi_j \\tag{2.24} \\end{equation}\\] para todo \\(h\\geq0\\); recuerde que \\(\\gamma(-h)=\\gamma(h)\\). Finalmente como mencionamos anteriormente, un caso importante en el cual una serie débilmente estacionaria es también estrictamente estacionaria es la serie normal o gaussiana. Definición 2.14 Un proceso \\(\\{x_t\\}\\), se dice que es un proceso gaussiano si el \\(k\\)-ésimo vector dimensional \\(\\hat{x}=(x_{t_1},x_{t_2},\\ldots,x_{t_k})\\), para cada conjunto de puntos \\(t_1,t_2,\\ldots,t_k\\) y cada entero positivo \\(k\\) tiene distribución normal multivariada. Definiendo \\(k\\times1\\) vector de medias \\(\\hat{\\mu}=(\\mu_{t_1},\\mu_{t_2},\\ldots,\\mu_{t_k})&#39;\\) y la \\(k\\times k\\) matriz de covarianza positiva como \\(\\Gamma=\\{\\gamma(t_i,t_j);i,j=1,\\ldots,k\\}\\), la función de densidad normal multivariada se puede escribir como \\[\\begin{equation} f(\\hat{x})=(2\\pi)^{-k/2}|\\Gamma|^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\hat{x}-\\hat{\\mu})&#39;\\Gamma^{-1}(\\hat{x}-\\hat{\\mu})\\right\\} \\tag{2.25} \\end{equation}\\] donde \\(|\\cdot|\\) denota el determinante. Esta distribución forma la base para resolver problemas que envuelven inferencia estadística para series de tiempo. Si una serie de tiempo gaussiana \\(\\{x_t\\}\\) es débilmente estacionaria, entonces \\(\\mu_t=\\mu\\) y \\(\\gamma(t_i,t_j)=\\gamma(|t_i-t_j|)\\), de modo que el vector \\(\\hat{\\mu}\\) y la matriz \\(\\Gamma\\) son independientes del tiempo. Este hecho implica que todas las distribuciones finitas, (2.25) de la serie \\(\\{x_t\\}\\) dependen sólo del salto de tiempo y no del tiempo actual, y por consiguiente la serie debe ser estrictamente estacionaria. 2.2 Estimación de la Tendencia En esta sección introducimos los métodos para la estimación de la tendencia. En esencia, existen dos métodos para estimar la tendencia y la componente estacional de una serie de tiempo: Método paramétrico: Se basa en Proponer modelos paramétricos para expresar la relación que guardan la tendencia y la componente estacional con el tiempo. Ajustar dichos modelos a la serie de tiempo (por ejemplo, a través del método de mínimos cuadrados). Aislar la tendencia y la componente estacional por medio de los modelos ajustados. Método no paramétrico: Se basa en Asumir “suavidad” en la relación que guardan la tendencia y la componente estacional con el tiempo. Aislar la tendencia y la componente estacional a través de la suavización del gráfico de la serie (aplicando, por ejemplo, filtros de promedios móviles). Hay otros métodos que no consideraremos en este libro, por ejemplo, wavelets. En ocasiones la expresión “suavizar una serie” es equivalente a “extracción de la tendencia de una serie”, y ambas equivalen a la estimación de la tendencia. A continuación presentamos una lista de posibles modelos para la tendencia \\(T_t\\): Lineal \\[\\begin{equation} T_t=\\beta_0+\\beta_1t \\tag{2.26} \\end{equation}\\] Cuadrático \\[\\begin{equation} T_t=\\beta_0+\\beta_1t+\\beta_2t^2 \\tag{2.27} \\end{equation}\\] Cúbico \\[\\begin{equation} T_t=\\beta_0+\\beta_1t+\\beta_2t^2+\\beta_3t^3 \\tag{2.28} \\end{equation}\\] Exponencial \\[\\begin{equation} T_t=\\exp(\\beta_0+\\beta_1t) \\tag{2.29} \\end{equation}\\] Logístico \\[\\begin{equation} T_t=\\frac{\\beta_2}{1+\\beta_1\\exp(-\\beta_0t)} \\tag{2.30} \\end{equation}\\] En la tendencia cuadrática podemos observar: Si \\(\\beta_1,\\beta_2&gt;0\\), \\(T_t\\) es monótona creciente. Si \\(\\beta_1,\\beta_2&lt;0\\), \\(T_t\\) es monótona decreciente. Si \\(\\beta_1&gt;0\\) y \\(\\beta_2&lt;0\\), \\(T_t\\) es cóncava. Si \\(\\beta_1&lt;0\\) y \\(\\beta_2&gt;0\\), \\(T_t\\) es convexa. Otro modelo propuesto para la tendencia es el dado por la siguiente definición. Definición 2.15 El modelo Logarítmico Lineal o Log-Lineal se define como \\[\\begin{equation} \\ln X_t = \\beta_0+\\beta_1t + \\epsilon_t \\tag{2.31} \\end{equation}\\] El modelo anterior corresponde a un modelo con tendencia lineal para el logaritmo de \\(X_t\\). En (2.31) al tomar exponencial se tiene \\(X_t = \\exp(\\beta_0+\\beta_1t + \\epsilon_t)\\), que es similar al modelo con tendencia exponencial (2.29). Sin embargo, son modelos diferentes y se estiman por métodos diferentes. Para la estimación de los parámetros \\(\\beta_0,\\beta_1,\\beta_2\\) en los modelos lineales (2.26), (2.27), (2.28) y (2.31) utilizaremos el método de mínimos cuadrados clásico (MCC). En este método los parámetros estimados son aquellos que producen el valor mínimo de la suma de errores cuadrados. Para los modelos (2.29) y (2.30) se usa el método de mínimos cuadrados no lineales, que también minimiza la suma de errores cuadrados. El modelo Log-Lineal (2.31) es equivalente, algebráicamente, a \\[X_t = \\exp(\\beta_0 + \\beta_1t + \\epsilon_t).\\] Sin embargo, este último modelo es no lineal y no coincide con el modelo exponencial,(2.29), \\(X_t = \\exp(\\beta_0+\\beta_1t)+\\epsilon_t\\). Es posible estimar por mínimos cuadrados ordinarios el modelo Log-Lineal y utilizar los parámetros estimados \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\) como valores iniciales en la estimación del modelo exponencial por mínimos cuadrados no lineales. Pero los parámetros estimados en ambos modelos no necesariamente coinciden. Aunque la serie tenga una componente estacional \\(E_t\\), \\(X_t = T_t + E_t + \\epsilon_t\\), solamente consideramos un modelo de regresión entre \\(X_t\\) y \\(T_t\\), tal que \\(X_t = T_t + \\eta_t\\), donde \\(\\eta_t\\) es el término de error, de forma que \\(\\eta_t=E_t+\\epsilon_t\\). Por ejemplo, En el caso lineal \\(T_t = \\beta_0 + \\beta_1t\\), ajustamos el modelo de regresión lineal: \\(X_t = \\beta_0 + \\beta_1t + \\eta_t\\). En el caso cuadrático \\(T_t = \\beta_0 +\\beta_1t+\\beta_2t^2\\), ajustamos el modelo de regresión cuadrático \\(X_t = \\beta_0+\\beta_1t+\\beta_2t^2 +\\eta_t\\). Nótese que en este caso hay que definir una variable explicativa adicional \\(t^2\\). En general, para que datos de series de tiempo sean estacionarias, es necesario hacer un promedio de productos en el tiempo. Como para datos de serie de tiempo es importante medir la dependencia entre los valores de la serie; al menos, debemos ser capaces de estimar las autocorrelaciones con precisión. Será difícil medir la dependencia de estos valores si la estructura de dependencia no es regular o si cambia en el tiempo. De ahí, que para realizar cualquier análisis estadístico significativo de datos de series de tiempo, será crucial que las funciones de media y autocovarianza satisfagan las condiciones de estacionaridad dadas en la Definición ??. A menudo, este no es el caso, y en esta sección daremos algunos métodos para lidiar con los efectos de no-estacionaridad sobre las propiedades estacionarias de las series a estudiar. Quizás la forma más fácil de trabajar con series no-estacionarias es el modelo de tendencia estacionaria donde el proceso tiene comportamiento estacionario alrededor de una tendencia. Podemos escribir este tipo de modelos como \\[\\begin{equation} X_t=T_t+Y_t \\tag{2.32} \\end{equation}\\] donde \\(X_t\\) son las observaciones, \\(T_t\\) denota la tendencia y \\(Y_t\\) es un proceso estacionario. Por lo general, una tendencia fuerte \\(T_t\\) puede oscurecer el comportamiento del proceso estacionario \\(Y_t\\), como veremos en ejemplos posteriores. De aquí, será una ventaja el que podamos remover la tendencia como un primer paso para un análisis exploratorio de los datos. Los pasos envuelven obtener un estimador razonable del componente de tendencia, llamémoslo \\(\\hat{T}_t\\) y entonces trabajar con el residual \\[\\begin{equation} \\hat{Y}_t=X_t-\\hat{T}_t. \\tag{2.33} \\end{equation}\\] El primer paso en el análisis de cualquier tipo de serie es un gráfico de los datos. Si existe alguna aparente discontinuidad en la serie, tal como un cambio súbito en el nivel de la serie, esto puede darnos una idea para el análisis de la serie, un primer paso sería dividir la serie en segmentos homogéneos. Si existen observaciones o datos “outliers”, estos deben ser estudiados con cuidado para verificar si existe alguna justificación para descartar estas observaciones, como por ejemplo si una observación ha sido registrada de algún otro proceso por error. La inspección del gráfico también podría sugerir la representación de los datos como una realización de un proceso, como el modelo clásico de descomposición dado por (1.8). Si la componente estacional y la componente aleatoria o ruido parecen incrementarse con el nivel del proceso entonces una transformación preliminar de los datos es a menudo usada para hacer que los datos transformados sean compatibles con el modelo (1.8). En esta sección discutiremos algunas técnicas para identificar y eliminar las componentes en (1.8). Nuestro objetivo es estimar y extraer las componentes determinísticas \\(T_t\\) y \\(E_t\\) con la esperanza de que el residual o la componente aleatoria \\(\\epsilon_t\\) llegue a ser un proceso estacionario. Entonces podremos usar la teoría de tales procesos para hallar un modelo probabilístico satisfactorio para el proceso \\(\\epsilon_t\\), analizar sus propiedades y usarlo en conjunto con \\(T_t\\) y \\(E_t\\) para hacer pronósticos y control de \\(X_t\\). Los dos enfoques para la eliminación de las componentes de tendencia y estacional son: Estimación de \\(T_t\\) y \\(E_t\\) en el modelo (1.8), Diferencia de los datos \\(X_t\\). Ilustraremos ambos enfoque con varios ejemplos 2.2.1 Estimación de la tendencia en ausencia de estacionalidad Si tenemos una serie de tiempo para la cual está ausente la componente estacional \\(E_t\\) el modelo (1.8) llega ser \\[\\begin{equation} X_t = T_t + \\epsilon_t,\\quad t=1,\\ldots,n \\tag{2.34} \\end{equation}\\] donde, sin perdida de generalidad, podemos suponer que \\(\\mathbb{E}(\\epsilon_t)=0\\). A continuación vamos a describir tres métodos para estimar la tendencia \\(T_t\\). Método T1: Estimación de \\(T_t\\) por mínimos cuadrados. El objetivo de este método es intentar ajustar una familia paramétrica de funciones como las vistas en las ecuaciones (2.26) a (??), a los datos eligiendo los parámetros que minimicen \\(\\sum_t(X_t-T_t)^2\\). Esto es, asumiendo que $(_t)=0, se tiene \\[\\mathbb{E}(X_t)=T_t=f(t)\\] Una suposición común es que la función \\(f\\) depende de ciertos parámetros (desconocidos) \\(\\beta_1,\\ldots,\\beta_p\\), es decir, \\[\\begin{equation} f(t)=f(t;\\beta_1,\\ldots,\\beta_p) \\tag{2.35} \\end{equation}\\] Sin embargo, el tipo de función es conocida. Los parámetros \\(\\beta_1,\\ldots,\\beta_p\\) serán estimados a partir de una realización \\(x_t\\) de la variable aleatoria \\(X_t\\). La aproximación por estimación de mínimos cuadrados \\(\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_p\\) debe satisfacer \\[\\begin{equation} \\sum_t(x_t-f(t;\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_p))^2 = \\min_{\\beta_1,\\ldots,\\beta_p}\\sum_t(x_t-f(t;\\beta_1,\\ldots,\\beta_p))^2 \\tag{2.36} \\end{equation}\\] cuya solución, si existe, es un problema numérico. El valor \\(\\hat{x}_t=f(t;\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_p)\\) servirá como una predicción de futuros valores \\(x_t\\). Las diferencias observadas \\(x_t-\\hat{x}_t\\) son llamadas residuales. Ellas contienen información sobre la bondad de ajuste del modelo a los datos. Ejemplo 2.2 El archivo “USPOP.txt” contiene la información de la población de Estados Unidos de América desde 1780 hasta 1980 segun el censo poblacional cada 10 años. En el gráfico podemos observar que no existe estacionalidad, por lo que podemos aplicar el método descrito para ajustar la tendencia. uspop=ts(scan(&quot;data/USPOP.txt&quot;),frequency=1/10,start=1790) pop=window(uspop,start=1790) plot(pop,type=&quot;o&quot;,ylab=&quot;Poblacion (millones)&quot;) Podemos notar del gráfico que la tendencia es creciente y parece tener un comportamiento cuadrático, por lo que ajustando una función de la forma (2.27) para la población de los datos USPOP para \\(1790\\leq t\\leq1980\\) nos da los parámetros estimados \\[\\hat{a}_0=2.101\\times10^{10};\\quad \\hat{a}_1=-2.338\\times10^{7}; \\hat{a}_2=6.506\\times10^{3}.\\] En el gráfico siguiente se puede observar la curva ajustada y los datos originales. Los valores estimados del proceso de ruido \\(\\epsilon_t, 1790\\leq t\\leq1980\\), son los residuales obtenidos por sustracción de \\(\\hat{T}_t=\\hat{a}_0+\\hat{a}_1t+\\hat{a}_2t^2\\) de la serie \\(X_t\\). La componente de tendencia \\(T_t\\) nos proporciona un predictor natural de los valores futuros de \\(X_t\\). Por ejemplo si deseamos estimar \\(T_{1990}\\) por su valor medio, obtenemos \\[T_{1990} = 2.4853\\times10^8\\] para la población de EE.UU en 1990. Sin embargo si los residuales \\(\\hat{\\epsilon}_t\\) están altamente correlacionados podemos ser capaces de usar esos valores para dar una mejor estimación de \\(T_{1990}\\) y por consiguiente de \\(X_{1990}\\). x=time(pop) reg=lm(pop~x+I(x^2),na.action=NULL) summary(reg) ## ## Call: ## lm(formula = pop ~ x + I(x^2), na.action = NULL) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6947521 -358167 436285 1481410 3391761 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.10e+10 6.59e+08 31.9 &lt;2e-16 *** ## x -2.34e+07 6.98e+05 -33.5 &lt;2e-16 *** ## I(x^2) 6.51e+03 1.85e+02 35.2 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2770000 on 18 degrees of freedom ## Multiple R-squared: 0.999, Adjusted R-squared: 0.999 ## F-statistic: 8.05e+03 on 2 and 18 DF, p-value: &lt;2e-16 plot(pop,type=&quot;o&quot;,xlab=&quot;Años&quot;,ylab=&quot;Poblacion (millones)&quot;) lines(reg$fitted.values,col=&quot;red&quot;) Ejemplo 2.3 El archivo “Population-North-Rhine-Westphalia.txt” contiene la población de la región North-Rhine-Westphalia (Alemania) en millónes cada 5 años desde 1935 hasta 1980. Observando el gráfico podemos suponer que la tendencia se puede ajustar por el modelo cúbico (2.28), esto es \\[T_t=\\beta_0+\\beta_1t+\\beta_2t^2+\\beta_3t^3\\] El código en R para el gráfico y el ajuste es NRWpop=read.table(&quot;data/Population-North-Rhine-Westphalia.txt&quot;, header = TRUE) knitr::kable(head(NRWpop,booktabs=TRUE, caption=&quot;Población (en millones) de North-Rhine-Westphalia, Alemania, 1935-1980&quot;)) Year Population 1935 11772 1940 12059 1945 11200 1950 12926 1955 14442 1960 15694 plot(NRWpop, type = &quot;b&quot;,col=&quot;blue&quot;,xlab = &quot;Años&quot;,ylab = &quot;Población (millones)&quot;) # Modelo cúbico t=NRWpop[,1] pob=NRWpop[,2] modelo=lm(pob~t+I(t^2)+I(t^3),na.action = NULL) summary(modelo) ## ## Call: ## lm(formula = pob ~ t + I(t^2) + I(t^3), na.action = NULL) ## ## Residuals: ## Min 1Q Median 3Q Max ## -813.0 -199.2 67.1 275.6 493.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.11e+09 5.10e+08 4.13 0.0061 ** ## t -3.23e+06 7.81e+05 -4.14 0.0061 ** ## I(t^2) 1.65e+03 3.99e+02 4.14 0.0061 ** ## I(t^3) -2.81e-01 6.79e-02 -4.14 0.0061 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 472 on 6 degrees of freedom ## Multiple R-squared: 0.974, Adjusted R-squared: 0.962 ## F-statistic: 76.2 on 3 and 6 DF, p-value: 3.63e-05 lines(t,modelo$fitted.values,col=&quot;red&quot;) La curva punteada en azul corresponde a los datos originales, la curva en rojo corresponde al ajuste mediante el modelo cúbico. Método T2: Suavizado por medio de un promedio móvil. Sea \\(q\\) un entero no negativo y consideremos un promedio móvil de la forma \\[\\begin{equation} W_t = \\frac{1}{2q+1}\\sum_{j=-q}^{q}X_{t+j} \\tag{2.37} \\end{equation}\\] de un proceso \\(\\{X_t\\}\\) definido por (2.34). Entonces para \\(q+1\\leq t\\leq n-q\\), \\[\\begin{eqnarray} W_t &amp;=&amp; \\frac{1}{2q+1}\\sum_{j=-q}^qT_{t+j}+\\frac{1}{2q+1}\\sum_{j=-q}^q\\epsilon_{t+j}\\\\ \\nonumber &amp;\\simeq&amp; T_t \\tag{2.38} \\end{eqnarray}\\] suponiendo que \\(T_t\\) es aproximadamente lineal sobre el intervalo \\([t-q,t+q]\\) y que el promedio del término de error sobre este intervalo es cercano a cero. El promedio móvil entonces nos provee con el estimador \\[\\begin{equation} \\hat{T}_t = \\frac{1}{2q+1}\\sum_{j=-q}^qX_{t+j},\\quad q+1\\leq t\\leq n-q. \\tag{2.39} \\end{equation}\\] Dado que \\(X_t\\) es no observado para \\(t\\leq0\\) o \\(t\\geq n\\) no podemos usar (2.39) para \\(t\\leq q\\) o \\(t&gt;n-q\\). Una forma de resolver este problema es haciendo \\(X_t=X_1\\) para \\(t&lt;1\\) y \\(X_t=X_n\\) para \\(t&gt;n\\). A continuación presentamos un ejemplo Ejemplo 2.4 El gráfico siguiente muestra las huelgas ocurridas en EE.UU, de 1951 a 1980, según la Oficina de Estadísticas Laborales del Departamento de Trabajo de los EE.UU. A estos datos le aplicamos un promedio móvil de 5 puntos, la Figura muestra la serie suavizada y el término de error estimado \\(\\hat{\\epsilon}_t = X_t - \\hat{T}_t\\) se muestra en la Figura . Como era de esperarse ellos no presentan una tendencia clara. Las instrucciones en R para el suavizado y los gráficos son los siguientes: H=read.table(&quot;data/Huelgas.txt&quot;) # Promedio móvil por medio de la función &quot;filter&quot; W=filter(H[,2],sides=2,rep(1/5,5)) # Residuales de X_t y=H[,2]-W # Graficos par(mfrow=c(3,1)) plot(H,xlab=&quot;años&quot;,ylab=&quot;Huelgas&quot;,type=&#39;b&#39;, main = &quot;Huelgas en EE.UU., años 1951-1980&quot;) plot(H[,1],W,xlab=&quot;años&quot;,ylab=&quot;Huelgas&quot;,type=&#39;b&#39;, main = &quot;Promedio móvil de 5 puntos para los datos de Huelga&quot;) plot(H[,1],y,xlab=&quot;años&quot;,ylab=&quot;Residuales&quot;,type=&#39;b&#39;, main = &quot;Residuales e_t=X_t-T_t&quot;) Para cada valor fijo \\(a\\in[0,1]\\), el promedio móvil de un lado \\(\\hat{T}_t, t=1,\\ldots,n\\), definido por la recursión \\[\\begin{equation} \\hat{T}_t = aX_t+(1-a)\\hat{T}_t,\\quad t=2,\\ldots,n \\tag{2.40} \\end{equation}\\] y \\[\\hat{T}_1=X_1,\\] se puede calcular usando la opción sides=1 en la función filter de R. Es usual pensar como aplicación de la ecuación (2.40) como un suavizado exponencial, dado que se sigue de la recursión que para \\(t\\leq2, \\hat{T}_t=\\sum_{j=0}^{t-2}a(1-a)^jX_{t-j}+(1-a)^{t-1}X_1\\), es un promedio móvil con peso de \\(X_t,X_{t-1},\\ldots\\), con pesos decreciendo exponencialmente (excepto para el último término). Es útil pensar en \\(\\{\\hat{T}_t\\}\\) en (filter) como un proceso obtenido de \\(\\{X_t\\}\\) por aplicación de un operador lineal o filtro lineal \\(\\hat{T}_t=\\sum_{j=-\\infty}^{\\infty}a_jX_{t+j}\\) con pesos \\(a_j=(2q+1)^{-1},-q\\leq j\\leq q\\), y \\(a_j=0,|j|&gt;q\\). Este filtro particular es un filtro de “paso-bajo” ya que toma los datos \\(\\{X_t\\}\\) y remueve la componente de rápida fluctuación (o de alta frecuencia) \\(\\{\\hat{\\epsilon}_t\\}\\), para dejar el término de la tendencia estimada de lenta variación \\(\\{\\hat{T}_t\\}\\). Método T3: Diferenciación para generar datos estacionarios. En lugar de intentar remover el ruido por suavizado como en el Método T2, ahora intentaremos eliminar la tendencia por diferenciación. Definamos primero el operador diferencia \\(\\nabla\\) por \\[\\begin{equation} \\nabla x_t = x_t-x_{t-1}=(1-B)x_t, \\tag{2.41} \\end{equation}\\] donde \\(B\\) es el operador de desplazamiento hacia atrás (backward shift operator en inglés) u operador de cambio \\[\\begin{equation} Bx_t=x_{t-1}. \\tag{2.42} \\end{equation}\\] Las potencias de los operadores \\(B\\) y \\(\\nabla\\) se definen de manera obvia, esto es, \\(B^j(x_t)=x_{t-j}\\) y \\(\\nabla^j(x_t)=\\nabla(\\nabla^{j-1}(x_t)),j\\geq1\\) con \\(\\nabla^0(x_t)=x_t\\). Los polinomios en \\(B\\) y \\(\\nabla\\) se manipulan de la misma manera que las funciones polinómicas de variables reales. Por ejemplo \\[\\begin{eqnarray*} \\nabla^2x_t &amp;=&amp; \\nabla(\\nabla x_t) = (1-B)(1-B)x_t = (1-2B+B^2)x_t \\\\ &amp;=&amp; x_t-2x_{t-1}+x_{t-2}. \\end{eqnarray*}\\] Si el operador \\(\\nabla\\) se aplica a una función con tendencia lineal \\(T_t=at+b\\), entonces obtenemos la función constante \\(\\nabla T_t=a\\). De la misma manera cada tendencia polinomial de grado \\(k\\) se puede reducir a una constante por aplicación del operador \\(\\nabla^k\\). Iniciando entonces con el modelo \\(X_t=T_t+\\epsilon_t\\), donde \\(T_t=\\sum_{j=0}^ka_jt^j\\) y \\(\\epsilon_t\\) es estacionario con media cero, obtenemos \\[\\nabla^kX_t = k!a_k+\\nabla^k\\epsilon_t,\\] un proceso estacionario con media \\(k!a_k\\). Esta consideración sugiere la posibilidad, dada una sucesión \\(\\{X_t\\}\\) de datos, de aplicar el operador \\(\\nabla\\) repetidamente hasta conseguir una sucesión \\(\\{\\nabla^kX_t\\}\\) la cual puede ser apropiadamente modelada como una realización de un proceso estacionario. Se encuentra a menudo en la práctica que el orden \\(k\\) de diferenciación es bastante pequeño, frecuentemente uno o dos. 2 Ejemplo 2.5 Aplicando esta técnica al ejemplo 2.2 de población de los EE.UU, hallamos que dos operaciones de diferenciación son suficientes para producir una serie sin aparente tendencia. Los datos diferenciados se muestran en la Figura. Note que la magnitud de las fluctuaciones en \\(\\nabla^2X_n\\) se incrementa con el valor de \\(n\\). Este efecto se puede suprimir tomando primero logaritmo natural, \\(y_n=\\ln X_n\\) y entonces aplicando el operador \\(\\nabla^2\\) a la serie \\(\\{y_n\\}\\). Las instrucciones en R son las siguientes: Dx=diff(uspop,difference=2) plot(Dx,type=&quot;b&quot;,xlab=&quot;Año&quot;, ylab=&quot;Diferencias&quot;) 2.2.2 Estimación de la tendencia y la estacionalidad Los métodos descritos para estimar y remover la tendencia pueden ser adaptados de manera natural para estimar tanto la tendencia como la estacionalidad en el modelo general \\[\\begin{equation} X_t = T_t + E_t + \\epsilon_t \\end{equation}\\] donde \\(\\mathbb{E}(\\epsilon_t)=0, E_{t+d}=E_t\\) y \\(\\sum_{j=1}^dE_t=0\\). Ilustraremos estos métodos con referencia al siguiente ejemplo de accidentes. El archivo “Accidentes3.txt” muestra el número de accidentes mortales de automóviles mensual ocurridos en EE.UU., entre los años 1973 y 1978. En la tabla siguiente se muestran los datos X&lt;-read.table(&quot;data/Accidentes3.txt&quot;, header = TRUE) Mes X1973 X1974 X1975 X1976 X1977 X1978 Ene 9007 7750 8162 7717 7792 7836 Feb 8106 6981 7306 7461 6957 6892 Mar 8928 8038 8124 7776 7726 7791 Abr 9137 8422 7870 7925 8106 8129 May 10017 8714 9387 8634 8890 9115 Jun 10826 9512 9556 8945 9299 9434 En la figura podemos observar que los datos presentan claramente una componente estacional con periodo \\(d=12\\). Será conveniente para el primer método indexar los datos por mes y año. Entonces \\(X_{j,k}, j=1,\\ldots,12, k=1,\\ldots,6\\) denotará el número de muertes accidentales reportados para el \\(j\\)-ésimo mes del \\(k\\)-ésimo año, \\((1972+k)\\). En otras palabras, definimos \\[X_{j,k}=X_{j+12(k-1)},\\quad j=1,\\ldots,12; k=1,\\ldots,6.\\] Método E1: Método de la tendencia pequeña. Si la tendencia es pequeña (como en los datos de accidentes) no es irrazonable suponer que el término de la tendencia es constante, digamos \\(T_k\\) para el año \\(k\\). Dado que \\(\\sum_{j=1}^{12}E_j=0\\), nos lleva al estimador insesgado natural para la tendencia \\[\\begin{equation} \\hat{T}_k = \\frac{1}{12}\\sum_{j=1}^{12}X_{j,k}, \\tag{2.43} \\end{equation}\\] mientras que para la estacionalidad \\(E_j, j=1,\\ldots,12\\) tenemos el estimador \\[\\begin{equation} \\hat{E}_j = \\frac{1}{6}\\sum_{k=1}^6(X_{j,k}-\\hat{T}_k), \\tag{2.44} \\end{equation}\\] el cual automáticamente satisface el requisito de que \\(\\sum_{j=1}^{12}\\hat{E}_j=0\\). El término de error estimado para el mes \\(j\\) del año \\(k\\) es por supuesto \\[\\begin{equation} \\hat{\\epsilon}_{j,k} = X_{j,k}-\\hat{T}_k-\\hat{E}_j, \\quad j=1,\\ldots,12; k=1,\\ldots,6. \\tag{2.45} \\end{equation}\\] La generalización de (2.43) a (2.45) para datos con estacionalidad con un periodo distinto de 12 es bastante sencillo de realizar, simplemente cambiamos 12 por el correspondiente valor de \\(d\\). Así, en general, si tenemos \\(n\\) años (meses, semanas, días, etc.) y estacionalidad con periodo \\(d\\), los estimadores seran: Para la tendencia \\(T_k\\): \\[\\begin{equation} \\hat{T}_k=\\frac{1}{d}\\sum_{j=1}^dX_{j,k} \\tag{2.46} \\end{equation}\\] Para la estacionalidad \\(E_j\\): \\[\\begin{equation} \\hat{E}_j=\\frac{1}{n}\\sum_{k=1}^n(X_{j,k}-\\hat{T}_k),\\quad j=1,\\ldots,d \\tag{2.47} \\end{equation}\\] Para el error \\[\\begin{equation} \\hat{\\epsilon}_{j,k}=X_{j,k}-\\hat{T}_k-\\hat{E}_j,\\quad k=1,\\ldots,n; j=1,\\ldots,d. \\tag{2.48} \\end{equation}\\] Las Figuras siguientes muestran respectivamente las observaciones con la tendencia removida \\(X_{j,k}-\\hat{T}_k\\), la componente estacional estimada \\(\\hat{E}_j\\) y las observaciones con la tendencia y la estacionalidad removida \\(\\hat{\\epsilon}_{j,k}=X_{j,k}-\\hat{T}_k-\\hat{E}_j\\). En la última gráfica (para el error) no se observa una aparente tendencia o estacionalidad. # Estimacion de la tendencia Tk=numeric(n*d) for(k in 1:n) { for(j in 1:d) { Tk[(k-1)*d+j]=Tk[(k-1)*d+j]+(1/12)*X[j,k+1] } } # Grafico con la tendencia removida plot(V-Tk,type = &quot;l&quot;,xlab = &quot;Meses&quot;,ylab = &quot;Num. de accidentes&quot;, main = &quot;Accidentes mortales mensuales con la tendencia T_k removida&quot;) # Estimacion de la estacionalidad Ej=numeric(n*d) for(j in 1:d) { aux=0 for(k in 1:n) { aux=aux+(X[j,k+1]-Tk[(k-1)*d+j]) } for(k in 1:n) { Ej[(k-1)*d+j]=(1/n)*aux } } # Grafico de la estacionalidad plot(Ej,type = &quot;l&quot;,xlab = &quot;Meses&quot;,ylab = &quot;Num. de accidentes&quot;, main = &quot;Estacionalidad de los accidentes mortales mensuales&quot;) # Estimacion del error error=V-Tk-Ej # Grafico del error estimado plot(error,type = &quot;l&quot;,xlab = &quot;Meses&quot;,ylab = &quot;Error estimado&quot;, main = &quot;Error estimado de los accidentes mortales&quot;) grid(col = &quot;darkgray&quot;) Método E2: Estimación por promedio móvil. El siguiente método es preferible al Método E1 ya que no se basa en la suposición de que \\(T_t\\) es casi constante sobre cada ciclo de estacionalidad. Suponga que tenemos las observaciones \\(\\{x_1,\\ldots,x_n\\}\\). El priemr paso es estimar la tendencia aplicando un filtro de promedio móvil especialmente elegido para eliminar la componente estacional y para amortiguar el ruido. Si el periodo \\(d\\) es par, digamos \\(d=2q\\), entonces usamos \\[\\begin{equation} \\hat{T}_t = (0.5x_{t-q} + x_{t-q+1} + \\cdots + x_{t+q-1} + 0.5x_{t+q})/d, q&lt;t\\leq n-q. \\tag{2.49} \\end{equation}\\] Si el periodo es impar, digamos \\(d=2q+1\\), entonces usamos un promedio móvil simple \\[\\begin{equation} \\hat{T}_t=\\frac{1}{d}\\sum_{j=-q}^qX_{t-j},\\quad q+1\\leq t\\leq n-q \\tag{2.50} \\end{equation}\\] nuevamente, haciendo uso del ejemplo de accidentes mortales, la figura siguiente muestra la tendencia estimada \\(\\hat{T}_t\\) usando el filtro mostrado en (2.49). También muestra la tendencia constante a trozos obtenida por el Método E1. En la misma se puede observar que para los ciclos 2 a 5 la tendencia se mantiene cercana, pero para los ciclos 1 y 6 hay una diferencia mas marcada. # Filtro para ciclo par d q=d/2 N=n*d aux=0 T.est=numeric(N) for (t in 1:N) { if (t&lt;=q|t&gt;N-q) { T.est[t]=V[t] } else { aux=0 for (k in -q:q) { if (k==-q|k==q) { aux=aux+0.5*V[t+k] } else { aux=aux+V[t+k] } } T.est[t]=(1/d)*aux } } # Grafico de las tendencias con los metodos E1 y E2 plot(T.est,type = &quot;l&quot;,xlab = &quot;Meses&quot;, ylab = &quot;Num. de accidentes&quot;) lines(Tk,col=&quot;blue&quot;) El segundo paso, es estimar la componente estacional. Para cada \\(k=1,\\ldots,d\\), calculamos el promedio \\(w_k\\) de las desviaciones \\(\\{(X_{k+jd}-\\hat{T}_{k+jd}):q&lt;k+jd\\leq n-q\\}\\). Dado que este promedio de desviaciones no necesariamente suma cero, estimamos la componente estacional \\(E_k\\) como \\[\\begin{equation} \\hat{E}_k = w_k -\\frac{1}{d}\\sum_{i=1}^dw_i,\\quad k=1,\\ldots,d, \\tag{2.51} \\end{equation}\\] y \\(\\hat{E}_k=\\hat{E}_{k-d},k&gt;d\\). Los datos sin la componente estacional se definen entonces como la serie original con la componente estacional removida, es decir, \\[\\begin{equation} d_t = X_t-\\hat{E}_t,\\quad t=1,\\ldots,n. \\tag{2.52} \\end{equation}\\] Finalmente, reestimamos la tendencia de \\(\\{d_t\\}\\) aplicando un filtro de promedio móvil como se describió para los datos no estacionales (método T2) o fijando un polinomio de grado \\(k\\) a la serie \\(\\{d_t\\}\\). El término del ruido estimado llega a ser entonces \\[\\hat{\\epsilon}_t = X_t - \\hat{E}_t - \\hat{E}_t, \\quad t=1,\\ldots,n.\\] Los resultados de aplicar los Métodos E1 y E2 a los datos de accidentes mortales son casi iguales, dado que en este caso la constante a trozos y el promedio móvil de \\(\\hat{T}_t\\) están razonablemente cercanos. Una comparación de los valores estimados de \\(\\hat{E}_k, k=1,\\ldots,12\\), obtenido con ambos métodos se muestra en la tabla siguiente k 1 2 3 4 5 6 7 8 9 10 11 12 Mét E1 -7434 -1504 -724 -523 338 808 1665 961 -87 197 -321 -67 Mét E2 -804 -1522 -737 -526 343 746 1680 987 -109 258 -259 -57 Método E3: Diferenciación a paso \\(\\mathbf{d}\\). La técnica de diferenciación la cual aplicamos antes a datos no estacionales se pueden adaptar para lidiar con el caso estacional de periodo \\(d\\) introduciendo el operador de diferencia de paso \\(d\\), \\(\\nabla_d\\) definido por \\[\\begin{equation} \\nabla_dX_t = X_t-X_{t-d} = (1-B^d)X_t. \\tag{2.53} \\end{equation}\\] Este operador no debe confundirse con el operador \\(\\nabla^d = (1-B)^d\\) definido por (). Aplicando el operador \\(\\nabla_d\\) al modelo \\[X_t = T_t + E_t + \\epsilon_t,\\] donde \\(\\{E_t\\}\\) tiene periodo \\(d\\), obtenemos \\[\\nabla_dX_t = T_t-T_{t-d} + \\epsilon_t-\\epsilon_{t-d},\\] lo cual nos da una descomposición de la diferencia \\(\\nabla_dX_t\\) en una componente de tendencia \\((T_t-T_{t-d})\\) y un término de ruido \\((\\epsilon_t-\\epsilon_{t-d})\\). La tendencia \\((T_t-T_{t-d})\\) se puede eliminar usando los métodos ya descritos, por ejemplo, aplicando alguna potencia del operador \\(\\nabla\\). La figura siguiente muestra el resultado de aplicar el operador \\(\\nabla_{12}\\) a los datos de accidentes mortales. # Operador Nabla_d, usamos la funcion diff con lag=12 NdX=diff(V,lag=12) plot(NdX,type = &quot;l&quot;) La componente estacional evidente en la Figura~ está ausente en la Figura de \\(\\nabla_{12}X_t,13\\leq t\\leq72\\). Sin embargo todavía parece haber una tendencia decreciente. Si ahora aplicamos el operador \\(\\nabla\\) a \\(\\nabla_{12}X_t\\) y graficamos las diferencias \\(\\nabla\\nabla_{12}X_t,t=14,\\ldots,72\\) obtenemos el gráfico mostrado en la Figura siguiente, la cual no tiene una aparente tendencia o componente estacional. DNdX=diff(NdX) plot(DNdX,type = &quot;l&quot;) 2.3 Estimación de la tendencia por regresión clásica Los modelos de regresión son importantes para modelos en el dominio de tiempo y de frecuencia que discutiremos posteriormente. La idea principal depende de poder expresar una serie respuesta \\(X_t\\) como una combinación lineal de entradas \\(z_{t_1},z_{t_2},\\ldots,z_{t_q}\\). La estimación de los coeficientes \\(\\beta_1,\\beta_2,\\ldots,\\beta_q\\) de la combinación por mínimos cuadrados proporciona un método para modelar \\(X_t\\) en términos de las entradas. 2.3.1 Regresión Clásica Supongamos que tenemos \\(X_t\\), para \\(t=1,2,\\ldots,n\\) influenciada por una colección de series independientes \\(z_{t_1},z_{t_2},\\ldots,z_{t_q}\\), donde consideraremos primero que las entradas son fijas y conocidas. Podemos expresar esta relación como \\[\\begin{equation} X_t=\\beta_1z_{t_1}+\\beta_2z_{t_2}+\\cdots+\\beta_qz_{t_q}+w_t \\tag{2.54} \\end{equation}\\] donde \\(\\beta_1,\\beta_2,\\ldots,\\beta_q\\) son los coeficientes de regresión fijos y desconocidos, \\(\\{w_t\\}\\) es un error aleatorio o un proceso de ruido consistente de variables normales iid con media cero y varianza \\(\\sigma_w^2\\). El modelo lineal descrito en (2.54) se puede escribir de forma más general por medio de definir los vectores columna \\(\\mathbf{z}_t=(z_{t_1},z_{t_2},\\ldots,z_{t_q})^t\\) y \\(\\mathbf{\\beta}=(\\beta_1,\\beta_2,\\ldots,\\beta_q)^t\\) donde \\(t\\) denota la traspuesta, así (2.54) será \\[\\begin{equation} X_t=\\mathbf{\\beta}^t\\mathbf{z}_t+w_t \\tag{2.55} \\end{equation}\\] donde \\(w_t\\sim iid(0,\\sigma_w^2)\\). Es natural considerar la estimación de los coeficientes del vector \\(\\mathbf{\\beta}\\) minimizando la suma residual de cuadrados \\[\\begin{equation} RSS=\\sum_{t=1}^{n}(X_t-\\mathbf{\\beta}^t\\mathbf{z}_t)^2 \\tag{2.56} \\end{equation}\\] con respecto a \\(\\beta_1,\\beta_2,\\ldots,\\beta_q\\). Minimizando RSS obtenemos el estimador común de mínimos cuadrados. Esta minimización se puede hacer por diferenciación de (2.56) con respecto al vector \\(\\mathbf{\\beta}\\) o usando las propiedades de proyección. En la notación anterior, obtenemos la ecuación normal \\[\\begin{equation} \\left(\\sum_{t=1}^{n}\\mathbf{z}_t\\mathbf{z}_t^t\\right)\\hat{\\mathbf{\\beta}}=\\sum_{t=1}^{n}\\mathbf{z}_tX_t \\tag{2.57} \\end{equation}\\] Definiendo la matriz \\(Z=(\\mathbf{z}_1,\\mathbf{z}_2,\\ldots,\\mathbf{z}_n)^t\\) como una matriz \\(n\\times q\\) compuesta de \\(n\\) muestras de las variables de entradas y el vector observado \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)^t\\) se puede hacer una simplificación de la notación. Esta identificación nos lleva a \\[\\begin{equation} (Z^tZ)\\hat{\\mathbf{\\beta}}=Z^t\\mathbf{x} \\tag{2.58} \\end{equation}\\] y la solución es \\[\\begin{equation} \\hat{\\mathbf{\\beta}}=(Z^tZ)^{-1}Z^t\\mathbf{x}, \\tag{2.59} \\end{equation}\\] cuando la matriz \\(Z^tZ\\) es de rango \\(q\\). El residual minimizado de suma de cuadrados (2.56) tiene la forma matricial equivalente \\[\\begin{eqnarray} RSS&amp;=&amp;(\\mathbf{x}-Z\\hat{\\mathbf{\\beta}})^t(\\mathbf{x}-Z\\hat{\\mathbf{\\beta}})\\\\ \\nonumber &amp;=&amp;\\mathbf{x}^t\\mathbf{x}-\\hat{\\mathbf{\\beta}}^tZ^t\\mathbf{x}\\\\ \\nonumber &amp;=&amp;\\mathbf{x}^t\\mathbf{x}-\\mathbf{x}^tZ(Z^tZ)^{-1}Z^t\\mathbf{x}. \\tag{2.60} \\end{eqnarray}\\] El estimador común de mínimos cuadrados es insesgado, esto es, \\(\\mathbb{E}(\\hat{\\mathbf{\\beta}})=\\mathbf{\\beta}\\), y tiene la menor varianza de todos los estimadores insesgados lineales. Si los errores \\(w_t\\) son normalmente distribuidos (Gaussianos), \\(\\hat{\\mathbf{\\beta}}\\) es también el estimador de máxima verosimilitud para \\(\\mathbf{\\beta}\\) y es normalmente distribuido con \\[\\begin{eqnarray} \\text{cov}(\\hat{\\mathbf{\\beta}})&amp;=&amp;\\sigma_w^2\\left(\\sum_{t=1}^{n}\\mathbf{z}_t\\mathbf{z}_t^t\\right)^{-1}\\\\ \\nonumber &amp;=&amp;\\sigma_w^2(Z^tZ)^{-1}\\\\ \\nonumber &amp;=&amp;\\sigma_w^2C, \\tag{2.61} \\end{eqnarray}\\] donde \\[\\begin{equation} C=(Z^tZ)^{-1}. \\tag{2.62} \\end{equation}\\] Un estimador insesgado para la varianza \\(\\sigma_w^2\\) es \\[\\begin{equation} s_w^2=\\frac{RSS}{n-q} \\tag{2.63} \\end{equation}\\] contrastado con el estimador de máxima verosimilitud \\(\\hat{\\sigma}_w^2=RSS/n\\) el cual tiene divisor \\(n\\). Bajo la suposición de que \\(s_w^2\\) tiene distribución proporcional a una variable aleatoria chi-cuadrado con \\(n-q\\) grados de libertad, \\(\\chi_{n-q}^2\\), e independiente de \\(\\hat{\\beta}\\), se sigue que \\[\\begin{equation} t_{n-q}=\\frac{(\\hat{\\beta}_i-\\beta_i)}{s_w\\sqrt{c_{ii}}} \\tag{2.64} \\end{equation}\\] tiene distribución \\(t\\)-de Student con \\(n-q\\) grados de libertad; \\(c_{ii}\\) denota el \\(i\\)-ésimo elemento de la diagonal de \\(C\\), como se definió en (2.62). Hay varios modelos que podemos utilizar de manera de seleccionar el mejor subconjunto de variables independientes. Suponga que tenemos un modelo que sólo considera un subconjunto \\(q_1&lt;q\\) de variables independientes \\(\\mathbf{z}_{1t}=(z_{t_1},z_{t_2},\\ldots,z_{t_q1})^t\\) que influencian a la variable \\(X_t\\), así el modelo \\[\\begin{equation} X_t=\\mathbf{\\beta}_1^t\\mathbf{z}_{1t}+w_t \\tag{2.65} \\end{equation}\\] llega a ser la hipótesis nula, donde \\(\\mathbf{\\beta}_1=(\\beta_1,\\beta_2,\\ldots,\\beta_{q1})^t\\) es un subconjunto de los coeficientes de las \\(q\\) variables originales. Podemos contrastar el modelo reducido (2.65) contra el modelo completo (2.55) comparando el residual de la suma de cuadrados bajo los dos modelos usando el estadístico \\(F\\) \\[\\begin{equation} F_{q-q1,n-q}=\\frac{RSS_1-RSS}{RSS}\\frac{n-q}{q-q1} \\tag{2.66} \\end{equation}\\] el cual tiene distribución \\(F\\) con \\(q-q1\\) y \\(n-q\\) grados de libertad cuando (2.66) es el modelo correcto. La información envuelta en la prueba se resume en una tabla de Análisis de Varianza (ANOVA) como la mostrada en la Tabla siguiente para este caso particular. La diferencia en el numerador es llamada regresión de la suma de cuadrados. Fuente g.l Suma de cuadrados Medias Cuadrados \\(z_{t,q_1+1},\\ldots,z_{t,q}\\) \\(q-q_1\\) \\(SS_{reg}=RSS_1-RSS\\) \\(MS_{reg}=SS_{reg}/(q-q_1)\\) Error \\(n-q\\) \\(RSS\\) \\(s_e^2=RSS/(n-q)\\) Total \\(n-q_1\\) \\(RSS_1\\) En términos de la Tabla, por convención escribimos el estadístico \\(F\\) dado en (2.66) como el radio de dos medias cuadrados, obteniéndose \\[\\begin{equation} F_{q-q1,n-q}=\\frac{M S_{reg}}{s_w^2}. \\tag{2.67} \\end{equation}\\] Un caso de especial interés es para \\(q_1=1\\) y \\(z_{1t}=1\\), así el modelo en (2.65) es \\[X_t=\\beta_1+w_t\\] y la proporción de variación explicada por las otras variables es \\[\\begin{equation} R_{xz}^2=\\frac{RSS_0-RSS}{RSS_0}, \\tag{2.68} \\end{equation}\\] donde la suma residual de cuadrados bajo el modelo reducido dada por \\[\\begin{equation} RSS_0=\\sum_{t=1}^{n}(X_t-\\bar{X})^2 \\tag{2.69} \\end{equation}\\] es precisamente la suma de desviaciones al cuadrado de la media \\(\\bar{X}\\). La medida \\(R_{xz}^2\\) es la correlación múltiple cuadrado entre \\(X_t\\) y \\(z_{t2},z_{t3},\\ldots,z_{tq}\\). Las técnicas discutidas se pueden usar para hacer comparación entre varios modelos. Estas pruebas han sido usadas en el pasado en una manera gradual, donde las variables son añadidas o suprimidas cuando los valores de la prueba \\(F\\) exceden o fallan en exceder algunos niveles predeterminados. El procedimiento, llamado regresión múltiple por pasos, es útil para conseguir un conjunto de variables que sea de utilidad. Una manera alternativa es enfocándose en un procedimiento para selección del modelo que no sea secuencial, sino simplemente evaluar cada modelo en base a sus propios méritos. Suponga que consideramos un modelo de regresión con \\(k\\) coeficientes y denotemos el estimador de máxima verosimilitud para la varianza como \\[\\begin{equation} \\hat{\\sigma}_k^2=\\frac{RSS_k}{n} \\tag{2.70} \\end{equation}\\] donde \\(RSS_k\\) denota la suma residual de cuadrados bajo el modelo con \\(k\\) coeficientes de regresión. Entonces, Akaike (1969, 1973, 1974) sugirió medir la bondad del ajuste para este modelo en particular equilibrando el error del ajuste contra el número de parámetros en el modelo; definiendo lo siguiente Definición 2.16 (Criterio de Información de Akaike (AIC)) El Criterio de Información de Akaike se define como \\[\\begin{equation} AIC=\\ln\\hat{\\sigma}_k^2+\\frac{n+2k}{n} \\tag{2.71} \\end{equation}\\] donde \\(\\hat{\\sigma}_k^2\\) está dado por (2.70) y \\(k\\) es el número de parámetros en el modelo El criterio de información de Akaike (AIC) es una medida de la calidad relativa de un modelo estadístico, para un conjunto dado de datos. Como tal, el AIC proporciona un medio para la selección del modelo. El valor de \\(k\\) que minimiza \\(AIC\\) especifica el mejor modelo. La idea es que la minimización de \\(\\hat{\\sigma}_k^2\\) sea razonablemente objetiva, excepto que decrezca monótonamente cuando \\(k\\) crece. Por lo tanto, debemos penalizar la variación del error por un término proporcional al número de parámetros. La elección del término de penalización dado por (2.71) no es único. Ejemplo 2.6 (Temperatura global) Consideremos los datos de temperatura global del archivo “globtemp2.txt”. Estos datos corresponden a 125 años de mediciones de temperatura, representan la desviación promedio entre las temperaturas en tierra y aire medidos en grados centígrados (°C), desde 1880 hasta 2004. # Lectura del archivo de datos globtemp=read.table(&quot;data/globtemp2.txt&quot;) # Grafico de la serie de datos plot(globtemp[,1],globtemp[,2],type = &quot;l&quot;,col = &quot;red&quot;, xlab = &quot;Años&quot;, ylab = &quot;Desv. temp. global (°c)&quot;, main=&quot;Desviación de la temperatura promedio global (1880-2004)&quot;) grid(col = &quot;darkgray&quot;) Ajustemos una regresión simple de la forma \\[x_t+\\beta_1+\\beta_2t+w_t\\text{, con }t=1900,1901,\\ldots,1997.\\] Esta es la forma del modelo de regresión (2.54) con \\(q=2,z_{t_1}=1,z_{t_2}=t\\). Note que podemos usar \\(t=0,1,\\ldots,97\\), sin afectar la interpretación del coeficiente de la pendiente \\(\\beta_2\\), solo se afectaría la intercepción \\(\\beta_1\\). Usando regresión lineal obtuvimos los valores estimados de los coeficientes: \\(\\hat{\\beta}_1=-10.44,\\hat{\\beta}_2=0.0053\\), con un error estándar de \\(4.9\\times10^{-4}\\), dando un incremento estimado de 0.6 grados por cada 100 años. En la figura podemos observar la serie de tiempo con la recta de regresión \\[\\hat{x}_t=-10.44+0.0053t\\] # Convertimos a serie de tiempo gtemp=ts(globtemp[,2],start = 1880) # Tomamos el subconjunto de 1900 a 1997 gtemp=window(gtemp,start = 1900, end = 1997) # Calculamos la regresion lineal fit=lm(gtemp~time(gtemp),na.action = NULL) summary(fit) ## ## Call: ## lm(formula = gtemp ~ time(gtemp), na.action = NULL) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.3578 -0.0899 -0.0055 0.1064 0.2671 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.04e+01 9.56e-01 -10.9 &lt;2e-16 *** ## time(gtemp) 5.36e-03 4.91e-04 10.9 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.137 on 96 degrees of freedom ## Multiple R-squared: 0.554, Adjusted R-squared: 0.549 ## F-statistic: 119 on 1 and 96 DF, p-value: &lt;2e-16 # Grafico de la serie y la regresion plot(gtemp,type = &quot;o&quot;, xlab = &quot;Años&quot;, ylab = &quot;Temperatura (°C)&quot;, col = &quot;red&quot;) abline(fit, col = &quot;blue&quot;) grid(col=&quot;darkgray&quot;) Podemos ver los residuales de la temperatura global \\(e_t=x_t-\\hat{x}_t\\), así como la ACF de los mismos. En las gráficas siguientes se muestran los mismo. En el gráfico para la ACF podemos observar que existe una correlación importante entre \\(x_t\\) y \\(x_{t-1}\\), y también cierta correlación entre \\(x_t\\) y \\(x_{t-4},x_{t-5}\\) y \\(x_{t-6}\\). En el capítulo de modelos \\(AR\\) veremos más sobre esta relación. # Residuales e.temp=gtemp-fit$fitted.values plot(e.temp, xlab=&quot;Años&quot;, main=&quot;Residuales de la temperatura global&quot;) # ACF de los residuales acf(e.temp, main=&quot;ACF de los residuales de la temperatura global&quot;) Ejemplo 2.7 (Uso de regresión para descubrir una señal de ruido) Consideremos el modelo \\[\\begin{equation} x_t=A\\cos(2\\pi\\omega t+\\phi)+w_t \\tag{2.72} \\end{equation}\\] donde \\(\\omega=1/50,A=2,\\phi=0.6\\pi\\) y \\(\\sigma_w=5\\), para \\(t=1,2,\\ldtos,500\\). Usando identidad trigonométrica, podemos escribir \\[\\begin{eqnarray*} A\\cos(2\\pi\\omega t+\\phi)&amp;=&amp;A\\cos(\\phi)\\cos(2\\pi\\omega t)-A\\sin(\\phi)\\sin(2\\pi\\omega t)\\\\ &amp;=&amp;\\beta_1\\cos(2\\pi\\omega t)+\\beta_2\\sin(2\\pi\\omega t), \\end{eqnarray*}\\] donde \\(\\beta_1=A\\cos(\\phi)\\) y \\(\\beta_2=-A\\sin(\\phi)\\). Ahora podemos escribir el modelo (2.72) en la forma de regresión lineal dada por \\[\\begin{equation} x_t=\\beta_1\\cos(2\\pi t/50)+\\beta_2\\sin(2\\pi t/50)+w_t. \\tag{2.73} \\end{equation}\\] Usando regresión lineal sobre la serie generada, el modelo fijado será \\[\\begin{equation} \\hat{x}_t=-0.84_{(0.32})\\cos(2\\pi t/50)-1.99_{(0.32})\\sin(2\\pi t/50) \\tag{2.74} \\end{equation}\\] con \\(\\hat{\\sigma}_w=5.08\\), donde los valores entre paréntesis son los errores estándar. Las instrucciones en R son las siguientes: C=2*cos(2*pi*1:500/50+0.6*pi) w=rnorm(500,0,1) Xt=-0.84*cos(2*pi*1:500/50)-1.99*sin(2*pi*1:500/50) plot(C+5*w, type=&quot;l&quot;, col=&quot;red&quot;) lines(Xt,col=&quot;blue&quot;) Figura 2.1: Datos generados por el modelo senosoidal [línea punteado roja] con modelo de regresión [línea solida azul Ejemplo 2.8 (Uso de periodograma para descubrir una señal de ruido) En algunos ejemplos puede lucir engañoso la periodicidad porque damos por supuesto que conocemos el valor del parámetro \\(\\omega\\). Si no conocemos el parámetro \\(\\omega\\), podemos tratar de fijar el modelo (2.72) usando regresión no lineal con \\(\\omega\\) como un parámetro. Otro método es intentar con distintos valores de \\(\\omega\\) de forma sistemática. Una medida apropiada de la presencia de una frecuencia de oscilación de \\(j\\) ciclos en \\(n\\) puntos de tiempos de un conjunto datos podría ser \\[\\begin{equation} P(j/n)=\\hat{\\beta}_1^2(j/n)+\\hat{\\beta}_2^2(j/n) \\tag{2.75} \\end{equation}\\] lo cual es básicamente una medida de correlación cuadrada. La cantidad (2.75) es usualmente llamada el periodograma. La Figura siguiente muestra el periodograma para los datos generados por (2.72), y es fácil descubrir la componente periódica con frecuencia \\(\\omega=0.02=10/500\\). t=1:500 x=2*cos(2*pi*t/50+0.6*pi+rnorm(500,0,5)) I=abs(fft(x)/sqrt(500))^2 P=(4/500)*I f=0:250/500 plot(f, P[1:251],type=&quot;l&quot;,xlab=&quot;frequency&quot;, ylab=&quot; &quot;,col=&quot;blue&quot;) abline(v=seq(0,.5,.02),lty=&quot;dotted&quot;) Finalmente mencionamos que no es necesario realizar una regresión grande \\[\\begin{equation} x_t=\\sum_{j=0}^{n/2}\\beta_1(j/n)\\cos(2\\pi tj/n)+\\beta_2(j/n)\\sin(2\\pi tj/n) \\tag{2.76} \\end{equation}\\] para obtener los valores de \\(\\beta_1(j/n)\\) y \\(\\beta_2(j/n)\\) \\([\\beta_2(0)=\\beta_2(1/2)=0]\\) porque ellos se pueden calcular fácilmente si \\(n\\) es un entero grande. No hay error en (2.76) porque son \\(n\\) observaciones y \\(n\\) parámetros; la regresión ajusta bien. La Transformada Discreta de Fourier (DFT) es un promedio a valores complejos de los datos dados por \\[\\begin{equation} d(j/n)=n^{-1/2}\\sum_{t=1}^{n}x_t\\exp(-2\\pi i tj/n) \\tag{2.77} \\end{equation}\\] a los valores \\(j/n\\) son llamados frecuencias fundamentales o de Fourier. Como un número grande de cálculos redundantes en (??) se pueden realizar rápidamente usando la Transformada Rápida de Fourier (FFT), la cual está disponible en muchos paquete de computación entre ellos Matlab y R. Note que 3 \\[\\begin{equation} |d(j/n)|^2=\\frac{1}{n}\\left(\\sum_{t=1}^{n}x_t\\cos(2\\pi tj/n)\\right)^2+\\frac{1}{n}\\left(\\sum_{t=1}^{n}x_t\\sin(2\\pi tj/n)\\right)^2 \\tag{2.78} \\end{equation}\\] y esta cantidad es lo que llamamos el periodograma; por lo que podemos escribir \\[I(j/n)=|d(j/n)|^2\\] De modo, que podemos calcular el periodograma (2.75) usando la expresión \\[\\begin{equation} P(j/n)=\\frac{4}{n}I(j/n) \\end{equation}\\] Discutiremos esta aproximación con más detalles en el Tema [Análisis Espectral]. Note que la desigualdad de Cauchy-Schwartz implica \\(|\\gamma(s,t)|^2\\leq\\gamma(s,s)\\gamma(t,t)\\).}.↩ Esto depende del hecho de que muchas funciones pueden ser aproximadas bastante bien, en un intervalo de longitud finita, por un polinomio de grado razonablemente bajo.↩ \\(e^{-i\\alpha}=\\cos(\\alpha)-i\\sin(\\alpha)\\) y si \\(z=a-ib\\) entonces \\(|z|^2=z\\bar{z}=(a-ib)(a+ib)=a^2+b^2\\).↩ "],
["modelos-de-series-de-tiempo.html", "Capítulo 3 Modelos de series de tiempo 3.1 Modelos Estocásticos 3.2 Modelos lineales", " Capítulo 3 Modelos de series de tiempo Como indicamos en el capítulo anterior el objetivo principal en el análisis de series de tiempo es desarrollar modelos matemáticos que provean una descripción apropiada para los datos muestrales. Recordando las definiciones 1.7 y 2.1 podemos describir los modelos generales útiles para la descripción de series de tiempo 3.1 Modelos Estocásticos 3.1.1 Procesos Estocásticos De la definición de procesos estocásticos (Definición 2.1), las variables aleatorias de la familia (medibles para todo \\(t\\in T\\)) son funciones de la forma \\[x(\\omega,t):\\Omega\\times T\\to\\mathbb{R}\\] Para \\(T=\\mathbb{N}\\), tenemos un proceso en tiempo discreto y para \\(T\\subset\\mathbb{R}\\) tenemos un proceso en tiempo continuo. En lo que respecta a este libro, consideraremos como subconjunto de índices \\(T=(0,\\infty)\\). Como ya indicamos, usaremos la notación \\(X_t\\) para denotar la realización de un proceso estocástico \\(x_t(\\omega*)\\) cuando no haya lugar a confución. De esta forma, adoptaremos sin pérdida de generalidad, el conjunto de índices habitual de las series de tiempo en el ámbito de las finanzas y economía \\(I=(1,T)\\). De lo anterior, se tiene que los procesos estocásticos suelen ser descritos mediante su distribución conjunta de probabilidades, de manera que la relación que existe entre una realización y un proceso estocástico es análoga a la existente entre la muestra y la población en el análisis estadístico clásico. 3.1.2 Momentos, Varianza, Covarianza y Correlación Definición 3.1 El valor esperado y varianza de un proceso estocástico están dados por \\[\\begin{equation} \\mathbb{E}(x_t)=\\int_{\\Omega}x(\\omega,t)dP(\\omega),\\quad t\\in[0,T] \\tag{3.1} \\end{equation}\\] y \\[\\begin{equation} Var(x_t)=\\mathbb{E}(x_t-\\mathbb{E}(x_t))^2,\\quad t\\in[0,T] \\tag{3.2} \\end{equation}\\] siempre que las integrales existan y sean finitas. Definición 3.2 El \\(k\\)-ésimo momento de \\(x_t\\), con \\(k\\geq1\\), se define como \\(\\mathbb{E}(x_t^k)\\) para todo \\(t\\in[0,t]\\). Definición 3.3 La función de covarianza del proceso para dos instantes de tiempo \\(t\\) y \\(s\\) está dada por \\[\\gamma(t,s)=Cov(x_t,x_s)=\\mathbb{E}[(x_t-\\mathbb{E}(x_t))(x_s-\\mathbb{E}(x_s))]\\] La cantidad \\(x_t-x_s\\) es llamada el proceso de incrementos desde \\(s\\) a \\(t\\), con \\(s&lt;t\\). 3.1.3 Variación de un proceso Sea \\(P_n=\\{0=t_0&lt;t_1&lt;\\cdots&lt;t_i&lt;\\cdots&lt;t_n=t\\}\\) una partición cualquiera del intervalos \\([0,t]\\) en \\(n\\) subintervalos y denotemos por \\[||P_n||=\\max\\{j=0,1,\\ldots,n-1(t_{j+1}-t_j)\\}\\] el tamaño de paso máximo de discretización de la partición \\(P_n\\). Definición 3.4 La variación del proceso \\(x\\) se define como \\[\\begin{equation} V_t(x)=p-\\lim_{||P_n||}\\sum_{k=0}^{n-1}|x_{t_{k+1}}-x_{t_k}| \\tag{3.3} \\end{equation}\\] Si \\(x\\) es diferenciable, entonces \\(V_t(x)=\\int_0^t|x&#39;(u)|du\\). Si \\(V_t(X)&lt;\\infty\\), entonces decimos que \\(x\\) es de variación acotada en \\([0,t]\\). Si es cierto para todo \\(t\\geq0\\), entonces decimos que \\(x\\) tiene variación acotada. Definición 3.5 La variación cuadrática de un proceso estocástico \\(x\\), denotada por \\([x,x]_t\\), se define como \\[\\begin{equation} [x,x]_t = p-\\lim_{||P_n||}\\sum_{k=0}^{n-1}|x_{t_{k+1}}-x_{t_k}|^2 \\tag{3.4} \\end{equation}\\] Para procesos estocásticos con trayectorias continua, el límite existe, y en dicho caso usamos la notación \\(\\langle x,x\\rangle_t\\) y podemos definirla alternativamente como \\[\\begin{equation} \\langle x,x\\rangle_t = p-\\lim_{n\\to\\infty}\\sum_{k=1}^{2^n}\\left(x_{\\min(t,k/2^n)} - x_{\\min(t,(k-1)/2^n)}\\right)^2 \\tag{3.5} \\end{equation}\\] Si \\(x\\) es continuo y tiene variación acotada cuadrática finita, entonces su variación total es infinita. Note que \\(V_t(x)\\) y \\([x,x]_t\\) son también procesos estocásticos. 3.1.4 Martingalas En teoría de probabilidad, un proceso estocástico de tipo martingala (galicismo de martingale) es todo proceso caracterizado por no tener deriva. Este tipo de procesos estocásticos reciben su nombre de la estrategia de la martingala, un método de apuestas que tuvo cierta fama en el siglo XVIII. La estrategia de la martingala consiste en volver a apostar por el total perdido al momento de incurrir en una pérdida en un juego de azar,. En la nueva apuesta, el jugador tiene la posibilidad de recobrar todas sus pérdidas, por lo que podría parecer que a largo plazo la esperanza de ganancia con esta estrategia se mantienen constantes y a favor del jugador. De hecho, estadísticamente es así: el capital medio del jugador (esto es, el dinero que el jugador tiene a su disposición para jugar) se mantiene constante. El problema reside en que, al incurrir en sucesivas pérdidas, el jugador que siga la estrategia de la martingala se ve obligado a apostar de nuevo cantidades cada vez mayores (las pérdidas acumuladas), que tienden a crecer exponencialmente. Al cabo de unos pocos ciclos de apuestas, el jugador, cuyos recursos son habitualmente muy inferiores a los de la banca, se ve arruinado al ser incapaz de apostar de nuevo por el total de sus pérdidas. Evitar jugadores que intenten seguir la estratega de la martingala es de todos modos una de las razones por las que los casinos actuales establecen límites máximos de apuesta. La estrategia de la martingala se popularizó en el siglo XVIII con fama de ser una estrategia ingenua y propia de mentes simples, puesto que aunque en apariencia es infalible, sin embargo, está abocada a arruinar al jugador. Recibe el nombre de los habitantes de la localidad francesa de Martigues (martingales en francés), situada en las cercanías de Marsella, que por aquel entonces tenían fama de ser ingenuos y simplones. El concepto de la martingala en la teoría de probabilidades fue introducido por Paul Pierre Lévy, y una gran parte del desarrollo original de la teoría la realizó Joseph Leo Doob. Parte de la motivación para ese esfuerzo era demostrar la inexistencia de estrategias de juego infalibles. El concepto fue inmediatamente aplicado al análisis de procesos bursátiles. Uno de los resultados más importantes de la matemática financiera es, precisamente, que un mercado perfecto sin posibilidades de arbitraje es una martingala. Definición 3.6 Sea \\((\\omega,\\mathcal{F},P)\\) un espacio de probabilidad. Una filtración \\(\\{\\mathcal{F}_t,t\\geq0\\}\\) es una familia creciente de sub-\\(\\sigma\\)-álgebras de \\(\\mathcal{F}\\) indexadas por \\(t\\geq0\\); es decir, para cada \\(s,t&gt;0\\) tal que \\(s&lt;t\\), se tiene \\(\\mathcal{F}_s\\subset\\mathcal{F}_t\\) con \\(\\mathcal{F}_0=\\{\\Omega,\\emptyset\\}\\). Para cada proceso estocástico \\(\\{x_t\\}_{t\\geq0}\\) y para cada \\(t\\), podemos asociar una \\(\\sigma\\)-álgebra denotada por \\(\\mathcal{F}_t=\\sigma\\{x_s:0\\leq s\\leq t\\}\\), y que además es la \\(\\sigma\\)-álgebra generada por \\(x\\); es decir, la \\(\\sigma\\)-álgebra más pequeña (minimal) de \\(\\mathcal{F}\\) que hace a \\(x(s,\\omega)\\) medible para cada \\(0\\leq s\\leq t\\). Definición 3.7 Dado un proceso estocástico \\(\\{X_t\\}_{t\\geq0}\\) y una filtración \\(\\{\\mathcal{F}_t, t\\geq0\\}\\) (no necesariamente la que genera \\(X\\)), el proceso \\(X\\) se denomina adaptado a \\(\\{\\mathcal{F}_t, t\\geq0\\}\\) (\\(\\mathcal{F}_t\\)-adaptado) si para cada \\(t\\geq0\\), \\(X(t)\\) es \\(\\mathcal{F}_t\\)-medible. En otras palabras \\(X=\\{X_t\\}_{t\\geq0}\\) es \\(\\mathcal{F}_t\\)-adaptado cuando el valor de \\(X_t\\) en el tiempo \\(t\\) solo depende de la información contenida en la realización hasta el instante \\(t\\). Dado un espacio de probabilidad \\((\\Omega,\\mathcal{F},P)\\) y una filtración \\(\\{\\mathcal{F}_t,t\\geq0\\}\\), entonces definimos el espacio de probabilidad filtrado a la cuaterna \\((\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\geq0},P)\\). Definición 3.8 Sea \\((\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\geq0},P)\\) un espacio de probabilidad filtrado. Un proceso \\(X_t\\) con \\(t\\in T\\), \\(T\\subseteq\\mathcal{R}\\) un conjunto de índices, es una martingala relativo a la filtración \\(\\{\\mathcal{F}_t,t\\geq0\\}\\), si \\(X_t\\) es adaptado a la filtración \\(\\{\\mathcal{F}_t,t\\geq0\\}\\) \\(X_t\\) es integrable, es decir, \\(\\mathbb{E}|X_t|&lt;\\infty\\), Para cualesquieras \\(s\\) y \\(t\\) con \\(s&lt;t\\), \\(\\mathbb{E}(X_t|\\mathcal{F}_s)=X_s\\) c.s. Decimos que el proceso es una submartingala si \\[\\mathbb{E}(X_t|\\mathcal{F}_s)\\geq X_s \\text{ c.s.}\\] Decimos que es una supermartingala si \\[\\mathbb{E}(X_t|\\mathcal{F}_s)\\leq X_s \\text{ c.s.}\\] Ejemplo 3.1 Sean \\(X_0,X_1,\\ldots,X_n\\) variables aleatorias iid tal que \\(\\mathbb{E}(X_1)=\\mu\\) y sean \\[\\begin{eqnarray*} M_0 &amp;=&amp; X_0 \\\\ M_1 &amp;=&amp; X_0+X_1 \\\\ \\vdots &amp; &amp; \\vdots \\\\ M_n &amp;=&amp; X_0+X_1+\\cdots+X_n \\end{eqnarray*}\\] La sucesión de variables aleatorias \\(M_n\\) se llama paseo aleatorio y es una supermartingala si \\(\\mu\\leq0\\), una martingala si \\(\\mu=0\\) y una submartingala si \\(\\mu\\geq0\\). Es fácil demostrarlo, sencillamente usamos el hecho de que \\[M_{n+1}=M_n+X_{n+1}\\] y que \\(M_n\\) y \\(X_{n+1}\\) son independientes. Podemos generar tal proceso en R. n=100 mu=0 sigma=1 X=rnorm(n,mu,sigma) M=cumsum(X) plot(M,type = &quot;l&quot;,xlab = &quot;t&quot;,ylab = &quot;M_n&quot;) Ejemplo 3.2 (Precio de acciones) Sean \\(Y_0,Y_1,\\ldots,Y_n\\) variables aleatorias independientes y positivas. Supongamos que una acción tiene precio \\(M_0\\) a tiempo \\(t=0\\). Un modelo común para modelar el precio de la acción en tiempo \\(t=n\\) es \\[M_{n+1}=M_nY_n\\] donde \\((Y_n-1)\\times100\\) representa (en porcentaje) la variabilidad de la acción. Usando las propiedades de esperanza condicional (Apéndice), es muy sencillo demostrar que \\[\\mathbb{E}(M_{n+1}|M_0,\\ldots,M_n)=M_n\\mathbb{E}(Y_n)\\] En particular, si \\(Y_1,\\ldots,Y_n\\) son idénticamente distribuidas con \\(\\mathbb{E}(Y_1)=\\mu\\) tenemos que \\(M_n\\) es Una martingala si \\(\\mu=1\\) Una submartingala si \\(\\mu&gt;1\\) Una supermartingala si \\(\\mu&lt;1\\). Dos modelos bien conocidos de lo anterior son Modelo Black-Scholes discreto. Sean \\(Y_1,\\ldots,Y_n\\) definidas por \\[Y_n=e^{Z_n}\\] donde \\(Z_1,\\ldots,Z_n\\) son variables aleatorias independientes normales \\(N(\\mu,\\sigma^2)\\). Modelo Binomial. Sean \\(Y_1,\\ldots,Y_n\\) definidas por \\[P(Y_i=(1+t)e^{-r})=p\\quad\\text{ y }\\quad P(Y_i=(1+t)^{-1}e^{-r})=1-p\\] La constante \\(r\\) es la tasa de interés y los factores \\((1+t)\\) y \\((1+t)^{-1}\\) modelan las variaciones del mercado y garantizan que el precio tiene la forma \\(M_0(1+t)^ye^{-nr}\\), con \\(|y|\\leq n\\). La volatilidad del precio está asociada a \\(p\\). Definición 3.9 Una variable aleatoria \\(X\\) es cuadrado integrable si \\(\\mathbb{E}(X^2)&lt;\\infty\\). Un proceso estocástico \\(X_t\\) en el intervalo \\([0,T]\\), donde \\(T\\) puede ser infinito, es cuadrado integrable si \\[\\begin{equation} \\sup_{t\\in[0,T]}\\mathbb{E}(X_t^2)&lt;\\infty \\tag{3.6} \\end{equation}\\] es decir, si sus segundos momentos son acotados. Definición 3.10 Un proceso estocástico \\(X_t, 0\\leq t\\leq T\\) se dice que es uniformemente integrable si \\[\\mathbb{E}(|X_t|\\mathbf{1}_{\\{|X_t|&gt;n\\}})\\] converge a 0 cuando \\(n\\to\\infty\\) uniformemente en \\(t\\). 3.1.5 Propiedad de Markov La propiedad de Markov establece que si conocemos el estado actual de un proceso estocástico, entonces el comportamiento futuro de dicho proceso es independiente de su pasado. Un proceso \\(X_t\\) tiene la propiedad de Markov si la distribución condicional del proceso \\(X_t\\) dado el proceos en el instante \\(X_t=x\\), no depende de los valores pasados. Definición 3.11 \\(X\\) es un proceso de Markov si para cualquier \\(t\\) y \\(s&gt;0\\), \\[P(X_{t+s}\\leq y|\\mathcal{F}_t) = P(X_{t+s}\\leq y|X_t) \\text{ c.s.}\\] donde \\(\\mathcal{F}_t\\) es la \\(\\sigma\\)-álgebra generada por el proceso hasta el tiempo \\(t\\). Definición 3.12 La función de transición de probabilidad de un proceso \\(X\\) se define como \\[P(y,t,x,s) = P(X_y\\leq y|X_s\\leq x)\\] la función de distribución condicional del proceso en el instante \\(t\\), dado que éste está en el punto \\(x\\) en el instante \\(s&lt;t\\). La propiedad de Markov implica una expresión que resulta muy útil en términos de la esperanza condicional por la \\(\\sigma\\)-álgebra de eventos, la cual es válida tanto para procesos en tiempo discreto como en tiempo continuo. Las definiciones y propiedades anteriores son temas de estudio de gran importancia y con una amplia teoría matemática que está fuera del alcance de este libro, pero lo que hemos descrito es suficiente para el objetivo del mismo. 3.2 Modelos lineales Los modelos lineales proporcionan un enfoque natural que permite analizar el comportamiento de los procesos estocásticos o series de tiempo y en especial a lo referente a finanzas y economía. En esta sección discutiremos la estructura de dependencia, autocorrelación, modelización y predicción de los modelos lineales teóricos, con los correspondientes comandos en R para generar y nalaizar dichos procesos. 3.2.1 Proceso de Ruido Blanco Definición 3.13 Un proceso \\(\\{w_t\\}\\) se denomina ruido blanco (white noise) de media 0 y varianza \\(\\sigma^2\\) si satisface \\[\\begin{eqnarray*} \\mathbb{E}(w_t) &amp;=&amp; 0,\\quad Var(w_t)=\\sigma_w^2&lt;\\infty \\\\ Cov(w_t,w_{t-k}) &amp;=&amp; 0, \\forall k\\neq0 \\end{eqnarray*}\\] Las series de tiempo generadas de esta manera son muy usadas como modelos para ruido en aplicaciones de ingeniería. La designación “blanco” se origina de la analogía con la luz blanca e indica que todos los posibles períodos de oscilación están presentes con igual intensidad. En particular, una sucesión de variables aleatorias iid con media 0 y varianza \\(\\sigma_w^2\\) representa un caso especial de un proceso de ruido blanco. Este proceso lo denotaremos por \\(w_t\\sim WN(0,\\sigma_w^2)\\). Un muy usado ruido blanco es el ruido blanco gaussiano, donde las \\(w_t\\) son variables aleatorias normales o gaussianas con media 0 y varianza \\(\\sigma_w^2\\) y denotadas como \\(w_t\\sim iidN(0,\\sigma_w^2)\\). La función de media de un ruido blanco es trivial, es decir \\[\\mu_w=\\mathbb{E}(w_t)=0.\\] Calculemos la función de autocovarianza de \\(w_t\\) \\[\\begin{eqnarray*} \\gamma_w(s,t) &amp;=&amp; \\mathbb{E}[(w_s-\\mu_s)(w_t-\\mu_t)] \\\\ &amp;=&amp; \\mathbb{E}[w_sw_t] \\\\ &amp;=&amp; \\begin{cases} \\sigma_w^2, &amp;\\text{ si }s=t \\\\ 0, &amp;\\text{ si }s\\neq t \\end{cases} \\end{eqnarray*}\\] La última igualdad se sigue del hecho de que \\(w_s\\) y \\(w_t\\) son no-correlacionados para \\(s\\neq t\\) por lo que \\(\\mathbb{E}(w_sw_t) = \\mathbb{E}(w_s)\\mathbb{E}(w_t)=0\\). Ejemplo 3.3 (Estacionaridad de un ruido blanco) La función de autocovarianza de un ruido blanco es fácil de evaluar como \\[\\gamma_w(h) = \\mathbb{E}(w_{t+h}w_t) = \\begin{cases} \\sigma_w^2,&amp;\\text{ si }h=0\\\\ 0,&amp;\\text{ si }h\\neq0 \\end{cases}\\] donde \\(\\sigma_w^2\\) es la varianza del ruido blanco. Esto significa que la serie es débilmente estacionaria o estacionaria. Si las variables de ruido blanco también son gaussianas, el proceso es estrictamente estacionario, como se pueder ver evaluando (2.10) usando la relación (2.2). #----------------------------------------- # Ruidos blancos #----------------------------------------- # Uniforme [0,1] wu=runif(500,0,1) # Gaussiano wn=rnorm(500,0,1) # Graficos par(mfrow=c(2,1)) plot(wu,type = &quot;l&quot;,xlab = &quot;Num. de observaciones&quot;, main = &quot;Ruido blanco uniforme en [0,1]&quot;) plot(wn,type = &quot;l&quot;,xlab = &quot;Num. de observaciones&quot;, main = &quot;Ruido blanco gaussiano&quot;) # Funciones de autocovarianza (ACF) acf(wu) acf(wn) Ejemplo 3.4 Podemos reemplazar las series de ruido blanco \\(w_t\\) por un promedio móvil que suavice la serie. Por ejemplo, consideremos la serie \\(w_t\\) en la ecuación ( ) y reemplacémosla por un promedio móvil de 3 puntos, dado por \\[\\begin{equation} v_t = \\frac{1}{3}(w_{t-1}+w_t+w_{t+1}) \\tag{3.7} \\end{equation}\\] lo cual nos da una serie suavizada. Tomando la serie del ejemplo anterior y usando la función ‘filter’ de R se obtienen los gráficos siguientes: #------------------------------------------ # Promedio movil #------------------------------------------ # Uniforme vu=filter(wu,sides = 2,rep(1/3,3)) par(mfrow=c(2,1),mar=c(3,4,3,2))# plot.ts(wu,xlab=&quot; &quot;,ylab=&quot;Ruido blanco unif.&quot;) plot.ts(vu,ylim=c(0,1),ylab=&quot;Promedio móvil&quot;) # Gaussiano vn=filter(wn,sides = 2,rep(1/3,3)) par(mfrow=c(2,1),mar=c(3,4,3,2)) plot.ts(wn,xlab=&quot; &quot;,ylab=&quot;Ruido blanco gauss.&quot;) plot.ts(vn,ylim=c(-3,3),ylab=&quot;Promedio móvil&quot;) En la parte superior de cada uno se observan los ruidos blancos y en la parte inferior los respectivos promedios móviles. Podemos notar que las series de promedio móvil suavizan el comportamiento de las series originales, si tomamos más puntos en el promedio mayor será el suavizado. Ejemplo 3.5 (Función de media de un promedio móvil) Si \\(w_t\\) denota una serie de ruido blanco, entonces \\(\\mu_{wt}=\\mathbb{E}(w_t)=0\\) para todo \\(t\\). Luego para el promedio móvil de 3 puntos se tiene \\[\\mu_{wt} = \\mathbb{E}(v_t) = \\frac{1}{3}\\mathbb{E}(w_{t-1}+w_t+w_{t+1}) = \\frac{1}{3}(\\mathbb{E}(w_{t-1})+\\mathbb{E}(w_t)+\\mathbb{E}(w_{t+1}))=0.\\] Ejemplo 3.6 (Autocovarianza de un promedio móvil) Consideremos el promedio móvil de 3 puntos del ejemplo anterior y calculemos su función de autocovarianza \\[\\begin{eqnarray*} \\gamma_v(s,t) &amp;=&amp; \\mathbb{E}[(v_s-\\mu_s)(v_t-\\mu_t)] \\\\ &amp;=&amp; \\mathbb{E}[(v_s-o)(v_t-0)] \\\\ &amp;=&amp; \\frac{1}{9}\\mathbb{E}[(w_{s-1}+w_s+w_{s+1})(w_{t-1}+w_t+w_{t+1})] \\end{eqnarray*}\\] Consideremos \\(s-t=h\\), para \\(h=0,\\pm1,\\pm2,\\ldots\\). Entonces, tenemos para \\(h=0\\) \\[\\begin{eqnarray*} \\gamma_v(t,t) &amp;=&amp; \\frac{1}{9}\\mathbb{E}[(w_{t-1}+w_t+w_{t+1})(w_{t-1}+w_t+w_{t+1})] \\\\ &amp;=&amp; \\frac{1}{9}[\\mathbb{E}(w_{t-1}w_{t-1})+\\mathbb{E}(w_tw_t)+\\mathbb{E}(w_{t+1}w_{t+1})] \\\\ &amp;=&amp; \\frac{3}{9} \\end{eqnarray*}\\] Para \\(h=1\\), tenemos \\[\\begin{eqnarray*} \\gamma_v(t+1,t) &amp;=&amp; \\frac{1}{9}\\mathbb{E}[(w_t+w_{t+1}+w_{t+2})(w_{t-1}+w_t+w_{t+1})] \\\\ &amp;=&amp; \\frac{1}{9}[\\mathbb{E}(w_tw_t)+\\mathbb{E}(w_{t+1}w_{t+1})] \\\\ &amp;=&amp; \\frac{2}{9} \\end{eqnarray*}\\] Usando el hecho de que \\(\\mathbb{E}(w_tw_s)=0\\) si \\(s\\neq t\\). Cálculos similares nos dan \\(\\gamma_v(t-1,t)=2/9, \\gamma_v(t+2,t)=\\gamma_v(t-2,t)=1/9\\) y 0 para \\(h\\geq3\\). Resumiendo se tiene \\[\\begin{equation} \\gamma_v(s,t) = \\begin{cases} 3/9, &amp;\\text{ si }s=t\\\\ 2/9, &amp;\\text{ si }|s-t|=1\\\\ 1/9, &amp;\\text{ si }|s-t|=2\\\\ 0, &amp;\\text{ si }|s-t|\\geq3 \\end{cases} \\tag{3.8} \\end{equation}\\] Ejemplo 3.7 (Estacionaridad de un promedio móvil) El proceso de promedio móvil usado en los ejemplos 3.4 y 3.5 es estacionario ya que podemos escribir la función de autocovarianza obtenida en (3.8) como \\[\\gamma_v(h) = \\begin{cases} 3/9, &amp;\\text{ si }h=0\\\\ 2/9, &amp;\\text{ si }h=\\pm1\\\\ 1/9, &amp;\\text{ si }h=\\pm2\\\\ 0, &amp;\\text{ si }|h|\\geq3 \\end{cases}\\] Ejemplo 3.8 Un modelo para analizar tendencias es el camino aleatorio con tendencia dado por \\[\\begin{equation} X_t = \\delta+X_{t-1}+w_t \\tag{3.9} \\end{equation}\\] para \\(t=1,2,\\ldots,\\) con condición inicial \\(X_0=0\\), y donde \\(w_t\\) es un ruido blanco. La constante \\(\\delta\\) es llamada tendencia, y cuando \\(\\delta=0\\), (3.9) es llamado simplemente camino aleatorio. El término camino aleatorio viene del hecho de que cuando \\(\\delta=0\\) el valor de la serie de tiempo en tiempo \\(t\\) es el valor de la serie de tiempo al tiempo \\(t-1\\) más un movimiento completamente aleatorio determinado por \\(w_t\\). La expresión (3.9) la podemos reescribir como una suma de variables de ruido blanco, esto es, \\[\\begin{equation} X_t = \\delta t+\\sum_{j=1}^Nw_j \\tag{3.10} \\end{equation}\\] para \\(t=1,2,\\ldots.\\) A continuación generaremos un camino aleatorio usando R set.seed(154) w=rnorm(500,0,1) X=cumsum(w) wd=w+0.2; Xd=cumsum(wd) plot.ts(Xd,ylim=c(-40,80)) lines(X,col=&quot;red&quot;) lines(0.2*(1:500),lty=&quot;dashed&quot;,col=&quot;blue&quot;) Figura 3.1: Gráficos de caminos aleatorios: con tendencia (negro), sin tendencia (rojo) "],
["modelos-ar.html", "Capítulo 4 Modelos AR 4.1 Modelo AR(1) 4.2 Modelo AR(2) 4.3 Procesos AR(p) 4.4 Función de Autocorrelación Parcial 4.5 Criterios de Información 4.6 Estimación de Parámetros. 4.7 Predicciones con modelos AR", " Capítulo 4 Modelos AR Los modelos autoregresivos están basados en la idea de que el valor actual de la serie \\(x_t\\) se puede explicar como una función de \\(p\\) valores pasados \\(x_{t-1},x_{t-2},\\ldots,x_{t-p}\\) donde \\(p\\) determina el número de pasos en necesarios para predecir el valor actual. Una parte de las series de tiempo económicas y financieras suelen ser caracterizadas por los modelos autorregresivos. Entre los principales ejemplos de las finanzas tenemos valoración de precios y de dividendos, las tasas reales de cambio, tasas de interés y los diferenciales de tipos de interés (spreads). Definición 4.1 Un modelo autoregresivo de orden \\(p\\), abreviado \\(AR(p)\\) es de la forma \\[\\begin{equation} x_t=\\phi_1x_{t-1}+\\phi_2x_{t-2}+\\cdots+\\phi_px_{t-p}+w_t, \\tag{4.1} \\end{equation}\\] donde \\(x_t\\) es estacionario, \\(\\phi_1\\phi_2,\\ldots,\\phi_p\\) son constantes (\\(\\phi_p\\neq0\\)). A menos que se declare lo contrario, se asume que \\(w_t\\) es un ruido blanco gaussiano de media cero y varianza \\(\\sigma_w^2\\). La media de \\(x_t\\) es (4.1) es cero. Si la media \\(\\mu\\) de \\(x_t\\) no es cero, reemplazamos \\(x_t\\) por \\(x_t-\\mu\\) en (4.1), es decir \\[x_t-\\mu=\\phi_1(x_{t-1}-\\mu)+\\phi_2(x_{t-2}-\\mu)+\\cdots+\\phi_p(x_{t-p}-\\mu)+w_t\\] o escribimos \\[\\begin{equation} x_t=\\alpha+\\phi_1x_{t-1}+\\phi_2x_{t-2}+\\cdots+\\phi_px_{t-p}+w_t, \\tag{4.2} \\end{equation}\\] donde \\(\\alpha=\\mu(1-\\phi_1-\\phi_2-\\cdots-\\phi_p)\\). Note que (4.2) es similar al modelo de regresión dado en (2.54) y por consiguiente el término autoregresión. Sin embargo, se presentan algunas dificultades técnicas para la aplicación de este modelo, porque los regresores \\(x_{t-1},x_{t-2},\\ldots,x_{t-p}\\) son aleatorios, mientras que \\(x_t\\) se asume fijo. Una forma más útil se deriva de usar el siguietne operador de cambio dado por (2.42). Para escribir el modelo \\(AR(p)\\) como \\[\\begin{equation} (1-\\phi_1B-\\phi_2B^2-\\cdots-\\phi_pB^p)x_t=w_t \\tag{4.3} \\end{equation}\\] o más conciso como \\[\\begin{equation} \\phi(B)x_t=w_t. \\tag{4.4} \\end{equation}\\] Las propiedades de \\(\\phi(B)\\) son importantes para resolver (4.4). Esto nos lleva a la siguiente definición. Definición 4.2 El operador autoregresivo de orden \\(p\\) se define como \\[\\begin{equation} \\phi(B) = 1-\\phi_1B-\\phi_2B^2-\\cdots-\\phi_pB^p \\tag{4.5} \\end{equation}\\] 4.1 Modelo AR(1) Iniciaremos el estudio de los modelos \\(AR\\) considerando el modelo de primer orden \\(AR(1)\\), dado por \\(x_t=\\phi x_{t-1}+w_t\\). Iterando el operador de cambio \\(k\\) veces, obtenemos \\[\\begin{eqnarray*} x_t &amp;=&amp; \\phi x_{t-1}+w_t = \\phi(\\phi x_{t-2}+w_{t-1})+w_t \\\\ &amp;=&amp; \\phi^2x_{t-2}+\\phi w_{t-1}+w_t \\\\ &amp;\\vdots&amp; \\\\ &amp;=&amp; \\phi^kx_{t-k}+\\sum_{j=0}^{k-1}\\phi^jw_{t-j}. \\end{eqnarray*}\\] Este método sugiere que por iteración continua del operador de cambio, siempre que \\(|\\phi|&lt;1\\) y \\(x_t\\) sea estacionario, podemos representar un modelo \\(AR(1)\\) como un proceso lineal dado por 4 \\[\\begin{equation} x_t \\sum_{j=0}^{\\infty}\\phi^jw_{t-j} \\tag{4.6} \\end{equation}\\] El proceso \\(AR(1)\\) definido en (4.6) es estacionario con media \\[\\mathbb{E}(x_t) = \\sum_{j=0}^{\\infty}\\phi^j\\mathbb{E}(w_{t-j})=0,\\] y la función de autocovarianza es \\[\\begin{eqnarray} \\gamma(h) &amp;=&amp; Cov(x_{t+h},x_t) \\nonumber \\\\ &amp;=&amp; \\mathbb{E}\\left[\\left(\\sum_{j=0}^{\\infty}\\phi^jw_{t+h-j}\\right) \\left(\\sum_{k=0}^{\\infty}\\phi^kw_{t-k}\\right)\\right] \\nonumber \\\\ &amp;=&amp; \\sigma_w^2\\sum_{j=0}^{\\infty}\\phi^j\\phi^{j+h} = \\sigma_w^2\\phi^h\\sum_{j=0}^{\\infty}\\phi^{2j} \\nonumber \\\\ &amp;=&amp; \\frac{\\sigma_w^2\\phi^h}{1-\\phi^2}, h&gt;0 \\tag{4.7} \\end{eqnarray}\\] Recuerde que \\(\\gamma(h)=\\gamma(-h)\\) de modo que basta presentar la función de autocovarianza para \\(h\\geq0\\). Si en (4.7), hacemos \\(h=0\\), obtenemos la varianza del proceso \\(AR(1)\\), siendo esta \\[Var(x_t)=\\frac{\\sigma_w^2}{1-\\phi^2},\\] asumiendo que \\(\\phi_1^2&lt;1\\). El requisito de que \\(\\phi_1^2&lt;1\\) resulta del hecho de que la varianza de una variable aleatoria es acotada y no negativa. Por consiguiente, la estacionaridad de un modelo \\(AR(1)\\) implica que \\(-1&lt;\\phi_1&lt;1\\). Pero si \\(-1&lt;\\phi_1&lt;1\\), entonces por (4.6) y la independencia de \\(\\{w_t\\}\\) se puede demostrar que la media y la varianza de \\(x_t\\) son finitas. Además, por la desigualdad de Cauchy-Schwartz todas las autocovarianzas de \\(x_t\\) son finitas. Por lo tanto, el modelo \\(AR(1)\\) es estacionario. En resumen, una condición necesaria y suficiente para que un proceso \\(AR(1)\\) sea estacionario es \\(|\\phi_1|&lt;1\\). De (4.7) la ACF de un modelo \\(AR(1)\\) es \\[\\begin{equation} \\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\phi^h, \\quad h&gt;0 \\tag{4.8} \\end{equation}\\] y \\(\\rho(h)\\) satisface la recursión \\[\\begin{equation} \\rho(h) = \\phi\\rho(h-1)\\text{, para }h=1,2,\\ldots. \\tag{4.9} \\end{equation}\\] Las ecuaciones (4.8) y (4.9) indican que la ACF de un modelo \\(AR(1)\\) estacionario tiene un decaimiento exponencial con tasa igual a \\(\\phi_1\\). Si \\(\\phi_1&gt;0\\) el decaimiento es constante. Si por el contrario, \\(\\phi_1&lt;0\\) entonces el decaimiento es compuesto y se presenta de forma alternante con tasa \\(\\phi_1^2\\). Para tener una idea de esto, consideremos los modelos autoregresivos de orden 1 simulados, para distintos valores de \\(\\phi_1\\). # Coeficientes phi phi1=0.9 phi2=-0.8 phi3=0.4 phi4=-0.5 # Ruido blanco gaussiano w=rnorm(100,0,1) # Series AR(1) ar1_1=filter(w,filter = phi1,method = &quot;recursive&quot;) ar1_2=filter(w,filter = phi2,method = &quot;recursive&quot;) ar1_3=filter(w,filter = phi3,method = &quot;recursive&quot;) ar1_4=filter(w,filter = phi4,method = &quot;recursive&quot;) # Graficos par(mfrow=c(2,2)) plot.ts(ar1_1, col=&quot;blue&quot;,type = &quot;l&quot;, main = &quot;AR(1) con phi=0.9&quot;,xlab=&quot;t&quot;,ylab=&quot;x_t&quot;) plot.ts(ar1_2, col=&quot;blue&quot;,type = &quot;l&quot;, main = &quot;AR(1) con phi=-0.8&quot;,xlab=&quot;t&quot;,ylab=&quot;x_t&quot;) plot.ts(ar1_3, col=&quot;blue&quot;,type = &quot;l&quot;, main = &quot;AR(1) con phi=0.4&quot;,xlab=&quot;t&quot;,ylab=&quot;x_t&quot;) plot.ts(ar1_4, col=&quot;blue&quot;,type = &quot;l&quot;, main = &quot;AR(1) con phi=-0.5&quot;,xlab=&quot;t&quot;,ylab=&quot;x_t&quot;) Figura 4.1: Simulaciones de procesos autoregresivos de orden 1, AR(1), para distintos valores de phi A continuación mostramos las funciones de autocovarianzas de las series AR(1) simuladas anteriormente par(mfrow=c(2,2)) acf(ar1_1,type = &quot;covariance&quot;, main=&quot;ACF de la Serie AR(1) con phi=0.9&quot;) acf(ar1_2,type = &quot;covariance&quot;, main=&quot;ACF de la Serie AR(1) con phi=-0.8&quot;) acf(ar1_3,type = &quot;covariance&quot;, main=&quot;ACF de la Serie AR(1) con phi=0.4&quot;) acf(ar1_4,type = &quot;covariance&quot;, main=&quot;ACF de la Serie AR(1) con phi=-0.5&quot;) Figura 4.2: Funciones de autocovarianzas para las series AR(1) simuladas Ejemplo 4.1 (Modelo AR Explosivo y causalidad) Ya hemos descritos las condiciones para que un proceso \\(AR(1)\\) sea estacionario. Nos preguntamos si existe un proceso AR(1) estacionario con \\(|\\phi|&gt;1\\). Tal proceso será llamado explosivo porque los valores de la serie de tiempo se hacen grande en magnitud rápidamente. Claramente, porque \\(|\\phi|^j\\) crece sin acotación cuando \\(j\\to\\infty\\). Por otra parte, \\(\\sum_{j=0}^{k-1}\\phi^jw_{t-j}\\) no converge (en media cuadrado) cuando \\(k\\to\\infty\\), de modo que la idea intuitiva usada para obtener (4.6) no funciona acá directamente. Podemos modificar el argumento para obtener un modelo estacionario como sigue. Escribimos \\(x_{t+1}=\\phi x_t+w_{t+1}\\) en cuyo caso \\[\\begin{eqnarray} x_t &amp;=&amp; \\phi^{-1}x_{t+1}-\\phi^{-1}w_{t+1}=\\phi^{-1}(\\phi^{-1}x_{t+2}-\\phi^{-1}w_{t+2})-\\phi^{-1}w_{t+1} \\nonumber\\\\ &amp;\\vdots&amp; \\nonumber\\\\ &amp;=&amp; \\phi^{-1}x_{t+k}-\\sum_{j=1}^{k-1}\\phi^{-1}w_{t+j},\\tag{4.10} \\end{eqnarray}\\] iterando \\(k\\) pasos hacia adelante. Porque \\(|\\phi|^{-1}&lt;1\\), este resultado sugiere el modelo AR(1) estacionario que depende del futuro \\[x_t=-\\sum_{j=1}^{\\infty}\\phi^{-1}w_{t+j}\\] Podemos verificar que este modelo es estacionario y de la forma AR(1), \\(x_t=\\phi x_{t-1}+w_t\\). Desafortunadamente, el modelo es inútil porque requiere conocer el futuro para predecir el futuro. Cuando un proceso no depende del futuro, tal como un AR(1) con \\(|\\phi|&lt;1\\), decimos que el proceso es causal. En el caso explosivo de este ejemplo, el proceso es estacionario, pero también depende del futuro, y no causal. 4.2 Modelo AR(2) Un proceso \\(AR(2)\\) tiene la forma general \\[\\begin{equation} x_t = \\alpha + \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + w_t \\tag{4.11} \\end{equation}\\] siendo \\(\\alpha = \\mu(1-\\phi_1-\\phi_2)\\), con \\(\\phi_2\\neq 0\\). Podemos calcular su función de media \\[\\begin{eqnarray*} \\mathbb{E}(x_t) &amp;=&amp; \\mathbb{E}(\\alpha + \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + w_t) \\\\ &amp;=&amp; \\alpha+\\phi_1\\mathbb{E}(x_{t-1})+\\phi_2\\mathbb{E}(x_{t-2}) \\end{eqnarray*}\\] Por estacionalidad, se tiene que \\(\\mathbb{E}(x_t)=\\mathbb{E}(x_{t-1})=\\mathbb{E}(x_{t-2})\\), luego \\[\\mathbb{E}(x_t)(1-\\phi_1-\\phi_2) = \\alpha\\] Así, \\(\\mathbb{E}(x_t) = \\frac{\\alpha}{1-\\phi_1-\\phi_2}\\), siempre que \\(\\phi_1+\\phi_2\\neq1\\). Usando \\(\\alpha=(1-\\phi_1-\\phi_2)\\mu\\) podemos reescribir el proceso \\(AR(2)\\) como \\[x_t-\\mu = \\phi_1(x_{t-1}-\\mu)+\\phi_2(x_{t-2}-\\mu)+w_t.\\] Multiplicando por \\(x_{t-h}-\\mu\\), tenemos \\[(x_{t-h}-\\mu)(x_t-\\mu) = \\phi_1(x_{t-h}-\\mu)(x_{t-1}-\\mu) + \\phi_2(x_{t-h}-\\mu)(x_{t-2}-\\mu) + (x_{t-h}-\\mu)w_t.\\] Tomando valor esperado y usando el hecho de que \\(\\mathbb{E}[(x_{t-h}-\\mu)w_t]=0\\), para \\(h&gt;0\\), obtenemos \\[\\begin{equation} \\gamma(h) = \\phi_1\\gamma(h-1)+\\phi_2\\gamma(h-2) \\text{, para }h&gt;0. \\tag{4.12} \\end{equation}\\] Este último resultado se conoce como la ecuación de momentos de un proceso estacionario \\(AR(2)\\). Dividiendo (4.12) por \\(\\gamma(0)\\), tenemos la propiedad \\[\\begin{equation} \\rho(h) = \\phi_1\\rho(h-1)+\\phi_2\\rho(h-2)\\text{, para }h&gt;0 \\tag{4.13} \\end{equation}\\] para la ACF de \\(x_t\\). En particular, para paso 1 (\\(h=1\\)) la ACF satisface \\[\\rho(1) = \\phi_1\\rho(0)+\\phi_2\\rho(-1) = \\phi_1+\\phi_2\\rho(1)\\] Por lo tanto, para un proceso \\(AR(2)\\) estacionario \\(x_t\\), tenemos \\[\\begin{eqnarray*} \\rho(0) &amp;=&amp; 1 \\\\ \\rho(1) &amp;=&amp; \\frac{\\phi_1}{1-\\phi_2} \\\\ \\rho(h) &amp;=&amp; \\phi_1\\rho(h-1)+\\phi_2\\rho(h-2),\\quad h\\geq2 \\end{eqnarray*}\\] El resultado de la ecuación (4.13) nos dice que la ACF de un proceso estacionario \\(AR(2)\\) satisface la ecuación en diferencias de segundo orden \\[\\begin{equation} (1-\\phi_1B-\\phi_2B^2)\\rho(h) = 0 \\tag{4.14} \\end{equation}\\] donde \\(B\\) es el operador definido en (2.42). La ecuación (4.14) determina las propiedades de la ACF de un proceso \\(AR(2)\\) estacionario. También determina el comportamiento de los pronósticos de \\(x_t\\). Correspondiendo a la ecuación en diferencias anterior, existe una ecuación polinómica de segundo orden \\[\\begin{equation} x^2-\\phi_1x-\\phi_2=0 \\tag{4.15} \\end{equation}\\] Las soluciones de esta ecuación son las raíces características de un proceso \\(AR(2)\\) y estas son \\[x=\\frac{\\phi_1\\pm\\sqrt{\\phi_1^2+4\\phi_2}}{2}\\] Denotamos las dos raíces por \\(r_1\\) y \\(r_2\\). Si ambos son reales, entonces la ecuación en diferencias de segundo orden la podemos factorizar como \\[(1-r_1B)(1-r_2B)\\] y el proceso \\(AR(2)\\) lo podemos considerar como un proceso \\(AR(1)\\) operando sobre otro proceso \\(AR(1)\\). La ACF de \\(x_t\\) es entonces una mezcla de dos decaimientos exponenciales. Pero si \\(\\phi_1^2+4\\phi_2&lt;0\\), entonces \\(r_1\\) y \\(r_2\\) son raíces complejas conjugadas, y el gráfico de la ACF de \\(x_t\\) mostrará un amortiguamiento de senos y cosenos. En aplicaciones financieras y económicas, las raíces caracteríticas complejas son importantes. Dan lugar al comportamiento de los ciclos económicos. Por lo tanto, es común que los modelos económicos de series de tiempo tengan raíces características de valor complejo. Para un proceso \\(AR(2)\\) dado por (4.11) con raíces características complejas, la longitud promedio de un ciclo estocástico es \\[k=\\frac{360°}{\\arccos(\\phi_1/2\\sqrt{-\\phi_2})},\\] donde el arcocoseno está expresado en grados. La figura siguiente muestra la ACF de 4 procesos estacionarios \\(AR(2)\\). Los procesos \\(AR(2)\\) mostrados son: \\(x_t=1.2x_{t-1}-0.35x_{t-2}+w_t\\) \\(x_t=0.6x_{t-1}-0.4x_{t-2}+w_t\\) \\(x_t=0.2x_{t-1}+0.35x_{t-2}+w_t\\) \\(x_t=-0.2x_{t-1}+0.35x_{t-2}+w_t\\) Figura 4.3: ACF de 4 procesos estacionarios AR(2) Figura 4.4: ACF de 4 procesos estacionarios AR(2) La serie (b) tiene raíces características complejas, en efecto \\[\\phi_1^2+4\\phi_2=(0.6)^2+4\\times(-0.4)-1.24&lt;0\\] Se puede notar que n el gráfico de la ACF que este exhibe un comportamiento de ondas de senos y cosenos. Los otros 3 procesos \\(AR(2)\\) tienen raíces características reales, por lo que las ACF decaen exponencialmente. La condición de estacionaridad de un proceso \\(AR(2)\\) es que los valores absolutos de sus raíces características sean menor que uno, esto es \\(|\\phi_1|&lt;1, |\\phi_2|&lt;1\\). Bajo esta condición, la ecuación recursiva (4.13) asegura que la ACF del proceso converge a cero cuando el salto \\(h\\) crece. Esta propiedad de convergencia es una condición necesaria para una serie de tiempo estacionaria. De hecho, la condición también aplica para un proceso \\(AR(1)\\) donde la ecuación polinómica es \\(x-\\phi_1=0\\). La raíz característica es \\(x=\\phi_1\\), la cual debe ser menor que uno en módulo para que \\(x_t\\) sea estacionario. Como mostramos antes, para un proceso estacionario \\(AR(1)\\) la ACF es \\(\\rho(h)=\\phi^h\\), (4.8). Así, la condición \\(|\\phi|&lt;1\\), asegura que \\(\\rho(h)=\\phi^h\\to0\\), cuando \\(h\\to\\infty\\). 4.3 Procesos AR(p) Los resultados de los procesos \\(AR(1)\\) y \\(AR(2)\\), los podemos generalizar a procesos \\(AR(p)\\). Así, la función de media del proceso \\(AR(p)\\) estacionario será \\[\\begin{equation} \\mathbb{E}(x_t) = \\frac{\\alpha}{1-\\phi_1-\\cdots-\\phi_p} \\tag{4.16} \\end{equation}\\] siempre que el denominador sea distinto de cero. La ecuación polinómica asociada al modelo es \\[\\begin{equation} x^p-\\phi_1x^{p-1}-\\phi_2x^{p-2}-\\cdots-\\phi_p=0 \\tag{4.17} \\end{equation}\\] la cual nos referimos como la ecuación característica del modelo. Si todas las raíces características de esta ecuación son menores qye uno en módulo, esto es \\(|r_j|&lt;1\\), con \\(j=1,\\ldots,p\\), entonces la serie \\(x_t\\) es estacionaria. Para un proceso \\(AR(p)\\) estacionario, la ACF satisface la ecuación en diferencias \\[(1-\\phi_1B-\\phi_2B^2-\\cdots-\\phi_pB^p)\\rho(h)=0\\text{, para }h&gt;0.\\] El gráfico de la ACF de un proceso \\(AR(p)\\) estacionario mostrará una mezcla de ondas de senos y cosenos con decaimientos exponenciales dependiendo de la naturaleza de sus raíces características. Ejemplo 4.2 Consideremos el modelo \\(AR(3)\\) de la forma \\[x_t=0.0047+0.35x_{t-1}+0.18x_{t-2}-0.14x_{t-3}+w_t.\\] Reescribiendo el proceso como \\[x_t-0.35x_{t-1}-0.18x_{t-2}+0.14x_{t-3}=0.0047+w_t\\] obtenemos la correspondiente ecuación en diferencias de orden 3, \\[(1-0.35B-0.18B^2+0.14B^3)=0\\] la cual podemos factorizar como \\[(1+0.52B)(1-0.87B+0.27B^2)=0\\] El primer factor \\((1+0.52B)=0\\), muestra u ndecaimiento exponencial en la ACF. Veamos ahora el segundo factor \\((1-0.87B-(-0.27)B^2)=0\\), tenemos que \\(\\phi_1^2+4\\phi_2=(0.87)^2+4(-0.27)=-0.3231&lt;0\\). Por consiguiente la ACF mostrará un comportamiento en ondas de senos y cosenos. xt&lt;-arima.sim(list(order=c(3,0,0),ar=c(0.35,0.18,-0.14)),n=100) par(mfrow=c(2,1)) plot(xt,type=&quot;l&quot;,main=&quot;Proceso AR(3)&quot;) acf(xt) En aplicaciones, se desconoce el orden \\(p\\) de una serie de tiempo autoregresiva, por lo que debe especificarse empíricamente. Esto se conoce como la determinación del orden de los modelos \\(AR\\). Existen dos enfoques generales para determinar el valor de \\(p\\). El primero es utilizar la función de autocorrelación parcial, y el segundo utilizar alguna función de criterio de información. En R existe una función para determinar el orden \\(p\\) de un proceso \\(AR\\), sin embargo vamos a estudiar primero los dos enfoques mencionados de manera de poder entender como funciona R al respecto. 4.4 Función de Autocorrelación Parcial La función de autocorrelación parcial PACF (siglas en inglés: Partial Autocorrelation Function) de una serie de tiempo es una función de su ACF y es una herramienta útil para determinar el orden \\(p\\) de un modelo autoregresivo. Una manera simple pero efectiva de introducir la PACF es considerando los siguientes modelos \\(AR\\) en órdenes consecutivos: \\[\\begin{eqnarray*} x_t &amp;=&amp; \\phi_{0,1}+\\phi_{1,1}x_{t-1}+w_{1t} \\\\ x_t &amp;=&amp; \\phi_{0,2}+\\phi_{1,2}x_{t-1}+\\phi_{2,2}x_{t-2}+w_{2t} \\\\ x_t &amp;=&amp; \\phi_{0,3}+\\phi_{1,3}x_{t-1}+\\phi_{2,3}x_{t-2}+\\phi_{3,3}x_{t-3}+w_{2t} \\\\ \\vdots &amp; &amp; \\\\ \\end{eqnarray*}\\] donde \\(\\phi_{0,j}, \\phi_{i,j}, \\{w_{jt}\\}\\) son respectivamente, el término constante, el coeficiente de \\(x_{t-j}\\) y el término de error del modelo \\(AR(j)\\). Estos modelos están en la forma de regresión lineal múltiple y se pueden estimar por mínimos cuadrados. La estimación de \\(\\hat{\\phi}_{1,1}\\) de la primera ecuación se llama PACF muestral de paso 1 de \\(x_t\\). LA estimación de \\(\\hat{\\phi}_{2,2}\\) de la segunda ecuación es la PACF muestral de paso 2 de \\(x_t\\). La estimación \\(\\hat{\\phi}_{3,3}\\) de la tercera ecuación es la PACF muestral de paso 3 de \\(x_t\\), y así sucesivamente. De la definición, la PACF de paso 2 muestra la contribución añadida de \\(x_{t-2}\\) a \\(x_t\\) sobre el modelo \\(AR(1)\\) \\(x_t=\\phi_0+\\phi_1x_{t-1}+w_{1t}\\). La PACF de paso 3 muestra la contribución añadida de \\(x_{t-3}\\) a \\(x_t\\) sobre un modelo \\(AR(2)\\), etc. Por lo tanto, para un modelo \\(AR(p)\\), la PACF de paso \\(p\\) no debería ser cero, sino que \\(\\hat{\\phi}_{j,j}\\) debería ser cercano a cero para todo \\(j&gt;p\\). Esta propiedad será útil para determinar el orden \\(p\\). De hecho, bajo ciertas condiciones de regularidad, se puede demostrar que la PACF muestral de un proceso \\(AR(p)\\) tiene las siguientes propiedades: \\(\\hat{\\phi}_{p,p}\\) converge a \\(\\phi_p\\) cuando el tamaño \\(N\\) de la muestra tiende a infinito. \\(\\hat{\\phi}_{l,l}\\) converge a cero para todo \\(l&gt;p\\). La varianza asintótica de \\(\\hat{\\phi}_{l,l}\\) es \\(1/N\\) para \\(l&gt;p\\). Estos resultados dicen que para una serie \\(AR(p)\\) la PACF muestral se corta en paso o salto \\(p\\). Para concluir esta sección, destaquemos que tanto la función de autocorrelación (ACF) como la función de autocorrelación parcial (PACF) nos permiten determinar el orden de un modelo \\(AR(p)\\). El cuadro siguiente indica como usarlas ACF PACF AR(p) Disminución gradual Corte en paso p Ejemplo 4.3 (La PACF de un AR(1) causal) Considere la PACF de un proceso AR(1) dado por \\(x_t=\\phi x_{t-1}+w_t\\) con \\(|\\phi|&lt;1\\). Por definición, \\(\\phi_{1,1}=\\rho(1)=\\phi\\). Para calcular \\(\\phi_{2,2}\\) considere la regresión de \\(x_2\\) en \\(x_1\\), \\(x_2^1=\\beta x_1\\). Minimicemos \\(\\beta\\) \\[\\mathbb{E}(x_2-\\beta x_1)^2 = \\gamma(0)-2\\beta\\gamma(1)+\\beta^2\\gamma(0).\\] Derivando e igualando a cero, tenemos \\(\\beta=\\gamma(1)/\\gamma(0)=\\rho(0)=\\phi\\). Entonces \\(x_2^1=\\phi x_1\\). Ahora, consideremos la regresión de \\(x_0\\) en \\(x_1\\), \\(x_0^1=\\beta x_1\\). Nuevamente, minimizamos \\(\\beta\\) \\[\\mathbb{E}(x_0-\\beta x_1)^2 = \\gamma(0)-2\\beta\\gamma(1)+\\beta^2\\gamma(0).\\] Esta ecuación es la misma que la anterior, por lo que \\(\\beta=\\phi\\) y \\(x_0^1=\\phi x_1\\). Por consiguiente, \\(\\phi_{22}=\\text{corr}(x_2-\\phi x_1,x_0-\\phi x_1)\\). Pero, note que \\[\\text{cov}(x_2-\\phi x_1,x_0-\\phi x_1)=\\gamma(2)-2\\phi\\gamma(1)+\\phi^2\\gamma(0)=0\\] porque \\(\\gamma(h)=\\gamma(0)\\phi^h\\). Entonces \\(\\phi_{2,2}=0\\). Ejemplo 4.4 (La PACF de un AR(p) causal) Sea \\(x_t=\\sum_{j=1}^{p}\\phi_jx_{t-j}+w_j\\) donde las raíces de \\(\\phi(z)\\) están fuera del círculo unitario. En particular, \\(x_h=\\sum_{j=1}^{p}\\phi_jx_{h-j}+w_h\\). Cuando \\(h&gt;p\\), la regresión de \\(x_h\\) en \\(x_{h-1},x_{h-2},\\ldots,x_1\\) es \\[x_h^{h-1}=\\sum_{j=1}^{p}\\phi_jx_{h-j}\\] Este resultado se demostrará en la sección “Pronósticos” del capítulo “Modelos ARMA”. Entonces, cuando \\(h&gt;p\\), \\[\\begin{eqnarray*} \\phi_{hh} &amp;=&amp; \\text{corr}(x_h-x_h^{h-1},x_0-x_0^{h-1}) \\\\ &amp;=&amp; \\text{corr}(w_h,x_0-x_0^{h-1}) \\end{eqnarray*}\\] ya que, por causalidad \\(x_0-x_0^{h-1}\\) depende sólo de \\(\\{w_{h-1},w_{h-2},\\ldots\\}\\). Cuando \\(h\\leq p\\), \\(\\phi_{pp}\\) no es cero y \\(\\phi_{11},\\phi_{22},\\ldots,\\phi_{p-1,p-1}\\) no son necesariamente ceros. La figura 4.5 muestra las ACF y PACF del modelo AR(2) dado por \\[x_t=1.5x_{t-1}-0.75x_{t-2}+w_t.\\] ACF=ARMAacf(ar=c(1.5,-0.75),ma=0,25) PACF=ARMAacf(ar=c(1.5,-0.75),ma=0,25,pacf=T) par(mfrow=c(1,2)) plot(ACF,type=&quot;h&quot;,ylim=c(-0.8,1),xlab=&quot;Salto&quot;) abline(h=0) plot(PACF,type=&quot;h&quot;,ylim=c(-0.8,1),xlab=&quot;Salto&quot;) abline(h=0) Figura 4.5: Las ACF y PACF de un modelo AR(2) con phi_1=1.5 y phi_2=-0.75 4.5 Criterios de Información Existen diversos criterios de información disponibles para determinar el orden \\(p\\) de un proceso autoregresivo. Todos ellos están basados en verosimilitud. El primer criterio ya lo definimos en 2.16 Definición 4.3 (Criterio de Información de Akaike (AIC)) El Criterio de Información de Akaike se define como \\[\\begin{equation} AIC = \\ln\\hat{\\sigma}_k^2+\\frac{n+2k}{n} \\tag{2.71} \\end{equation}\\] donde \\(\\hat{\\sigma}_k^2\\) es el estimador de máxima verosimilitud para la varianza, \\(k\\) es el número de parámetros en el modelo y \\(n\\) es el tamaño de la muestra 5 El Criterio de Información de Akaike (AIC) es una medida de la calidad relativa de un modelo estadístico, para un conjunto dado de datos. Como tal, el AIC proporciona un medio para la selección del modelo. El AIC maneja un trade-off 6 entre la bondad de ajuste del modelo y la complejidad del modelo. Se basa en la entropía de información: se ofrece una estimación relativa de la información perdida cuando se utiliza un modelo determinado para representar el proceso que genera los datos. El AIC no proporciona una prueba de un modelo en el sentido de probar una hipótesis nula, es decir el AIC puede decir nada acerca de la calidad del modelo en un sentido absoluto. Si todos los modelos candidatos encaja mal, el AIC nodará ningún aviso de ello. El valor de \\(k\\) que minimiza el AIC especifica el mejor modelo. La idea es que la minimización de \\(\\hat{\\sigma}_k^2\\) sea razonablemente objetiva, excepto que decrezca monótonamente cuando \\(k\\) crece. Por lo tanto, debemos penalizar la variación del error por un término proporcional al número de parámetros. La elección del término de penalización dado por (2.71) no es único. En el criterio AIC definido en la ecuación (2.71), el sesgo es aproximado por el número de parámetros los cuales son constantes y no tienen variabilidad. Para el modelo de regresión, la corrección del sesgo el logaritmo de la verosimilitud se define como Definición 4.4 (AIC con sesgo corregido (AICc)) \\[\\begin{equation} AICc = \\ln\\hat{\\sigma}_k^2+\\frac{n+k}{n-k-2} \\tag{4.18} \\end{equation}\\] donde \\(\\hat{\\sigma}_k^2\\) es el estimador de máxima verosimilitud de la varianza, \\(k\\) es el número de parámetros en el modelo y \\(n\\) es el tamaño de la muestra. Este modelo fue propuesto originalmente por N. Sugiura en el artículo “Further analysis of the data by Akaike’s information criterion and the finite corrections”. Communications in Statistics, Theory and Methods, Vol. 7, No. 1, pp. 13-26, 1978. Se tiene que el AICc es esencialmente el AIC con un término de penalización adicional para el número de parámetros. Nótese que cuando \\(n\\to\\infty\\), el término de penalización adicional converge a 0, y por lo tanto el AICc converge al AIC. De manera similar que en el AIC, se selecciona el modelo con el menor valor AICc. Definición 4.5 (Criterio de Información de Schwarz (SIC)) \\[\\begin{equation} SIC = \\ln\\hat{\\sigma}_k^2+\\frac{k\\ln n}{n} \\tag{4.19} \\end{equation}\\] usando la misma notación que en la definición 4.4. SIC también llamado Criterio Bayesiano de Información (BIC) es un criterio para la selección de modelos. Se basa, en parte, en la función de probabilidad y está estrechamente relacionado con el Criterio de Información de Akaike (AIC). Cuando se hace ajuste de modelos, es posible aumentar la probabilidad mediante la adición de parámetros, pero esto puede resultar en sobreajuste. Tanto BIC como AIc resuelven este problema mediante la introducción de un término de penalización para el núemro de parámetros en el modelo, el término de penalización es mayor en el BIC que en el AIC. El BIC fue desarrollado por Gideon E. Schwarz, quien dio un argumento bayesiano a favor de su adopción. Akaike también desarrolló su propio formalismo bayesiano, que ahora se conoce como la ABIC (siglas en inglés) por Criterio de Información Bayesiano de Akaike. Ejemplo 4.5 Consideremos el archivo de datos “recruit.txt”, consistente del número de nuevos peces en el Pacífico Central relacionado con el Índice de Oscilación del Sur (SOI) que mide los cambios en la presión del aire relativo a la temperatura de la superficie del mar. Son 453 registros de 1950 a 1987 tomados mensualmente. La ACF y la PACF indican que el modelo que mejor se ajusta es un \\(AR(2)\\). La ACF tiene ciclos correspondientes a aproximadamente períodos de 12 meses y la PACF tiene valores grandes para \\(h=1,2\\) y es esencialmente cero para paso de orden mayor. rec=ts(scan(&quot;data/recruit.txt&quot;),start = 1950, frequency = 12) par(mfrow=c(3,1)) plot(rec,type = &quot;l&quot;, ylab = &quot;&quot;, xlab = &quot;meses&quot;, main = &quot;Número de nuevos peces en el Pacífico Central (1950-1987)&quot;) acf(rec,48) pacf(rec,48) 4.6 Estimación de Parámetros. Para un modelo \\(AR(p)\\) especificado por (4.1), el método de mínimos cuadrados condicional, el cual inicia con \\(p+1\\) observaciones, se usa a menudo para estimar los parámetros. Específicamente, condicionando sobre las primeras \\(p\\) observaciones, tenemos \\[x_t=\\phi_0+\\phi_1x_{t-1}+\\cdots+\\phi_px_{t-p}+w_t,\\quad t=p+1,\\ldots,N,\\] el cual se puede estimar por mínimos cuadrados. Denotemos los estimadores de \\(\\phi_i\\) por \\(\\hat{\\phi}_i\\). El modelo ajustado será \\[\\hat{x}_t = \\hat{\\phi}_0+\\hat{\\phi}_1x_{t-1}+\\cdots+\\hat{\\phi}_px_{t-p}\\] y el residual asociado es \\[\\hat{w}_t=x_t-\\hat{x}_t\\] La serie \\(\\{\\hat{w}_t\\}\\) se llama serie residual, de la cual obtenemos \\[\\hat{\\sigma}_w^2 = \\frac{\\sum_{t=p+1}^N\\hat{w}_t^2}{N-2p-1}\\] Debemos examinar con cuidado un modelo ajustado para verificar si es adecuado o no. Si el modelo es adecuado, entonces la serie residual debe comportarse como un ruido blanco. La ACF y el estadístico de Ljung-Box 7 de los residuos son útiles para comprobar la cercanía de \\(\\hat{w}_t\\) a un ruido blanco. Para un modelo \\(AR(p)\\), el estadístico de Ljung-Box \\(Q(m)\\) se distribuye asintóticamente como una Chi-Cuadrado con \\(m-p\\) grados de libertad, para significar que los \\(p\\) coeficientes del proceso AR son estimados. Si descubirmos que un modelo ajustado es inadecuado, debemos refinarlo. 4.7 Predicciones con modelos AR La predicción es una aplicación importante del análisis de series de tiempo. Para el modelo \\(AR(p)\\) en la ecuación (4.1), supongamos que estamos en tiempo \\(m\\) y estamos interesados en predecir \\(x_{m+h}\\), donde \\(h\\geq1\\). El tiempo \\(m\\) se llama el origen de predicción y el entero \\(h\\) es el horizonte de predicción. Sea \\(\\hat{x}_m(h)\\) la predicción de \\(x_{m+h}\\) usando la función de pérdida de error mínimo al cuadrado. En otras palabras, escogemos la predicción \\(\\hat{x}_k(h)\\) tal que \\[\\mathbb{E}(x_{m+h}-\\hat{x}_m(h))\\leq\\min_g\\mathbb{E}(x_{m+h}-g)^2\\] donde \\(g\\) es una función de información disponible pen tiempo \\(m\\). Nos referimos a \\(\\hat{x}_m(h)\\) como la predicción de \\(h\\) pasos de \\(x_t\\) con origen de predicción \\(m\\). 4.7.1 Predicción de un paso Del modelo \\(AR(p)\\) tenemos \\[x_{m+1}=\\phi_0+\\phi_1x_m+\\cdots+\\phi_px_{m+1-p}+w_{m+1}\\] Bajo la función de pérdida de error mínimo al cuadrado, la predicción puntual \\(x_{m+1}\\) dado el modelo y las observaciones hasta tiempo \\(m\\), es el valor esperado condicional \\[\\hat{x}_m(1) = \\mathbb{E}(x_{m+1}|x_m,x_{m-1},\\ldots) = \\phi_0+\\sum_{i=1}^p\\phi_ix_{m+1-i}\\] y el error de predicción asociado es \\[e_m(1)=x_{m+1}-\\hat{x}_m(1)=w_{m+1}\\] Consecuentemente, la varianza del error de predicción de un paso es \\(Var(e_m(1))=Var(w_{m+1})=\\sigma_w^2\\). Si \\(w_t\\) se distribuye como una normal, entonces un intervalo de predicción de un paso del 95% de confianza está dado por \\[\\hat{x}_m(1)\\pm1.96\\sigma_w^2.\\] 4.7.2 Predicción de dos pasos Ahora consideremos la predicción de \\(x_{m+2}\\) con origen de predicción \\(m\\). De un modelo \\(AR(p)\\), tenemos \\[x_{m+2} = \\phi_0+\\phi_1x_{m+1}+\\cdots+\\phi_px_{m+2-p}+w_{m+2}\\] Tomando valor esperado condicional, tenemos \\[\\begin{eqnarray*} \\hat{x}_m(2) &amp;=&amp; \\mathbb{E}(x_{m+2}|x_m,x_{m-1},\\ldots) \\\\ &amp;=&amp; \\phi_0+\\phi_1\\hat{x}_m(1)+\\phi_2x_m+\\cdots+\\phi_px_{m+2-p} \\end{eqnarray*}\\] y el error de predicción asociado es \\[\\begin{eqnarray*} e_m(2) &amp;=&amp; x_{m+2}-\\hat{x}_m(2) = \\phi_1[x_{m+1}-\\hat{x}_m(1)]+w_{m+2} \\\\ &amp;=&amp; w_{m+2}-\\phi_1w_{m+1} \\end{eqnarray*}\\] La varianza del error de predicción es \\(Var(e_m(2))=(1+\\phi_1^2)\\sigma_w^2\\). Un intervalo de predicción para \\(x_{m+2}\\) lo podemos calcular de la misma manera que para \\(x_{m+1}\\). Es interesante notar que \\(Var(e_m(2))\\geq Var(e_m(1))\\), lo que indica que cuando el horizonte de predicción crece la incertidumbre de la predicción también crece. 4.7.3 Predicción de múltiples pasos En general, tenemos \\[x_{m+h}=\\phi_0+\\phi_1x_{m+h-1}+\\cdots+\\phi_px_{m+h-p}+w_{m+h}\\] La predicción de \\(h\\) pasos basado en la función de pérdida de error mínimo al cuadrado es el valor epsrado condicional de \\(x_{m+h}\\) dado \\(\\{x_{m-i}\\}_{i=0}^{\\infty}\\), el cual podemos escribir como \\[\\hat{x}_m(h) = \\phi_0+\\sum_{i=1}^p\\phi_1\\hat{x}_m(h-i)\\] donde se entiede que \\(\\hat{x}_m(i)=x_{m+i}\\) si \\(i&lt;0\\). Esta predicción se puede calcular recursivamente usando las predicciones \\(\\hat{x}_m(i)\\) para \\(i=1,\\ldots,h-1\\). El error de predicción de paso \\(h\\) es \\(e_m(h)=x_{m+h}-\\hat{x}_m(h)\\). Se puede demostrar que para un proceso \\(AR(p)\\) estacionario, \\(\\hat{x}_m(h)\\) converge a \\(\\mathbb{E}(x_t)\\) cuando \\(h\\to\\infty\\), esto es, para una serie \\(AR(p)\\) estacionaria, la predicción a largo plazo se aproxima a su media incondicional. Esta propiedad se conoce como la reversión media en la literatura financiera. La desviación del error de predicción se aproxima entonces a la desviación incondicional de \\(x_t\\). Ejemplo 4.6 Realicemos una predicción para la serie de nuevos peces dado en el ejemplo 4.5. Para ello usaremos la función “predict” de R. Vamos a hacer una predicción de 24 meses. Del ejemplo 4.5, pudimos notar en las ACF y PACF que un modelo que se ajusta a esta serie es un proceso \\(AR(2)\\), así que lo primero que hacemos es ajustar el modelo, y luego calculamos la predicción. Al realizar el gráfico de la serie y la predicción notamos que después de 12 meses, la predicción converge a la media de la serie tal como se describió previamente. # Ajuste AR(2) regr=ar.ols(rec,order=2,demean = FALSE, intercept = TRUE) regr$asy.se.coef ## $x.mean ## [1] 1.111 ## ## $ar ## [1] 0.04179 0.04188 # Prediccion fore=predict(regr, n.ahead=24) ## Warning in object$var.pred * vars: Recycling array of length 1 in array-vector arithmetic is deprecated. ## Use c() or as.vector() instead. ts.plot(rec,fore$pred,col=1:2,xlim=c(1980,1990), xlab=&quot;Años&quot;, ylab=&quot;Nuevos peces&quot;) lines(fore$pred,type = &quot;p&quot;,col=2) lines(fore$pred+fore$se,lty = &quot;dashed&quot;,col=4) lines(fore$pred-fore$se,lty = &quot;dashed&quot;,col=4) Note que \\(\\lim_{k\\to\\infty}\\mathbb{E}(x_t-\\sum_{j=0}^{\\infty}\\phi^jw_{t-j})^2 = \\lim_{k\\to\\infty}\\phi^{2k}\\mathbb{E}(x_{t-k}^2)=0\\), de modo que (4.6) existe en el sentido de media cuadrado.↩ Formalmente, el AIC se define como \\[-2\\ln L_k+2k\\] donde \\(L_k\\) es la verosimilud maximizada y \\(k\\) es el número de parámetros del modelo.↩ Trade-off o simplemente tradeoff , en algunas ocasiones traducido al español como sacrificio, es un anglicismo que describe una situación en la cual se debe perder cierta cualidad a cambio de otra cualidad. Un tradeoff se puede dar por varias razones, entre ellas por simples limitaciones de la física (dentro de una cantidad de espacio dada se pueden meter muchos objetos pequeños o una menor cantidad de objetos grandes). La idea de un tradeoff como una decisión por lo general implica que ésta es realizada con una compresión total de las ventajas y desventajas de la decisión en particular, como por ejemplo, es el caso cuando una persona decide invertir en acciones de una empresa (una inversión más riesgosa pero con mayor potencial) sobre bonos (por lo general más seguros pero con menor potencial de ganancias).↩ El estadístico de Ljung-Box está dado por \\[Q(m) = N(N+2)\\sum_{i=1}^m\\frac{\\hat{\\rho}_i^2}{N-i}\\] donde \\(\\hat{\\rho}_1,\\hat{\\rho}_2,\\ldots\\), son las funciones de autocorrelación muestral (ACF) de \\(x_t\\), \\(N\\) el tamaño de la muestra.↩ "],
["modelos-ma.html", "Capítulo 5 Modelos MA 5.1 Propiedades de los modelos MA 5.2 Identificación del orden de un MA 5.3 Estimación 5.4 Predicciones usando modelos MA", " Capítulo 5 Modelos MA En este capítulo describiremos otra clase de modelos simples que también son útiles en el modelado de series de retorno en finanzas. Estos modelos se denominam modelos de promedio móvil (\\(MA\\), siglas en inglés: Moving Average). Hay varias maneras de introducir los modelos \\(MA\\). Un enfoque es tratar el modelo como una extensión de una serie de ruido blanco; alternativamente a la representación autoregresiva en la cual \\(x_t\\) del lado izquierdo se asume como una combinación lineal, en los modelos de promedio móvil e orden \\(q\\) \\(MA(q)\\), asumimos el ruido blanco \\(w_t\\) del lado derecho de la ecuación que los define como una combinación lineal de los datos observados. Definición 5.1 El modelo de promedio móvil de orden \\(q\\) o modelo \\(MA(q)\\), se define como \\[\\begin{equation} x_t=w_t+\\theta_1x_{t-1}+\\theta_2x_{t-2}+\\cdots+\\theta_qx_{t-q} \\tag{5.1} \\end{equation}\\] donde hay \\(q\\) pasos o saltos en el promedio móvil y \\(\\theta_1,\\theta_2,\\ldots,\\theta_q (\\theta_q\\neq0)\\) son parámetros. 8 El ruido \\(w_t\\) se asume como un ruido blanco gaussiano. Podemos escribir también el modelo \\(MA(q)\\) en la forma equivalente \\[\\begin{equation} x_t = \\theta(B)w_t \\tag{5.2} \\end{equation}\\] usando la siguiente definición. Definición 5.2 El operador de promedio móvil se define como \\[\\begin{equation} \\theta(B) = 1+\\theta_1B+\\theta_2B^2+\\cdots+\\theta_qb^q \\tag{5.3} \\end{equation}\\] A diferencia del proceso autoregresivo \\(AR(p)\\), el proceso de promedio móvil \\(MA(q)\\) es estacionario para cada valor de los parámetros \\(\\theta_1,\\theta_2,\\ldots,\\theta_q\\). Otro enfoque para introducir los proceso de promedio móvil es tratar el modelo como un modelo \\(AR\\) de orden infinito con algunas restricciones. No hay una razón en particular, pero por simplicidad y sin pérdida de generalidad, asumiremos a priori que el orden del modelo es finito. Podemos suponer, al menos en teoría, un modelo \\(AR\\) con orden infinito como \\[x_t=\\phi_0+\\phi_1x_{t-1}+\\phi_2x_{t-2}+\\cdots+w_t\\] Sin embargo, tal modelo \\(AR\\) no esrealista porque tiene infinitos parámetros. Una forma de hacer práctico el modelo es asumir que los coeficientes \\(\\phi_i\\) satisfacen algunas restricciones para que sean determinados por un número finito de parámetros. Un caso especial de esta idea es \\[\\begin{equation} x_t=\\phi_0-\\theta_1x_{t-1}-\\theta_1^2x_{t-2}-\\theta_1^3x_{t-3}-\\cdots+w_t \\tag{5.4} \\end{equation}\\] donde los coeficientes dependen de un simple parámetro \\(\\theta_1\\) via \\(\\phi_i=-\\theta_1^i\\) para \\(i&gt;1\\). Para que el modelo en la ecuación (5.4) sea estacionario, \\(\\theta_1\\) debe ser menor que uno en valor absoluto, esto es, \\(|\\theta_1|&lt;1\\); de otra forma \\(\\theta_1^i\\) y la serie serán explosivas, porque los valores de la serie de tiempo se hacen grande en magnitud rápidamente. Claramente, porque \\(|\\phi_i|^j\\) crece sin acotación cuando \\(j\\to\\infty\\). Dado que \\(|\\theta_1|&lt;1\\), tenemos que \\(\\theta_1^i\\to0\\) cuando \\(i\\to\\infty\\). Por lo tanto, la contribución de $x_{t-i} a \\(x_t\\) decae exponencialmente cuando \\(i\\) crece. Esto es razonable, ya que la dependencia de una serie estacionaria \\(x_t\\) de su valor \\(x_{t-i}\\), si existe, debería decaer con el tiempo. El modelo en la ecuación (5.4) lo podemos escribir en forma más compacta. Para ello, reescribamosel modelo como \\[\\begin{equation} x_t+\\theta_1x_{t-1}+\\theta_1^2x_{t-2}+\\cdots=\\phi_0+w_t \\tag{5.5} \\end{equation}\\] El modelo para \\(x_{t-1}\\) es entonces \\[\\begin{equation} x_{t_1}+\\theta_1x_{t-2}+\\theta_1^2x_{t-3}+\\cdots=\\phi_0+w_{t-1} \\tag{5.6} \\end{equation}\\] Multiplicando la ecuación (5.6) por \\(\\theta_1\\) y restando el resultado a la ecuación (5.5), obtenemos \\[x_t=\\phi_0(1-\\theta_1)+w_t-\\theta_1w_{t-1}\\] que dice que excepto para el término constante, \\(x_t\\) es un promedio ponderado de \\(w_t\\) y \\(w_{t-1}\\). Por lo tanto, el modelo se denomina \\(MA\\) de orden 1 o modelo \\(MA(1)\\). La forma general de un modelo \\(MA(1)\\) es \\[\\begin{equation} x_t=c_0+w_t-\\theta_1w_{t-1} \\tag{5.7} \\end{equation}\\] donde \\(c_0\\) es constante y \\(\\{w_t\\}\\) es una serie de ruido blanco. De manera similar, un modelo \\(MA(2)\\) es de la forma \\[\\begin{equation} x_t=c_0+w_t-\\theta_1w_{t-1}-\\theta_2w_{t-2} \\tag{5.8} \\end{equation}\\] y un modelo \\(MA(q)\\) es de la forma \\[\\begin{equation} x_t=c_0+w_t-\\theta_1w_{t-1}-\\theta_2w_{t-2}-\\cdots-\\theta_qw_{t-q} \\tag{5.9} \\end{equation}\\] con \\(q&gt;0\\). 5.1 Propiedades de los modelos MA Al igual que con los modelos \\(AR\\), daremos las propiedades para los modelos \\(MA(1)\\) y \\(MA(2)\\), y luego generalizaremos a los modelos \\(MA(q)\\). 5.1.1 Estacionaridad Los modelos \\(MA\\) son siempre débilmente estacionarios porque son combinaciones lineales finitas de una sucesión de ruiod blanco para el cual los primeros dos momentos son invariantes en el tiempo. Por ejemplo, consideremos el modelo \\(MA(1)\\) dado en (5.7). Tomando el valor esperado obtenemos \\[\\begin{eqnarray*} \\mathbb{E}(x_t) &amp;=&amp; \\mathbb{E}(c_0+w_t-\\theta_1w_{t-1}) \\\\ &amp;=&amp; \\mathbb{E}(c_0) = c_0 \\end{eqnarray*}\\] el cual es invariante. Tomando la varianza en la misma ecuación (5.7), obtenemos \\[\\begin{eqnarray*} Var(x_t) &amp;=&amp; Var(c_0+w_t-\\theta_1w_{t-1}) \\\\ &amp;=&amp; Var(c_0)+Var(w_t)+\\theta_1^2Var(w_{t-1}) \\\\ &amp;=&amp; \\sigma_w^2+\\theta_1^2\\sigma_w^2 \\\\ &amp;=&amp; (1+\\theta_1^2)\\sigma_w^2 \\end{eqnarray*}\\] donde usamos el hecho de que \\(w_t\\) y \\(w_{t-1}\\) son no correlacionados. Nuevamente, \\(Var(x_t)\\) es invariante. Los cálculos anteriores los podemos aplicar a un modelo \\(MA(q)\\) general, de donde obtenemos dos propiedades generales: El término constante de un modelo \\(MA(q)\\) es la media de la serie, es decir, \\(\\mathbb{E}(x_t)=c_0\\) La varianza de un modelo \\(MA(q)\\) es \\[Var(x_t)=(1+\\theta_1^2+\\theta_2^2+\\cdots+\\theta_q^2)\\sigma_w^2\\] La serie en la figura siguiente corresponde a dos procesos \\(MA(1)\\), uno con \\(\\theta_1=0.5\\) y el otro con \\(\\theta_1=-0.5\\), en ambos casos \\(\\sigma_w^2=1\\). par(mfrow=c(2,1)) plot(arima.sim(list(order=c(0,0,1),ma=0.5),n=100),ylab=&quot;x1&quot;, main=expression(MA(1)~~~theta==+0.5)) plot(arima.sim(list(order=c(0,0,1),ma=-0.5),n=100),ylab=&quot;x2&quot;, main=expression(MA(1)~~~theta==-0.5)) Figura 5.1: Simulación de dos modelos MA(1) 5.1.2 Función de autocorrelación (ACF) Para simplificar podemos asumir que \\(c_0=0\\), para un modelo \\(MA(1)\\). Multiplicando el modelo por \\(x_{t-h}\\), obtenemos \\[x_{t-h}x_t=x_{t-h}w_t-\\theta_1x_{t-h}w_{t-1}\\] Tomando el valor esperado, obtenemos \\[\\begin{eqnarray} \\gamma(h) &amp;=&amp; \\mathbb{E}(x_{t-h}x_t) = \\mathbb{E}(x_{t-h}w_t-\\theta_1x_{t-h}w_{t-1}) \\nonumber \\\\ &amp;=&amp; \\begin{cases} -\\theta_1\\sigma_w^2,&amp;\\text{ si }h=1 \\\\ 0, &amp;\\text{ si }h&gt;1 \\end{cases} \\tag{5.10} \\end{eqnarray}\\] Usando el hecho de que \\(Var(x_t)=(1+\\theta_1^2)\\sigma_w^2\\), obtenemos la función de autocorrelacion ACF para un modelo \\(MA(1)\\), a saber \\[\\begin{equation} \\rho(h) = \\begin{cases} 1,&amp;\\text{ si }h=0 \\\\ \\frac{-\\theta_1}{1+\\theta_1^2},&amp;\\text{ si }h=1 \\\\ 0,&amp;\\text{ si }h&gt;1 \\end{cases} \\tag{5.11} \\end{equation}\\] Entonces, para un modelo \\(MA(1)\\), el paso 1 de la ACF es distinto de cero, y para orden o paso superior la ACF es cero. En otras palabras, la ACF de un modelo \\(MA(1)\\) corta en paso 1. Note que \\(|\\rho(1)|\\leq1/2\\) para todo valor de \\(\\theta_1\\). También, \\(x_t\\) está correlacionado con \\(x_{t-1}\\) pero no con \\(x_{t-2},x_{t-3},\\ldots\\). Constraste esto con el caso del modelo \\(AR(1)\\) en el cual la correlación entre \\(x_t\\) y \\(x_{t-k}\\) nunca es cero, para \\(k&gt;1\\). En la figura siguiente podemos observar las funciones de autocorrelacion de los modelos \\(MA(1)\\) simulados anteriormente par(mfrow=c(2,1)) acf(arima.sim(list(order=c(0,0,1),ma=0.5),n=100),ylab=&quot;x1&quot;, main=expression(MA(1)~~~theta==+0.5)) acf(arima.sim(list(order=c(0,0,1),ma=-0.5),n=100),ylab=&quot;x2&quot;, main=expression(MA(1)~~~theta==-0.5)) Figura 5.2: ACF para dos modelos MA(1) Podemos notar en cada una de las ACF, que efectivamente tienen un corte en paso 1. Para la serie con \\(\\theta_1=0.5\\), la correlación es positiva y para la serie con \\(\\theta=-0.5\\) la correlación es negativa. Para un modelo \\(MA(2)\\) dado por la ecuación (5.8) la función de autocorrelación está dada por \\[\\begin{equation} \\rho(h) = \\begin{cases} \\frac{-\\theta_1+\\theta_1\\theta_2}{1+\\theta_1^2+\\theta_2^2}, &amp;\\text{ si }h=1 \\\\ \\frac{-\\theta_2}{1+\\theta_1^2+\\theta_2^2}, &amp;\\text{ si }h=2 \\\\ 0, &amp;\\text{ si }h&gt;1 \\end{cases} \\tag{5.12} \\end{equation}\\] En este caso, la ACF corta en paso 2. Esta propiedad la podemos generalizar a los modelos \\(MA(q)\\). Así, para un modelo \\(MA(q)\\) la ACF se corta en paso \\(q\\), y vale cero para \\(h&gt;q\\). Consecuentemente, una serie \\(MA(q)\\) está solo linealmente relacionada con sus \\(q\\) primeros valores y por consiguiente es un modelo de “memoria finita”. Ejemplo 5.1 (No unicidad de modelos MA e Invertibilidad) Usando las funciones de autocovarianza (ec. (5.10)) y de autocorrelación (ec. (5.11)) de un modelo \\(MA(1)\\) podemos notar que \\(\\rho(h)\\) es el mismo para \\(\\theta\\) y \\(1/\\theta\\), probemos, por ejemplo, con \\(\\theta=5\\) y \\(\\frac{1}{\\theta}=\\frac{1}{5}\\) \\[\\begin{eqnarray*} \\rho(h) &amp;=&amp; \\begin{cases} \\frac{5}{(1+5^2)},&amp;\\text{ si }h=1 \\\\ 0,&amp;\\text{ si }h&gt;1 \\end{cases} = \\begin{cases} \\frac{5}{26},&amp;\\text{ si }h=1 \\\\ 0,&amp;\\text{ si }h&gt;1 \\end{cases} \\\\ \\rho(h) &amp;=&amp; \\begin{cases} \\frac{1/5}{(1+(1/5)^2)},&amp;\\text{ si }h=1 \\\\ 0,&amp;\\text{ si }h&gt;1 \\end{cases} = \\begin{cases} \\frac{5}{26},&amp;\\text{ si }h=1 \\\\ 0,&amp;\\text{ si }h&gt;1 \\end{cases} \\end{eqnarray*}\\] Además, el par \\(\\sigma_w^2=1\\) y \\(\\theta=5\\) llevan a la misma función de autocovarianza que el par \\(\\sigma_w^2=25\\) y \\(\\theta=1/5\\), esto es \\[\\gamma(h) = \\begin{cases} 26,&amp;\\text{ si }h=0\\\\ 5,&amp;\\text{ si }h=1\\\\ 0,&amp;\\text{ si }h&gt;1 \\end{cases}\\] Entonces los procesos \\(MA(1)\\) \\[x_t=w_t+\\frac{1}{5}w_{t-1}, w_t\\sim iidN(0,25)\\text{ y } x_t=v_t+5v_{t-1}, v_t\\sim iidN(0,1)\\] son los mismos debido a la normalidad, es decir, todas las distribuciones finitas son las mismas. Para descubrir cual de los modelos es el modelo invertible, podemos invertir los papeles de \\(x_t\\) y \\(w_t\\) (porque estamos copiando el caso \\(AR\\)) y escribir el modelo \\(MA(1)\\) como \\(w_t=-\\theta w_{t-1}+x_t\\). Siguiendo los pasos para (4.6), si \\(|\\theta|&lt;1\\), entonces \\(w_t=\\sum_{j=0}^{\\infty}(-\\theta)^jx_{t-j}\\), lo cual es la representación del modelo \\(AR\\) infinito deseado. Por consiguiente, elegimos el modelo con \\(\\sigma_w^2=25\\) y \\(\\theta=1/5\\) ya que este modelo es invertible 5.2 Identificación del orden de un MA La ACF es muy útil para identificar el orden de un modelo \\(MA\\). Para una serie de tiempo \\(x_t\\) con ACF \\(\\rho(h)\\), si \\(\\rho(q)\\neq0\\), pero \\(\\rho(h)=0\\) para \\(h&gt;q\\), entonces \\(x_t\\) sigue un modelo \\(MA(q)\\). La figura siguiente muestra el gráfico de la ACF para la serie de porcentajes de cambio diario de la Bolsa de Valores de New York del ejemplo 1.5. Las lineas discontinuas en azul en la ACF denotan los dos límites de error estándar. Se puede notar que la serie tiene un ACF significativo en pasos 1,2 y 5, lo que nos sugiere el siguiente modelo \\(MA(5)\\) \\[x_t=c_0+w_t-\\theta_1w_{t-1}-\\theta_2w_{t-2}-\\theta_5w_{t-5}\\] Figura 5.3: Serie de tiempo y función de autocorrelación de los porcentajes de cambio diario de la Bolsa de Valores de New York, desde el 2 de febrero de 1984 hasta el 31 de diciembre de 1991 5.3 Estimación La estimación de máxima verosimilitud se usa comúnmente para estimar modelos \\(MA\\). Existen dos enfoques para evaluar la función de verosimilitud de un modelo \\(MA\\). El primer enfoque asume que los valores iniciales, es decir, para \\(t\\leq0\\), son ceros. Como tal, los valores necesarios en el cálculo de la función de verosmilitud se obtiene recursivamente del modelo iniciando con \\(w_1=x_1-c_0\\) y \\(w_2=x_2-c_0+\\theta w_1\\). Este enfoque se conoce como el método de verosimilitud condicional y las estimaciones resultantes son las estimaciones de máxima verosimilitud condicional. El segundo enfoque trata los valores iniciales en \\(t\\leq0\\) como parámetros adicionales del modelo y los estima conjuntamente con otros parámetros. Este enfoque se conoce como el método de verosimilitud exacta. Las estimaciones de verosimilitud exacta son preferibles a las condicionales, pero requieren un cálculo más extenso. Si el tamaño de la muestra es grande, entonces los dos tipos de estimación de máxima verosimilitud están cerca uno del otro. 5.4 Predicciones usando modelos MA Las predicciones con modelos MA son fáciles de obtener. Como el modelo es de “memoria finita”, la predicción converge rápidamente a la media de la seire. Para ver esto, supongamos que la predicción inicia en \\(m\\). Para la predicción de un paso de un proceso \\(MA(1)\\), el modelo nos dice que \\[x_{m+1}=c_0+w_{m+1}-\\theta_1w_m\\] Tomando el valor esperado condicional, tenemos \\[\\begin{eqnarray*} \\hat{x}_m(1) &amp;=&amp; \\mathbb{E}(x_{m+1}|x_m,x_{m-1},\\ldots) = c_0-\\theta_1w_m \\\\ e_m(1) &amp;=&amp; x_{m+1}-\\hat{x}_m(1) = w_{m+1} \\end{eqnarray*}\\] La varianza del error de predicción de un paso es \\(Var[e_m(1)]=\\sigma_w^2\\). En la práctica, el valor \\(w_m\\) lo podemos obtener de varias maneras. Por ejemplo, suponemos que \\(w_0=0\\), entonces \\(w_1=x_1-c_0\\) y calculamos \\(w_t\\) para \\(2\\leq t\\leq m\\) recursivamente usando \\(w_t=x_t-c_0+\\theta_1w_{t-1}\\). Alternativamente, podemos calcularlo usando la representación \\(AR\\) del modelo \\(MA(1)\\). (Véase el ejemplo 5.1) Para la predicción de dos pasos, de la ecuación \\[x_{m+2}=c_0+w_{m+2}-\\theta_1w_{m+1}\\] obtenemos \\[\\begin{eqnarray*} \\hat{x}_m(2) &amp;=&amp; \\mathbb{E}(x_{m+2}|x_m,x_{m-1},\\ldots)=c_0\\\\ e_m(2) &amp;=&amp; x_{m+2}-\\hat{x}_m(2) = w_{m+2}-\\theta_1w_{m+1} \\end{eqnarray*}\\] La varianza del error de predicción es \\(Var[e_m(2)]=(1+\\theta_1^2)\\sigma_m^2\\), la cual es la varianza del proceso \\(MA\\) y es mayor o igual que la varianza del error de predicción de un paso. Este resultado muestra que para un modelo \\(MA(1)\\) la predicción de dos pasos de la serie es sencillamente la media incondicional del modelo. Esto es cierto para cualquier predicción iniciando en \\(m\\). Más generalmente \\(\\hat{x}_m(h)=c_0\\) para \\(h&gt;2\\). En resumen, para un modelo \\(MA(1)\\), la predicción de un paso a futuro con origen en \\(m\\) es \\(c_0-\\theta_1w_m\\) y la predicción de múltiples pasos es \\(c_0\\), el cual es la media incondicional del modelo. Si graficamos la predicción \\(\\hat{x}_m(h)\\) versus \\(h\\) podemos ver que la predicción es una linea horizontal después del paso uno. De manera similar, para un modelo \\(MA(2)\\), tenemos \\[x_{m+h} = c_0+w_{m+h}-\\theta_1w_{m+h-1}-\\theta_2w_{m+h-2}\\] para el cual obtenemos \\[\\begin{eqnarray*} \\hat{x}_m(1) &amp;=&amp; c_0-\\theta_1w_m-\\theta_2w_{m-1} \\\\ \\hat{x}_m(2) &amp;=&amp; c_0-\\theta_2w_m \\\\ \\hat{x}_m(h) &amp;=&amp; c_0\\text{, para }h&gt;2 \\end{eqnarray*}\\] Entonces, la predicción de múltiples pasos de un modelo \\(MA(2)\\) tiende a la media de la serie después de dos pasos. La varianza del error de predicción converge a la varianza de la serie después de dos pasos. En general, para un modelo \\(MA(q)\\), la predicción multipasos converge a la media de la serie después de los primeros \\(q\\) pasos. ** Resumen ** Para concluir daremos un resumen de las propiedades estudiadas para los modelos \\(AR\\) y \\(MA\\) en estos dos capítulos: Para los modelos \\(MA\\), la ACF es útil para especificar el orden dado que la ACF se corta en salto \\(q\\) para una serie \\(MA(q)\\). Para un modelo \\(AR\\), la PACF es útil para determinar el orden ya que la PACF se corta en salto \\(p\\) para un proceso \\(AR(p)\\). Una serie \\(MA\\) es siempre estacionaria, pero para que una serie \\(AR\\) sea estacionaria, todas sus raíces características deben ser menor que u1 en módulo. Para una serie estacionaria, la predicción de múltiples pasos converge a la media de la serie y la varianza del error de predicción converge a la varianza de la serie. Algunos libros y algunos paquetes estadísticos escriben el modelo \\(MA\\) con coeficientes negativos, esto es \\[x_t=w_t-\\theta_1x_{t-1}-\\theta_2x_{t-2}-\\cdots-\\theta_qx_{t-q}\\]↩ "],
["modelos-arma.html", "Capítulo 6 Modelos ARMA 6.1 Propiedades de los modelos ARMA(p,q) 6.2 Ecuaciones en Diferencias 6.2 Pronósticos 6.2 Estimación de parámetros 6.2 Estimación 6.2 Estimación por Máxima Verosimilitud y Mínimos Cuadrados 6.2 Estimación de mínimos cuadrados para modelos ARMA(p,q) 6.2 Modelos ARIMA 6.2 Construcción de modelos ARIMA 6.2 Modelos SARIMA 6.2 Modelos ARCH y GARCH 6.2 Estructura de los Modelos 6.2 Modelos ARCH 6.3 La Densidad Espectral 6.4 Periodograma y Transformada Discreta de Fourier 6.5 Estimación Espectral No-paramétrica 6.6 Procesos de Incremento Ortogonal sobre \\([-\\pi,\\pi]\\) 6.7 Integración con Respecto a un Proceso de Incremento Ortogonal 6.8 La Representación Espectral", " Capítulo 6 Modelos ARMA En el capítulo 2, introdujimos las funciones de autocorrelación y correlación cruzada (ACFs y CCFs) como herramientas para clarificar las relaciones que pueden ocurrir dentro y entre las series de tiempo en varios rezagos. Además, explicamos cómo construir modelos lineales basado en la teoría clásica de regresión para la explotación de las asociaciones indicadas por los valores grandes de la ACF o CCF. Los métodos de este capítulo, en dominio del tiempo, o de regresión, son apropiados cuando se trata de posiblemente no estacionaridad con series de tiempo cortas; estas series son la regla y no la excepción en muchas aplicaciones. La regresión clásica es a menudo insuficiente para explicar todas las dinámicas interesantes de una serie de tiempo. Por ejemplo, la ACF de los residuos del ajuste de regresión lineal simple a los datos globales de la temperatura (véase el Ejemplo 2.6 del capítulo 2, Sección 2.3.1) revela una estructura adicional en los datos que la regresión no captura. En lugar de ello, la introducción de correlación como un fenómeno que se puede generar a través de relaciones lineales retardadas lleva a proponer los modelos autorregresivo (AR) y autorregresivo de promedio móvil (ARMA). Añadiendo modelos no estacionarios a la combinación conduce al modelo autorregresivo integrado de media móvil (ARIMA) popularizado en el destacado trabajo de Box y Jenkins (1970). El método de Box-Jenkins para la identificación de un posible modelo ARIMA se da en el siguiente capítulo junto con técnicas para la estimación de parámetros y la previsión para estos modelos. El modelo de regresión clásico del Capítulo 3 fue desarrollado para el caso estático, es decir, que sólo permite que la variable dependiente sea influenciada por los valores actuales de las variables independientes. En el caso de series de tiempo, es deseable permitir que la variable dependiente sea influenciada por los valores pasados de las variables independientes y posiblemente por sus propios valores pasados. Si el presente puede ser modelado plausiblemente en términos de sólo los valores pasados de las entradas independientes, tenemos la atractiva posibilidad de que la predicción será posible. Ahora procederemos con un desarrollo más general de modelos autoregresivos, de promedio móvil y mezcla de ambos modelos para series de tiempo estacionarias. Definición 6.1 Una serie de tiempo \\(\\{x_t; t=0,\\pm1,\\pm2,\\ldots\\}\\) es un proceso autoregresivo de promedio móvil, denotado \\(ARMA(p,q)\\), si es estacionario y \\[\\begin{equation} x_t=\\phi_1x_{t-1}+\\cdots+\\phi_px_{t-p}+w_t+\\theta_1w_{t-1}+\\cdots+\\theta_qw_{t-q} \\tag{6.1} \\end{equation}\\] con \\(\\phi_p\\neq0,\\theta_q\\neq0\\) y \\(\\sigma_w^2&gt;0\\). Los parámetros \\(p\\) y \\(q\\) son llamados ordenes autoregresivos y de promedio móvil respectivamente. Si \\(x_t\\) tiene media \\(\\mu\\) distinto de cero, hacemos \\(\\alpha=\\mu(1-\\phi_1-\\cdots-\\phi_p)\\) y escribimos el modelo como \\[\\begin{equation} x_t=\\alpha+\\phi_1x_{t-1}+\\cdots+\\phi_px_{t-p}+w_t+\\theta_1w_{t-1}+\\cdots+\\theta_qw_{t-q} \\tag{6.2} \\end{equation}\\] A menos que se declare lo contrario, \\(\\{w_t;t=0,\\pm1,\\pm2,\\ldots\\}\\) es una sucesión de ruido blanco gaussiano. Como se observó previamente, cuando \\(q=0\\), el modelo es llamado modelo autoregresivo de orden \\(p\\), AR(p), y cuando \\(p=0\\) el modelo es llamado modelo de promedio móvil de orden \\(q\\) MA(q). Como ayuda en la investigación de los modelos ARMA, será útil escribir estos usando el operador AR (4.5) y el operador MA (5.3). En particular el modelo ARMA(p,q) en (6.1) se puede escribir en forma concisa como \\[\\begin{equation} \\phi(B)x_t=\\theta(B)w_t \\tag{6.3} \\end{equation}\\] Antes de discutir las condiciones bajo la cual (6.1) es causal e invertible, veamos un potencial problema con el modelo ARMA. Ejemplo 6.1 En econometría, es usual considerar modelos dinámicos del siguiente tipo; llamemos \\(x_1,\\ldots,x_r\\) a la característica numérica de interés en un sector económico arbitrario dado (precios, nivel de producción, ingresos, inversiones, etc.), asumiendo por ejemplo que las observaciones son anuales. Para el año \\(n\\) existe un vector asociado \\(X(n)\\) con coordenadas \\(x_1(n),\\ldots,x_r(n)\\). Asumimos que \\(X(n)\\) verifica la recursión lineal del tipo \\[X(n) = A_0X(n)+A_1X(n-1)+\\cdots+A_jX(n-j)\\] donde \\(A_0,\\ldots,A_j\\) son matrices, pero esa relación es perturbada por un efecto aleatorio \\(w(n)\\) con media cero, no correlacionado para diferentes años. Entonces \\(X(n)\\) llega a ser un vector aleatorio que verifica \\[\\begin{equation} X(n) = \\sum_{k=0}^jA_kX(n-k) + w(n). \\tag{6.4} \\end{equation}\\] Se puede demostrar, con unas pocas restricciones sobre los \\(A_k\\), que si \\(w(n)\\) es un ruido blanco, entonces existe un proceso estacionario \\(X(n)\\) que satisface (6.4) tal que para cada \\(i=1,\\ldots,r\\), \\(x_i(n)\\) es un proceso ARMA. Ejemplo 6.2 (Redundancia de Parámetro) Considere un proceso de ruido blanco \\(x_t=w_t\\). Equivalentemente podemos escribir este como \\(0.5x_{t-1}=0.5w_{t-1}\\) utilizando el operador de cambio una vez y multiplicando por 0.5. Ahora, restamos las dos representaciones para obtener \\[\\begin{equation} x_t-0.5x_{t-1}=w_t-0.5w_{t-1}\\text{ ó }x_t=0.5x_{t-1}-0.5w_{t-1}+w_t \\tag{6.5} \\end{equation}\\] el cual luce como un modelo ARMA(1,1). De hecho, \\(x_t\\) es todavía un ruido blanco; nada ha cambiado en esta consideración [esto es, \\(x_t=w_t\\) es la solución de (6.5), pero hemos escondido el hecho de que \\(x_t\\) es un ruido blanco debido al parámetro de redundancia o sobre-parametrización. Escribiendo el parámetro de redundancia en la forma de operador como \\(\\phi(B)x_t=\\theta(B)w_t\\) o \\[(1-0.5B)x_t=(1-0.5B)w_t\\] Aplicando el operador \\(\\phi(B)^{-1}=(1-0.5B)^{-1}\\) a ambos lados de la igualdad, obtenemos \\[x_t=(1-0.5B)^{-1}(1-0.5B)x_t=(1-0.5B)^{-1}(1-0.5B)w_t=w_t\\] el cual es el modelo original. Podemos fácilmente detectar el problema de sobre-parametrización con el uso de los operadores o sus polinomios asociados. Esto es, escribimos el polinomio AR \\(\\phi(z)=(1-0.5z)\\), el polinomio MA \\(\\theta(z)=(1-0.5z)\\) y note que ambos polinomios tienen un factor común, este es \\((1-0.5z)\\). Este factor común identifica los parámetros de redundancia de inmediato. Descartando el factor en cada uno, nos queda \\(\\phi(z)=1\\) y \\(\\theta(z)=1\\) de donde se concluye que \\(\\phi(B)=1\\) y \\(\\theta(B)=1\\), y deducimos que el modelo es un ruido blanco. La consideración de parámetros de redundancia será crucial cuando discutamos la estimación de modelo ARMA en general. Como apuntó este ejemplo, podemos fijar un modelo ARMA(1,1) a un ruido blanco y conseguir que los parámetros estimados sean significativos. Si no fuéramos conscientes de la redundancia de parámetros, se podría alegar que los datos están correlacionados, cuando en realidad no lo son. Los ejemplos 4.1, 5.1 y 6.2 señalan un número de problemas con la definición general de los modelos ARMA(p,q) dados por (6.1) o equivalentemente por (6.3). En resumen, tenemos los siguientes problemas: Modelos de parámetros redundantes, Modelos AR estacionarios que dependen del futuro, y Modelos MA que no son únicos. Para resolver estos problemas, requeriremos algunas restricciones adicionales sobre los parámetros de los modelos, pero primero, daremos las siguientes definiciones: Definición 6.2 Los Polinomios AR y MA se definen como \\[\\begin{equation} \\phi(z)=1-\\phi_1z-\\cdots-\\phi_pz^p\\text{, }\\phi_p\\neq0 \\tag{6.6} \\end{equation}\\] y \\[\\begin{equation} \\theta(z)=1+\\theta_1z+\\cdots+\\theta_qz^q\\text{, }\\theta_q\\neq0 \\tag{6.7} \\end{equation}\\] respectivamente, donde \\(z\\) es un número complejo. Para abordar el primer problema (Modelos de parámetros redundantes), de ahora en adelante nos referiremos a un modelo \\(ARMA(p,q)\\) el sentido de su forma más simple. Esto es, además de la definición original dada en la ecuación (6.1), requeriremos también que \\(\\phi(z)\\) y \\(\\theta(z)\\) no tengan factores comunes. Así, el proceso \\(x_t=0.5x_{t-1}-0.5w_{t-1}+w_t\\) discutido en el ejemplo 6.2 no se refiere a un proceso ARMA(1,1) porque en su forma reducida \\(x_t\\) es un ruido blanco. Para resolver el problema del modelo con dependencia del futuro, introduciremos formalmente el concepto de causalidad. Definición 6.3 Un modelo ARMA(p,q), \\(\\phi(B)x_t=\\theta(B)w_t\\), se dice que es causal si la serie de tiempo \\(\\{x_t:t=0,\\pm1,\\pm2,\\ldots\\}\\) se puede escribir como un proceso lineal de un lado, esto es \\[\\begin{equation} x_t=\\sum_{j=0}^{\\infty}\\psi_jw_{t-j}=\\psi(B)w_t \\tag{6.8} \\end{equation}\\] donde \\(\\psi(B)=\\sum_{j=0}^{\\infty}\\psi_jB^j\\) y \\(\\sum_{j=0}^{\\infty}|\\psi_j|&lt;\\infty\\); haciendo \\(\\psi_0=1\\) En el ejemplo 4.1 el proceso \\(AR(1)\\) \\(x_t=\\phi z_{t-1}+w_t\\) es causal solo cuando \\(|\\phi|&lt;1\\). Equivalentemente, el proceso es causal sólo cuando la raíz de \\(\\phi(z)=1-\\phi z\\) es mayor que uno en valor absoluto. Esto es, la raíz \\(z_0\\) de \\(\\phi(z)\\) es \\(z_0=1/\\phi\\) (porque \\(\\phi(z_0)=0\\)) y \\(|z_0|&gt;1\\), porque \\(|\\phi|&lt;1\\). Definición 6.4 Un modelo ARMA(p,q) \\(\\phi(B)x_t=\\theta(B)w_t\\) se dice invertible si la serie de tiempo \\(\\{x_t:t=0,\\pm1,\\pm2,\\ldots\\}\\) se puede escribir como \\[\\begin{equation} \\pi(B)x_t=\\sum_{j=0}^{\\infty}\\pi_jx_{t-j}=w_t \\tag{6.9} \\end{equation}\\] donde \\(\\pi(B)=\\sum_{j=0}^{\\infty}\\pi_jB^j\\) y \\(\\sum_{j=0}^{\\infty}|\\pi_j|&lt;\\infty\\); hacemos \\(\\pi_0=1\\) 6.1 Propiedades de los modelos ARMA(p,q) Proposición 6.1 (Propiedad 1: Causalidad) Un modelo ARMA(p,q) es causal si y solo si \\(\\phi(z)\\neq0\\) para \\(|z|\\leq1\\). El coeficiente del proceso lineal dado en (6.8) se puede determinar resolviendo \\[\\psi(z)=\\sum_{j=0}^{\\infty}\\psi_jz^j=\\frac{\\theta(z)}{\\phi(z)}\\text{, }|z|&lt;1.\\] Otra manera de ver la propiedad 1, es que un modelo ARMA es causal sólo cuando las raíces de \\(\\phi(z)\\) están fuera del círculo unitario, esto es \\(\\phi(z)=0\\) sólo cuando \\(|z|&gt;1\\). Demostración. Supongamos primero que las raíces de \\(\\phi(z)\\), digamos \\(z_1,\\ldots,z_p\\), están fuera del círculo unitario. Escribimos las raíces en el siguiente orden \\(1&lt;|&lt;_1|\\leq|z_2|\\leq\\ldots\\leq|z_p|\\), note que \\(z_1,\\ldots,z_p\\) no son necesariamente únicas, y hacemos \\(|z_1|=1+\\epsilon\\), para algún \\(\\epsilon&gt;0\\). Entonces, \\(\\phi(z)\\neq0\\) siempre que \\(|z|&lt;|z_1|=1+\\epsilon\\) y por consiguiente, \\(\\phi^{-1}(z)\\) existe y tiene un desarrollo en serie de potencias \\[\\frac{1}{\\phi(z)} = \\sum_{j=0}^{\\infty}a_jz^j,\\quad |z|&lt;1+\\epsilon.\\] Ahora, elegimos un valor de \\(\\delta\\) tal que \\(0&lt;\\delta&lt;\\epsilon\\), y hacemos \\(z=1+\\delta\\), el cual está dentro del radio de convergencia. Se sigue entonces que \\[\\begin{equation} \\phi^{-1}(1+\\delta) = \\sum_{j=0}^{\\infty}a_j(1+\\delta)^j&lt;\\infty. \\tag{6.10} \\end{equation}\\] Así, podemos cada término de la suma en (6.10) por una constante, sea esta \\(|a_j(1+\\delta)^j|&lt;c\\), para \\(c&gt;0\\). Por consiguiente, \\(|a_j|&lt;c(1+\\delta)^{-j}\\), de donde se sigue que \\[\\begin{equation} \\sum_{j=0}^{\\infty}|a_j|&lt;\\infty. \\tag{6.11} \\end{equation}\\] En consecuencia, \\(\\phi^{-1}(B)\\) existe y podemos aplicar este a ambos lados del modelo ARMA, \\(\\phi(B)x_t=\\theta(B)w_t\\), para obtener \\[x_t = \\phi^{-1}(B)\\phi(B)x_t = \\phi^{-1}(B)\\theta(B)w_t.\\] Entonces, haciendo \\(\\psi(B)=\\phi^{-1}(B)\\theta(B)\\), tenemos \\[x_t = \\psi(B)w_t = \\sum_{j=0}^{\\infty}\\psi_jw_{t-j},\\] donde los \\(\\psi\\)-pesos, los cuales son absolutamente sumables, pueden ser evaluados por medio de \\(\\psi(z) = \\phi^{-1}(z)\\theta(z)\\) para \\(|z|\\leq1\\). Ahora, supongamos que \\(x_t\\) es un proceso causal, esto es, el proceso tiene la representación \\[x_t = \\sum_{j=0}^{\\infty}\\psi_jw_{t-j},\\quad\\text{con}\\quad \\sum_{j=0}^{\\infty}|\\psi_j|&lt;\\infty.\\] En este caso, escribimos \\[x_t = \\psi(B)w_t,\\] y multiplicando a ambos lados por \\(\\phi(B)\\) nos queda \\[\\begin{equation} \\phi(B)x_t = \\phi(B)\\psi(B)w_t. \\tag{6.12} \\end{equation}\\] Además de (6.12), el modelo ARMA se puede escribir como \\[\\begin{equation} \\phi(B)x_t = \\theta(B)w_t. \\tag{6.13} \\end{equation}\\] De (6.12) y (6.13), notamos que \\[\\begin{equation} \\phi(B)\\psi(B)w_t = \\theta(B)w_t. \\tag{6.14} \\end{equation}\\] Ahora, sea \\[a(z) = \\phi(z)\\psi(z) = \\sum_{j=0}^{\\infty}a_jz^j,\\quad |z|\\leq1\\] y, por consiguiente, podemos escribir (6.14) como \\[\\begin{equation} \\sum_{j=0}^{\\infty}a_jw_{t-j} = \\sum_{j=0}^{q}\\theta_jw_{t-j}. \\tag{6.15} \\end{equation}\\] A continuación, multiplicamos ambos lados de (6.15) por \\(w_{t-h}\\), para \\(h=0,1,2,\\ldots\\), y tomamos esperanza. Haciendo esto, obtenemos \\[\\begin{eqnarray} a_h &amp;=&amp; \\theta_h,\\quad h=0,1,\\ldots,q \\nonumber \\\\ a_h &amp;=&amp; 0, \\quad h&gt;q. \\tag{6.16} \\end{eqnarray}\\] De (6.16) concluimos que \\[\\begin{equation} \\phi(z)\\psi(z) = a(z) = \\theta(z),\\quad |z|\\leq1. \\tag{6.17} \\end{equation}\\] Si existe un número en el círculo unitario, digamos \\(z_0\\), para el cual \\(\\phi(z_0)=0\\), entonces por (6.17), \\(\\theta(z_0)=0\\). Pero, si existe tal \\(z_0\\) entonces \\(\\phi(z)\\) y \\(\\theta(z)\\) tienen un factor común lo cual no es posible. En consecuencia, podemos escribir \\(\\psi(z) = \\theta(z)/\\phi(z)\\). Además, por hipótesis, tenemos que \\(|\\psi(z)|&lt;\\infty\\) para \\(|z|\\leq1\\), y por lo tanto \\[\\begin{equation} |\\psi(z)| = \\left|\\frac{\\theta(z)}{\\phi(z)}\\right|&lt;\\infty,\\text{ para }|z|\\leq1. \\tag{6.18} \\end{equation}\\] Finalmente, la ecuación (6.18) implica que \\(\\phi(z)\\neq0\\) para \\(|z|\\leq1\\); esto es, las raíces de \\(\\phi(z)\\) están fuera del círculo unitario. Proposición 6.2 (Propiedad 2: Invertibilidad) Un modelo ARMA(p,q) es invertible si y solo si \\(\\theta(z)\\neq0\\) para \\(|z|\\leq1\\). El coeficiente \\(\\pi_j\\) de \\(\\pi(B)\\) dado en (6.9) se puede determinar al resolver \\[\\pi(z)=\\sum_{j=0}^{\\infty}\\pi_jz^j=\\frac{\\phi(z)}{\\theta(z)}\\text{, }|z|\\leq1.\\] Otra manera de escribir la propiedad 2, es que un proceso ARMA es invertible solo cuando las raíces de \\(\\theta(z)\\) están fuera del círculo unitario; esto es, \\(\\theta(z)=0\\) sólo cuando \\(|z|&gt;1\\). Demostración. La demostración de esta propiedad es similar a la propiedad 1, y se deja como ejercicio. Ejemplo 6.3 (Redundancia de Parámetros, Causalidad e Invertibilidad) Considere el proceso \\[x_t=0.4x_{t-1}+0.45x_{t-2}+w_t+w_{t-1}+0.25w_{t-2}\\] o, en la forma operador \\[(1-0.4B-0.45B^2)x_t=(1+B+0.25B^2)w_t\\] A primera vista \\(x_t\\) parece ser un proceso ARMA(2,2). Pero, los polinomios asociados \\[\\begin{eqnarray*} \\phi(z) &amp;=&amp; 1-0.4z-0.45z^2=(1+0.5z)(1-0.9z) \\theta(z) &amp;=&amp; (1+z+0.25z^2)=(1+0.5z)^2 \\end{eqnarray*}\\] tienen un factor común que se puede cancelar. Después de cancelar los factores comunes, los polinomios quedan \\(\\phi(z)=(1-0.9z)\\) y \\(\\theta(z)=(1+0.5z)\\), de modo que el modelo es un proceso ARMA(1,1) \\((1-0.9B)x_t=(1+0.5B)w_t\\) o \\[\\begin{equation} x_t=0.9x_{t-1}+0.5w_{t-1}+w_t \\tag{6.19} \\end{equation}\\] El modelo es causal, porque \\(\\phi(z)=(1-0.9z)=0\\) cuando \\(z=10/9\\) que está fuera del círculo unitario. El modelo también es invertible porque la raíz de \\(\\theta(z)=(1+0.5z)\\) es \\(z=-2\\) que también está fuera del circulo unitario. Para escribir el modelo como un proceso lineal, podemos obtener los \\(\\psi\\)-pesos usando la proposición 6.1: \\[\\begin{eqnarray*} \\psi(z) &amp;=&amp; \\frac{\\theta(z)}{\\phi(z)}=\\frac{(1+0.5z)}{(1-0.9z)} \\\\ &amp;=&amp; (1+0.5z)(1+0.9z+0.9^2z^2+0.9^3z^3+\\cdots)\\text{ }|z|\\leq1 \\end{eqnarray*}\\] El coeficiente de \\(z^j\\) en \\(\\psi(z)\\) es \\(\\psi_j=(0.5+0.9)0.9^{j-1}\\), para \\(j\\geq1\\), así, podemos escribir () como \\[x_t=w_t+1.4\\sum_{j=0}^{\\infty}0.9^{j-1}w_{t-j}\\] Similarmente, para hallar la representación invertible usando la proposición 6.2: \\[\\pi(z)=\\frac{\\phi(z)}{\\theta(z)}=(1-0.9z)(1-0.5z+0.5^2z^2-0.5^3z^3+\\cdots)\\text{ }|z|\\leq1.\\] En este caso, los \\(\\pi\\)-pesos están dados por \\(\\pi_j=(-1)^j(0.9+0.5)0.5^{j-1}\\) para \\(j\\geq1\\) y por lo tanto, podemos escribir (6.19) como \\[x_t=1.4\\sum_{j=0}^{\\infty}(-0.5)^{j-1}x_{t-j}+w_t\\] La PACF para los modelos \\(MA\\) se comporta como el ACF para los modelos \\(AR\\). También, la PACF para modelos \\(AR\\) se comporta como la ACF para modelos \\(MA\\). Debido a que un modelo ARMA invertible tiene una representación \\(AR\\) infinita, la PACF no tendrá corte. Resumimos estos resultados en la tabla siguiente AR(p) MA(q) ARMA(p,q) ACF Disminución Corte después Disminución gradual de paso \\(q\\) gradual PACF Corte después Disminución Disminución de paso \\(q\\) gradual gradual 6.2 Ecuaciones en Diferencias El estudio del comportamiento de los procesos ARMA es mucho mejor si se tiene un conocimiento básico de ecuaciones en diferencias, simplemente porque los procesos ARMA son ecuaciones en diferencias. Este tópico será también muy útil para el estudio de los modelos en dominio del tiempo y procesos estocásticos en general. Vamos a dar una breve y heurística reseña del tema junto con algunos ejemplos de la utilidad de la teoría. Supongamos que tenemos una sucesión de números \\(u_0,u_1,u_2,\\ldots\\) tal que \\[\\begin{equation} u_n-\\alpha u_{n-1}=0\\text{, }\\alpha\\neq0\\text{, }n=1,2,\\ldots \\tag{6.20} \\end{equation}\\] Por ejemplo, recuerde (4.8), en la cual mostramos que la ACF de un proceso AR(1) es una sucesión \\(\\rho(h)\\) que satisface \\[\\rho(h)=\\phi\\rho(h-1)=0\\text{, para }h=1,2\\ldots\\] La ecuación (6.20) representa un ecuación en diferencias homogénea de orden 1. Para resolver la ecuación, escribimos \\[\\begin{eqnarray*} u_1 &amp;=&amp; \\alpha u_0 \\\\ u_2 &amp;=&amp; \\alpha u_1 = \\alpha^2u_0 \\\\ &amp;\\vdots&amp; \\\\ u_n &amp;=&amp; \\alpha u_{n-1} = \\alpha^nu_0. \\end{eqnarray*}\\] Dando una condición inicial \\(u_0=c\\) podemos resolver (6.20), sea esta \\(u_n=\\alpha^nc\\). En la notación de operador, (6.20) se puede escribir como \\((1-\\alpha B)u_n=0\\). El polinomio asociado a (6.20) es \\(\\alpha(z)=1-\\alpha z\\), y las raíz \\(z_0\\) del polinomio es \\(z_0=1/\\alpha\\), esto es \\(\\alpha(z_0)=0\\). Conocemos la solución de (6.20) con condición inicial \\(u_0=c\\), esta es \\[u_n=\\alpha^nc=(z_0^{-1})^nc.\\] Esto es, la solución de la ecuación en diferencias (6.20) solo depende de la condición inicial y de la inversa de la raíz del polinomio asociado \\(\\alpha(z)\\). Supóngase ahora que la sucesión satisface \\[\\begin{equation} u_n-\\alpha_1u_{n-1}-\\alpha_2u_{n-2}=0\\text{, }\\alpha_2\\neq0\\text{, }n=2,3,\\ldots \\tag{6.21} \\end{equation}\\] Esta ecuación es una ecuación en diferencias homogénea de orden 2. El correspondiente polinomio es \\[\\alpha(z)=1-\\alpha_1z-\\alpha_2z^2\\] el cual tiene dos raíces \\(z_1\\) y \\(z_2\\), tal que \\(\\alpha(z_1)=\\alpha(z_2)=0\\). Consideremos dos casos: Caso 1: \\(z_1\\neq z_2\\).} La solución general en este caso es \\[\\begin{equation} u_n=c_1z_1^{-n}+c_2z_2^{-n} \\tag{6.22} \\end{equation}\\] donde \\(c_1\\) y \\(c_2\\) dependen de las condiciones iniciales. Esta afirmación puede ser verificada por la sustitución directa de (6.22) en (6.21): \\[\\begin{eqnarray*} c_1z_1^{-n}+c_2z_2^{-n} &amp;-&amp; \\alpha_1\\left(c_1z_1^{-(n-1)}+c_2z_2^{-(n-1)}\\right)-\\alpha_2\\left(c_1z_1^{-(n-2)}+c_2z_2^{-(n-2)}\\right) \\\\ &amp;=&amp; c_1z_1^{-n}(1-\\alpha_1z_1-\\alpha_2z_1^2)+c_2z_2^{-n}((1-\\alpha_1z_2-\\alpha_2z_2^2) \\\\ &amp;=&amp; c_1z_1^{-n}\\alpha(z_1)+c_2z_2^{-n}\\alpha(z_2) \\\\ &amp;=&amp; 0. \\end{eqnarray*}\\] Dando dos condiciones iniciales \\(u_0\\) y \\(u_1\\) podemos resolver para \\(c_1\\) y \\(c_2\\): \\[\\begin{eqnarray*} u_0 &amp;=&amp; c_1+c_2 \\\\ u_1 &amp;=&amp; c_1z_1^{-1}+c_2z_2^{-1} \\end{eqnarray*}\\] donde \\(z_1\\) y \\(z_2\\) se pueden resolver en términos de \\(\\alpha_1\\) y \\(\\alpha_2\\) usando la fórmula cuadrática por ejemplo. Caso 2: \\(z_1=z_2 (=z_0)\\). En este caso la solución general de (6.21) es \\[\\begin{equation} u_n=z_0^{-n}(c_1+c_2n) \\tag{6.23} \\end{equation}\\] Esta afirmación se puede verificar por sustitución directa de (6.23) en (6.21): \\[\\begin{eqnarray*} z_0^{-n}(c_1+c_2n) &amp;-&amp; \\alpha_1\\left(z_0^{-(n-1)}[c_1+c_2(n-1)]\\right)-\\alpha_2\\left(z_0^{-(n-2)}[c_1+c_2(n-2)]\\right) \\\\ &amp;=&amp; z_0^{-n}(c_1+c_2n)(1-\\alpha_1z_0-\\alpha_2z_0^2)+c_2z_0^{-n+1}(\\alpha_1+2\\alpha_2z_0) \\\\ &amp;=&amp; c_2z_0^{-n+1}(\\alpha_1+2\\alpha_2z_0). \\end{eqnarray*}\\] Para demostrar que \\((\\alpha_1+2\\alpha_2z_0)=0\\), escribimos \\(1-\\alpha_1z-\\alpha_2z^2=(1-z_0^{-1}z)^2\\) y derivamos respecto de \\(z\\) en ambos lados de la ecuación para obtener \\((\\alpha_1+2\\alpha_2z)=2z_0^{-1}(1-z_0^{-1}z)\\). Entonces \\((\\alpha_1+2\\alpha_2z_0)=2z_0^{-1}(1-z_0^{-1}z_0)=0\\). Finalmente, dando dos condiciones iniciales \\(u_0\\) y \\(u_1\\) podemos resolver para \\(c_1\\) y \\(c_2\\); \\[\\begin{eqnarray*} u_0 &amp;=&amp; c_1 \\\\ u_1 &amp;=&amp; (c_1+c_2)z_0^{-1} \\end{eqnarray*}\\] Resumiendo: En el caso de raíces distintas, la solución de la ecuación en diferencias homogénea de grado 2 es \\[\\begin{eqnarray*} u_n &amp;=&amp; z_1^{-n}\\times(\\text{un polinomio en } n \\text{ de grado }m_1-1) \\\\ &amp;+&amp; z_2^{-n}\\times(\\text{un polinomio en } n \\text{ de grado }m_2-1) \\end{eqnarray*}\\] donde \\(m_1\\) es la multiplicidad de la raíz \\(z_1\\) y \\(m_2\\) es la multiplicidad de la raíz \\(z_2\\). En este ejemplo, se tiene \\(m_1=m_2=1\\) y decimos que \\(c_1\\) y $c_2 $ son polinomios de grado cero respectivamente. En el caso de raíces repetidas, la solución es \\[u_n=z_0^{-n}\\times(\\text{un polinomio en } n \\text{ de grado }m_0-1),\\] donde \\(m_0\\) es la multiplicidad de la raíz \\(z_0\\); esto es \\(m_0=2\\). En este caso, escribimos el polinomio de grado uno como \\(c_1+c_2n\\). En ambos casos, resolvimos \\(c_1\\) y \\(c_2\\) dando dos condiciones iniciales \\(u_0\\) y \\(u_1\\). Veamos a continuacion algunos ejemplos de uso de las ecuaciones en diferencias, los dos primeros veremos la aplicación a procesos \\(AR(2)\\), y posteriormente daremos un ejemplo de uso para modelos ARMA. Ejemplo 6.4 (La ACF de un proceso AR(2)) Supóngase que \\(x_t=\\phi_1x_{t-1}+\\phi_2x_{t-2}+w_t\\) es un proceso \\(AR(2)\\) causal. Multiplicando ambos lados del modelo por \\(x_{t-h}\\) para \\(h&gt;0\\), tomando esperanza: \\[\\mathbb{E}(x_tx_{t-h})=\\phi_1\\mathbb{E}(x_{t-1}x_{t-h})+\\phi_2\\mathbb{E}(x_{t-2}x_{t-h})+\\mathbb{E}(w_tx_{t-h})\\] El resultado es \\[\\begin{equation} \\gamma(h)=\\phi_1\\gamma(h-1)+\\phi_2\\gamma(h-2)\\text{, para }h=1,2,\\ldots \\tag{6.24} \\end{equation}\\] En (6.24) estamos usando el hecho de que \\(\\mathbb{E}(x_t)=0\\) y para \\(h&gt;0\\), \\[\\mathbb{E}(w_tx_{t-h})=\\mathbb{E}\\left(w_t\\sum_{j=0}^{\\infty}\\psi_jw_{t-h-j}\\right)=0.\\] Dividimos (6.24) por \\(\\gamma(0)\\) para obtener la ecuación en diferencia de la ACF del proceso: \\[\\begin{equation} \\rho(h)-\\phi_1\\rho(h-1)-\\phi_2\\rho(h-2)=0\\text{, con }h=1,2,\\ldots. \\tag{6.25} \\end{equation}\\] Las condiciones iniciales son \\(\\rho(0)=1\\) y \\(\\rho(-1)=\\phi_1/(1-\\phi_2)\\), lo cual se obtiene evaluando \\(h=1\\) en (6.25) y observando que \\(\\rho(1)=\\rho(-1)\\). Usando los resultados para la ecuación en diferencias homogénea de orden dos, sean \\(z_1\\) y \\(z_2\\) las raíces del polinomio asociado \\(\\phi(z)=1-\\phi_1z-\\phi_2z^2\\). Como el modelo es causal sabemos que las raíces están fuera del círculo unitario: \\(|z_1|&gt;1\\) y \\(|z_2|&gt;1\\). Ahora consideremos la solución para los tres casos: Caso 1: Cuando \\(z_1\\) y \\(z_2\\) son reales y distintos, entonces \\[\\rho(h)=c_1z_1^{-h}+c_2z_2^{-h},\\] de modo que \\(\\rho(h)\\to0\\) exponencialmente cuando \\(h\\to\\infty\\). Caso 2: Cuando \\(z_1=z_2(=z_0)\\) son reales e iguales, entonces \\[\\rho(h)=z_0^{-h}(c_1+c_2h),\\] de modo que \\(\\rho(h)\\to0\\) exponencialmente cuando \\(h\\to\\infty\\). Caso 3: Cuando \\(z_1=\\bar{z}_2\\) son complejas conjugadas, entonces \\(c_2=\\bar{c}_1\\) (porque \\(\\rho(h)\\) es real) y \\[\\rho(h)=c_1z_1^{-h}+\\bar{c}_1\\bar{z}_1^{-h}.\\] Escribiendo \\(c_1\\) y \\(z_1\\) en coordenadas polares, por ejemplo \\(z_1=|z_1|e^{i\\theta}\\) donde \\(\\theta\\) es el ángulo cuya tangente es el radio de la parte imaginaria y la parte real de \\(z_1\\); el rango de \\(\\theta\\) es \\([-\\pi,\\pi]\\). Entonces, usando el hecho de que \\(e^{i\\alpha}+e^{-i\\alpha}=2\\cos(\\alpha)\\) la solución tiene la forma \\[\\rho(h)=a|z_1|^{-h}\\cos(h\\theta+b),\\] donde \\(a\\) y \\(b\\) se determinan de las condiciones iniciales. De nuevo \\(\\rho(h)\\) tiende a cero exponencialmente cuando \\(h\\to\\infty\\) pero en forma senosoidal. Ejemplo 6.5 (Camino muestral de un proceso AR(2) con raíces complejas) La Figura 6.1 muestra \\(n=144\\) observaciones de un modelo AR(2) \\[x_t=1.5x_{t-1}-0.75x_{t-2}+w_t\\] con \\(\\sigma_w^2=1\\) y con raíces complejas, así el proceso exhibe un comportamiento pseudo-cíclico con una frecuencia de un ciclo cada 12 puntos de tiempo. El polinomio autoregresivo para este modelo es \\(\\phi(z)=1-1.5z+0.75z^2\\). Las raíces de \\(\\phi(z)\\) son \\(1\\pm i/\\sqrt{3}\\) y \\(\\theta=\\tan^{-1}(1/\\sqrt{3})=2\\pi/12\\) radianes por unidad de tiempo. Para convertir el ángulo a ciclos por unidad de tiempo, dividimos por \\(2\\pi\\) para obtener \\(1/12\\) ciclos por unidad de tiempo. La ACF para este modelo se muestra en la parte inferior de la Figura 6.1. # Simulación del proceso AR(2) set.seed(5) ar2=arima.sim(list(order=c(2,0,0), ar=c(1.5,-0.75)), n=144) # Gráfico del proceso AR(2) par(mfrow=c(2,1)) plot(1:144/12,ar2,type=&quot;l&quot;,xlab=&quot;Tiempo (una unidad=12ptos)&quot;,ylab=&quot;AR(2)&quot;) abline(v=0:12,lty=&quot;dotted&quot;) # Raices del polinomio asociado Arg(polyroot(c(1,-1.5,0.75))[1])/(2*pi) ## [1] 0.08333 # Cálculo de la ACF ACF = ARMAacf(ar=c(1.5,0.75),ma=0,50) # Gráfico de la ACF plot(ACF,type=&quot;h&quot;,xlab=&quot;LAG&quot;) abline(h=0) Figura 6.1: Modelo AR(2) simulado, n=144, con phi_1=1.5, phi_2=-0.75 (parte superior) y la función de autocovarianza (parte inferior) Ahora, daremos la solución para una ecuación en diferencias homogénea general de orden \\(p\\): \\[\\begin{equation} u_n-\\alpha_1u_{n-1}-\\cdots-\\alpha_pu_{n-p}=0\\text{, con } \\alpha_p\\neq0\\text{, }n=p,p+1,\\ldots \\tag{6.26} \\end{equation}\\] El polinomio asociado es \\[\\alpha(z)=1-\\alpha_1z-\\alpha_2z^2-\\cdots-\\alpha_pz^p.\\] Suponga que \\(\\alpha(z)\\) tiene \\(r\\) raíces distintas, \\(z_1\\) con multiplicidad \\(m_1\\), \\(z_2\\) con multiplicidad \\(m_2,\\ldots,\\) y \\(z_r\\) con multiplicidad \\(m_r\\), tal que \\(m_1+m_2+\\cdots+m_r=p\\). La solución general para la ecuación (6.26) es \\[\\begin{equation} u_n=z_1^{-n}P_1(n)+z_2^{-n}P_2(n)+\\cdots+z_r^{-n}P_r(n), \\tag{6.27} \\end{equation}\\] donde \\(P_j(n)\\) para \\(j=1,2,\\ldots,r\\) es un polinomio en \\(n\\) de grado \\(m_j-1\\). Dadas las condiciones iniciales \\(u_0,u_1,\\ldots,u_{p-1}\\) podemos resolver \\(P_j(n)\\) explícitamente para \\(j=1,2,\\ldots,r\\) Ejemplo 6.6 (Determinación de los psi-pesos de un proceso ARMA(p,q) causal) Para un modelo ARMA(p,q) causal \\(\\phi(B)x_t=\\theta(B)w_t\\) donde los ceros de \\(\\phi(z)\\) están fuera del círculo unitario, recordemos que podemos escribir este como \\[x_t=\\sum_{j=0}^{\\infty}\\psi_jw_{t-j}\\] donde los \\(\\psi\\)-pesos se determinan usando la propiedad 1. (Proposición 6.1) Para un modelo \\(MA(q)\\) puro \\(\\psi_0=1,\\psi_j=\\theta_j\\) para \\(j=1,2,\\ldots,q\\) y \\(\\psi_j=0\\) en otro caso. Para el caso general de un modelo ARMA(p,q) la tarea de resolver los \\(\\psi\\)-pesos es más complicada, como se demostró en el ejemplo 6.3. La teoría de ecuaciones en diferencias homogénea será útil para resolver este problema. Para resolver los \\(\\psi\\)-pesos en general, debemos igualar los coeficientes en \\(\\psi(z)\\phi(z)=\\theta(z)\\) \\[(\\psi_0+\\psi_1z+\\psi_2z^2+\\cdots)(1-\\phi_1z-\\phi_2z^2-\\cdots)=(1+\\theta_1z+\\theta_2z^2+\\cdots)\\] Los primeros valores serán \\[\\begin{eqnarray*} \\psi_0 &amp;=&amp; 1 \\\\ \\psi_1-\\phi_1\\psi_0 &amp;=&amp; \\theta_1 \\\\ \\psi_2-\\phi_1\\psi_1-\\phi_2\\psi_0 &amp;=&amp; \\theta_2 \\\\ \\psi_3-\\phi_1\\psi_2-\\phi_2\\psi_1-\\phi_3\\psi_0 &amp;=&amp; \\theta_3 \\\\ &amp;\\vdots &amp; \\end{eqnarray*}\\] donde podemos tomar \\(\\phi_j=0\\) para \\(j&gt;p\\) y \\(\\theta_j=0\\) para \\(j&gt;q\\). Los \\(\\psi\\)-pesos satisfacen la ecuación en diferencias homogénea dada por \\[\\begin{equation} \\psi_j-\\sum_{k=1}^{p}\\phi_k\\psi_{j-k}=0\\text{, con }j\\geq\\max(p,q+1) \\tag{6.28} \\end{equation}\\] con condiciones iniciales \\[\\begin{equation} \\psi_j-\\sum_{k=1}^{j}\\phi_k\\psi_{j-k}=\\theta_j\\text{, con }0\\leq j&lt;\\max(p,q+1). \\tag{6.29} \\end{equation}\\] La solución general depende de las raíces del polinomio AR \\(\\phi(z)=1-\\phi_1z-\\cdots-\\phi_pz^p\\) como se ve de (6.28). La solución particular de hecho dependerá de las condiciones iniciales. Considere el proceso ARMA dado en (6.19) \\(x_t=0.9x_{t-1}+0.5w_{t-1}+w_t\\). Dado que \\(\\max(p,q+1)=2\\), usando (6.29), tenemos que \\(\\psi_0=1\\) y \\(\\psi_1=0.9+0.5=1.4\\). De (6.28) para \\(j=2,3,\\ldots,\\) los \\(\\psi\\)-pesos satisfacen \\(\\psi_j-0.9\\psi_{j-1}=0\\). La solución general es \\(\\psi_j=c0.9^j\\). Para hallar la solución particular, usamos la condición inicial \\(\\psi=1.4\\), de modo que \\(1.4=c0.9\\) ó \\(c=1.4/0.9\\). Finalmente \\(\\psi_j=1.4(0.9)^{j-1}\\) para \\(j\\geq1\\) como vimos en el ejemplo 6.3. Para ver los primeros 50 \\(\\psi\\)-pesos, usamos las siguientes instrucciones en R: ARMAtoMA(ar=0.9,ma=0.5, 50) ## [1] 1.400000 1.260000 1.134000 1.020600 0.918540 ## [6] 0.826686 0.744017 0.669616 0.602654 0.542389 ## [11] 0.488150 0.439335 0.395401 0.355861 0.320275 ## [16] 0.288248 0.259423 0.233481 0.210132 0.189119 ## [21] 0.170207 0.153187 0.137868 0.124081 0.111673 ## [26] 0.100506 0.090455 0.081410 0.073269 0.065942 ## [31] 0.059348 0.053413 0.048072 0.043264 0.038938 ## [36] 0.035044 0.031540 0.028386 0.025547 0.022992 ## [41] 0.020693 0.018624 0.016762 0.015085 0.013577 ## [46] 0.012219 0.010997 0.009898 0.008908 0.008017 plot(ARMAtoMA(ar=0.9,ma=0.5,50)) Figura 6.2: psi-pesos para el modelo ARMA, x_t=0.9x_{t-1}+0.5w_{t-1}+w_t 6.2 Función de Autocorrelación (ACF) para modelos ARMA Iniciemos mostrando la ACF de un proceso MA(q) \\(x_t=\\theta(B)w_t\\), donde \\(\\theta(B)=1+\\theta_1B+\\cdots+\\theta_qB^q\\). Dado que \\(x_t\\) es una combinación lineal de términos de ruido blanco, el proceso es estacionario con media \\[\\mathbb{E}(x_t)=\\sum_{j=0}^{q}\\theta_j\\mathbb{E}(w_{t-j})=0,\\] donde podemos escribir \\(\\theta_0=1\\), y la función de autocovarianza es \\[\\begin{eqnarray} \\gamma(h)=\\text{cov}(x_{t+h},x_t) &amp;=&amp; \\mathbb{E}\\left[\\left(\\sum_{j=0}^{q}\\theta_jw_{t+h-j}\\right)\\left(\\sum_{k=0}^{q}\\theta_kw_{t-k}\\right)\\right] \\nonumber \\\\ &amp;=&amp; \\begin{cases}\\sigma_w^2\\sum_{j=0}^{q-h}\\theta_j\\theta_{j+h},&amp;\\text{ si }0\\leq h\\leq q\\\\ 0,&amp;\\text{ si }h&gt;q\\end{cases}\\tag{6.30} \\end{eqnarray}\\] Recuerde que \\(\\gamma(h)=\\gamma(-h)\\), por eso solo mostramos \\(\\gamma(h)\\) para \\(h\\geq0\\). El corte de \\(\\gamma(h)\\) después de \\(q\\) saltos es la firma del modelo MA(q). Dividiendo () por \\(\\gamma(0)\\) conseguimos la ACF de un MA(q): \\[\\begin{equation} \\rho(h)=\\begin{cases}\\frac{\\sum_{j=0}^{q-h}\\theta_j\\theta_{j+h}}{1+\\theta_1^2+\\cdots+\\theta_q^2},&amp;\\text{ si }1\\leq h\\leq q\\\\ 0,\\text{ si }h&gt;q\\end{cases} \\tag{6.31} \\end{equation}\\] Para un modelo ARMA(p,q) causal \\(\\phi(B)x_t=\\theta(B)w_t\\), donde los ceros de \\(\\phi(z)\\) están fuera del círculo unitario, podemos escribir \\[x_t=\\sum_{j=0}^{\\infty}\\psi_jw_{t-j}\\] Se sigue inmediatamente que \\(\\mathbb{E}(x_t)=0\\). También, la función de autocovarianza de \\(x_t\\) se puede escribir como \\[\\begin{equation} \\gamma(h)=\\text{cov}(x_{t-h},x_t)=\\sigma_w^2\\sum_{j=0}^{\\infty}\\psi_j\\psi_{j+h}\\text{, }h\\geq0 \\tag{6.32} \\end{equation}\\] Podemos entonces usar (6.28) y (6.29) para resolver los \\(\\psi\\)-pesos. A su vez, podemos resolver para \\(\\gamma(h)\\) y la ACF \\(\\rho(h)=\\gamma(h)/\\gamma(0)\\). Como en el ejemplo 6.4, también es posible obtener una ecuación en diferencias homogénea directamente en términos de \\(\\gamma(h)\\). Primero, escribimos \\[\\begin{eqnarray} \\gamma(h) &amp;=&amp; \\text{cov}(x_{t+h},x_t)=\\mathbb{E}\\left[\\left(\\sum_{j=1}^{p}\\phi_jx_{t+h-j}+\\sum_{j=0}^{q}\\theta_jw_{t+h-j}\\right)x_t\\right]\\nonumber \\\\ &amp;=&amp; \\sum_{j=1}^{p}\\phi_j\\gamma(h-j)+\\sigma_w^2\\sum_{j=h}^{q}\\theta_j\\psi_{j-h}\\text{, }h\\geq0 \\tag{6.33} \\end{eqnarray}\\] donde hemos usado el hecho de que \\(x_t=\\sum_{k=0}^{\\infty}\\psi_jw_{t-k}\\) y para \\(h\\geq0\\), \\[\\mathbb{E}(w_{t+h-j}x_t)=\\mathbb{E}\\left[w_{t+h-j}\\left(\\sum_{k=0}^{\\infty}\\psi_kw_{t-k}\\right)\\right]=\\psi_{j-h}\\sigma_w^2.\\] De (6.33) podemos escribir una ecuación general homogénea para la ACF de un proceso ARMA causal: \\[\\begin{equation} \\gamma(h)-\\phi_1\\gamma(h-1)-\\cdots-\\phi_p\\gamma(h-p)=0\\text{, }h\\geq\\max(p,q+1) \\tag{6.34} \\end{equation}\\] con condiciones iniciales \\[\\begin{equation} \\gamma(h)-\\sum_{j=1}^{p}\\phi_j\\gamma(h-j)=\\sigma_w^2\\sum_{j=h}^{q}\\theta_j\\psi_{j-h}\\text{, }0\\leq h&lt;\\max(p,q+1). \\tag{6.35} \\end{equation}\\] Dividiendo (6.34) y (6.35) por \\(\\gamma(0)\\) nos permite resolver la ACF \\(\\rho(h)=\\gamma(h)/\\gamma(0)\\). Ejemplo 6.7 (La ACF de un proceso ARMA(1,1)) Consideremos el proceso ARMA(1,1) causal \\(x_t=\\phi x_{t-1}+\\theta w_{t-1}+w_t\\) donde \\(|\\phi|&lt;1\\). Basándonos en (6.34) la función de autocovarianza satisface \\[\\gamma(h)-\\phi\\gamma(h-1)=0\\text{, }h=2,3,\\ldots,\\] así, la solución general es \\(\\gamma(h)=c\\phi^h\\) para \\(h=1,2,\\ldots\\). Para obtener las condiciones iniciales, usamos (6.35): \\[\\begin{eqnarray*} \\gamma(0) &amp;=&amp; \\phi\\gamma(1)+\\sigma_w^2[1+\\theta\\phi+\\theta^2] \\\\ \\gamma(1) &amp;=&amp; \\phi\\gamma(0)+\\sigma_w^2\\theta \\end{eqnarray*}\\] Resolviendo para \\(\\gamma(0)\\) y \\(\\gamma(1)\\), obtenemos \\[\\begin{eqnarray*} \\gamma(0) &amp;=&amp; \\sigma_w^2\\frac{1+2\\theta\\phi+\\theta^2}{1-\\phi^2} \\\\ \\gamma(1) &amp;=&amp; \\sigma_w^2\\frac{(1+\\theta\\phi)(\\phi+\\theta)}{1-\\phi^2} \\end{eqnarray*}\\] Para resolver \\(c\\), note que \\(\\gamma(1)=c\\phi\\), en cuyo caso \\(c=\\gamma(1)/\\phi\\). Por consiguiente, la solución particular es \\[\\gamma(h)=\\sigma_w^2\\frac{(1+\\theta\\phi)(\\phi+\\theta)}{1-\\phi^2}\\phi^{h-1}\\] Finalmente, dividiendo por \\(\\gamma(0)\\) nos da la ACF \\[\\begin{equation} \\rho(h)=\\frac{(1+\\theta\\phi)(\\phi+\\theta)}{1+2\\theta\\phi+\\theta^2}\\phi^{h-1}\\text{, }h\\geq1 \\tag{6.36} \\end{equation}\\] 6.2 Pronósticos El objetivo en el pronóstico, es predecir los valores futuros de una serie de tiempo \\(x_{n+m}, m=1,2,\\ldots\\) basado en los valores de la serie observados hasta el tiempo actual \\(\\mathbf{x}=\\{x_n,x_{n-1},\\ldots,x_1\\}\\). En esta sección asumiremos que \\(x_t\\) es estacionario y que los parámetros del modelo son conocidos. El problema de hacer pronósticos cuando los parámetros del modelo son desconocidos se analizará en la siguiente sección. En los capítulos para modelos AR y modelos MA, vimos como realizar los pronósticos o predicciones para los mismos. A continuación daremos métodos más generales de predicción y que se pueden utilizar para los modelos AR, MA y ARMA. El mínimo del error cuadrático medio del predictor \\(x_{n+m}\\) es \\[x_{n+m}^n=\\mathbb{E}(x_{n+m}|x_n,x_{n-1},\\ldots,x_1)\\] porque el valor esperado condicional minimiza el error cuadrático medio \\[\\begin{equation} \\mathbb{E}[x_{n+m}-g(\\mathbf{x})]^2 \\tag{6.37} \\end{equation}\\] donde \\(g(\\mathbf{x})\\) es una función de las observaciones \\(\\mathbf{x}\\). Primero, nos restringiremos a los predictores que son función lineal de los datos, esto es, predictores de la forma \\[\\begin{equation} x_{n+m}^n=\\alpha_0+\\sum_{k=1}^{n}\\alpha_kx_k \\tag{6.38} \\end{equation}\\] donde \\(\\alpha_0,\\alpha_1,\\ldots,\\alpha_n\\) son números reales. Los predictores lineales de la forma () que minimizan el error cuadrático medio del predictor (6.37) son llamados el mejor predictor lineal (BLP’s). Como demostraremos luego, los predictores lineales dependen solo del segundo momento del proceso, lo cual es fácil de estimar a partir de los datos. A continuación daremos algunas propiedades y ejemplos. Proposición 6.3 (Mejor Predictor Lineal para Procesos Estacionarios) Dada las observaciones \\(x_1,x_2,\\ldots,x_n\\), el mejor predictor lineal \\(x_{n+m}^n=\\alpha_0+\\sum_{k=1}^{n}\\alpha_kx_k\\), de \\(x_{n+m}\\) para \\(m\\geq1\\), se halla resolviendo \\[\\begin{equation} \\mathbb{E}\\left[(x_{n+m}-x_{n+m}^n)x_k\\right]=0\\text{, para } k=0,1,2,\\ldots \\tag{6.39} \\end{equation}\\] donde \\(x_0=1\\). Las ecuaciones especificadas en (6.39) son llamadas ecuaciones de predicción, y son usadas para resolver los coeficientes \\(\\{\\alpha_0,\\alpha_1,\\ldots,\\alpha_n\\}\\). Si \\(\\mathbb{E}(x_t)=\\mu\\), la primera ecuación (\\(k=0\\)) de (6.39) implica \\[\\mathbb{E}(x_{n+m}^n)=\\mathbb{E}(x_{m+n})=\\mu.\\] Entonces, tomando valor esperado en (6.38), tenemos \\[\\mu=\\alpha_0+\\sum_{k=1}^{n}\\alpha_k\\mu\\text{ o }\\alpha_0=\\mu\\left(1-\\sum_{k=1}^{n}\\alpha_k\\right).\\] Por lo tanto, la forma del BLP es \\[x_{n+m}^n=\\mu+\\sum_{k=1}^{n}\\alpha_k(x_k-\\mu).\\] Sin perdida de generalidad, podemos considerar el caso \\(\\mu=0\\) en cuyo caso, \\(\\alpha_0=0\\) Consideremos primero la predicción de un paso. Esto es, dado \\(\\{x_1,x_2,\\ldots,x_n\\}\\), queremos predecir el valor la serie temporal en tiempo \\(t=n+1\\), o sea \\(x_{n+1}\\). El BLP de \\(x_{n+1}\\) es \\[\\begin{equation} x_{n+1}^n=\\phi_{n1}x_n+\\phi_{n2}x_{n-1}+\\cdots+\\phi_{nn}x_1 \\tag{6.40} \\end{equation}\\] donde, \\(\\alpha_k\\) en (6.38) lo escribiremos como \\(\\phi_{n,n+1-k}\\) en (6.40), para \\(k=1,2,\\ldots,n\\). Usando la proposición 6.3, los coeficientes \\(\\{\\phi_{n1},\\phi_{n2},\\ldots,\\phi_{nn}\\}\\) satisfacen \\[\\mathbb{E}\\left[\\left(x_{n+1}-\\sum_{j=1}^{n}\\phi_{nj}x_{n+1-j}\\right)x_{n+1-k}\\right]=0\\text{, para }k=1,2,\\ldots,n\\] o \\[\\begin{equation} \\sum_{j=1}^{n}\\phi_{nj}\\gamma(k-j)=\\gamma(k)\\text{, }k=1,2,\\ldots,n \\tag{6.41} \\end{equation}\\] Las ecuaciones de predicción (6.41) se pueden escribir en forma matricial como \\[\\begin{equation} \\Gamma_n\\vec{\\phi}_n=\\vec{\\gamma}_n \\tag{6.42} \\end{equation}\\] donde \\(\\Gamma_n=\\{\\gamma(k-j)\\}_{j,k=1}^n\\) es una matriz \\(n\\times n\\), \\(\\vec{\\phi}_n=(\\phi_{n1},\\phi_{n2},\\ldots,\\phi_{nn})^t\\) es un vector \\(n\\times1\\) y \\(\\vec{\\gamma}_n=(\\gamma(1),\\gamma(2),\\ldots,\\gamma(n))^t\\) es un vector \\(n\\times1\\). La matriz \\(\\Gamma_n\\) es no-negativa definida. Si \\(\\Gamma_n\\) es singular, existen muchas soluciones de (6.42), pero por el Teorema de Proyección (véase Cramer &amp; Leadbetter (1967)) \\(x_{n+1}^n\\) es único. Si \\(\\Gamma_n\\) es no singular, los elementos de \\(\\vec{\\phi}_n\\) son únicos, y están dados por \\[\\begin{equation} \\vec{\\phi}_n=\\Gamma_n^{-1}\\vec{\\gamma}_n. \\tag{6.43} \\end{equation}\\] Para un modelo ARMA, el hecho de que \\(\\sigma_w^2&gt;0\\) y \\(\\gamma(h)\\to0\\) cuando \\(h\\to\\infty\\) es suficiente para asegurar que \\(\\Gamma_n\\) es positiva definida. A veces es conveniente escribir el pronóstico de un paso en forma vectorial \\[\\begin{equation} x_{n+1}^n=\\vec{\\phi}^t_n\\mathbf{x} \\tag{6.44} \\end{equation}\\] donde \\(\\mathbf{x}=(x_1,x_2,\\ldots,x_n)^t\\). El error cuadrático medio de la predicción de un paso es \\[\\begin{equation} P_{n+1}^n=\\mathbb{E}(x_{n+1}-x_{n+1}^n)^2=\\gamma(0)-\\vec{\\gamma}^t_n\\Gamma_n^{-1}\\vec{\\gamma}_n. \\tag{6.45} \\end{equation}\\] Para verificar (6.45), usemos (6.43) y (6.44) \\[\\begin{eqnarray*} \\mathbb{E}(x_{n+1}-x_{n+1}^n)^2 &amp;=&amp; \\mathbb{E}(x_{n+1}-\\vec{\\phi}^t_n\\mathbf{x})^2=\\mathbb{E}(x_{n+1}-\\vec{\\gamma}^t_n\\Gamma_n^{-1}\\mathbf{x})^2 \\\\ &amp;=&amp; \\mathbb{E}(x_{n+1}^2-2\\vec{\\gamma}^t_n\\Gamma_n^{-1}\\mathbf{x}x_{n+1}+\\vec{\\gamma}^t_n\\Gamma_n^{-1}\\mathbf{xx^t}\\Gamma_n^{-1}\\vec{\\gamma}_n) \\\\ &amp;=&amp; \\gamma(0)-2\\vec{\\gamma}^t_n\\Gamma_n^{-1}\\vec{\\gamma}_n+\\vec{\\gamma}^t_n\\Gamma_n^{-1}\\Gamma_n\\Gamma_n^{-1}\\vec{\\gamma}_n \\\\ &amp;=&amp; \\gamma(0)-\\vec{\\gamma}^t_n\\Gamma_n^{-1}\\vec{\\gamma}^t_n. \\end{eqnarray*}\\] Ejemplo 6.8 (Predicción para un AR(2)) Suponga que tenemos un proceso AR(2) causal \\(x_t=\\phi_1x_{t-1}+\\phi_2x_{t-2}+w_t\\), y una observación \\(x_1\\). Entonces, usando la ecuación (6.43), la predicción de \\(x_2\\) basada en \\(x_1\\) es \\[x_2^1=\\phi_{11}x_1=\\frac{\\gamma(1)}{\\gamma(0)}x_1=\\rho(1)x_1\\] Ahora, supóngase que deseamos la predicción de \\(x_3\\) basado en dos observaciones \\(x_1\\) y \\(x_2\\). Podemos usar (6.43) de nuevo y resolver \\[x_3^2=\\phi_{21}x_2+\\phi_{22}x_1=(\\gamma(1),\\gamma(2))\\left( \\begin{array}{cc} \\gamma(0) &amp; \\gamma(1) \\\\ \\gamma(1) &amp; \\gamma(0) \\\\ \\end{array} \\right)^{-1}\\left( \\begin{array}{c} x_2 \\\\ x_1 \\\\ \\end{array} \\right) \\] pero, debe quedar claro a partir del modelo que \\(x_3^2=\\phi_1x_2+\\phi_2x_1\\). Dado que \\(\\phi_1x_2+\\phi_2x_1\\) satisface las ecuaciones de predicción (6.39) \\[\\begin{eqnarray*} \\mathbb{E}\\{[x_3-(\\phi_1x_2+\\phi_2x_1)]x_1\\} &amp;=&amp; \\mathbb{E}(w_3x_1)=0 \\\\ \\mathbb{E}\\{[x_3-(\\phi_1x_2+\\phi_2x_1)]x_2\\} &amp;=&amp; \\mathbb{E}(w_3x_2)=0 \\end{eqnarray*}\\] De ello se deduce que, de hecho \\(x_3^2=\\phi_1x_2+\\phi_2x_1\\), y por la unicidad de los coeficientes en este caso, que \\(\\phi_{21}=\\phi_1\\) y \\(\\phi_{22}=\\phi_2\\). Continuando de esta misma manera, es fácil verificar que para \\(n\\geq2\\) \\[x_{n+1}^n=\\phi_1x_n+\\phi_2x_{n-1}\\] Esto es, \\(\\phi_{n1}=\\phi_1\\), \\(\\phi_{n2}=\\phi_2\\) y \\(\\phi_{nj}=0\\) para \\(j=3,4,\\ldots,n\\) Del ejemplo 6.8, es claro que si la serie de tiempo es un proceso AR(p) causal, entonces para \\(n\\geq p\\) \\[\\begin{equation} x_{n+1}^n=\\phi_1x_n+\\phi_2x_{n-1}+\\cdots+\\phi_px_{n-p+1}. \\tag{6.46} \\end{equation}\\] Para modelos ARMA en general, las ecuaciones de predicción no serán tan simple como en el caso AR puro. Además, para \\(n\\) grande, el uso de (6.43) es prohibitivo, ya que requiere la inversión de una matriz grande. Sin embargo, existen soluciones iterativas que no requieren ninguna inversión de matriz. En particular, utilizaremos la solución recursiva de Levinson (1947) y Durbin (1960). Proposición 6.4 (Algoritmo de Durbin-Levinson) Las ecuaciones (6.43) y (6.45) se pueden resolver iterativamente como sigue: \\[\\begin{equation} \\phi_{00}=0\\text{, } P_1^0=\\gamma(0) \\tag{6.47} \\end{equation}\\] Para \\(n\\geq1\\) \\[\\begin{equation} \\phi_{nn}=\\frac{\\rho(n)-\\sum_{k=1}^{n-1}\\phi_{n-1,k}\\rho(n-k)}{1-\\sum_{k=1}^{n-1}\\phi_{n-1,k}\\rho(k)}\\text{, con }P_{n+1}^n=P_n^{n-1}(1-\\phi_{nn}^2) \\tag{6.48} \\end{equation}\\] donde, para \\(n\\geq2\\) \\[\\begin{equation} \\phi_{nk}=\\phi_{n-1,k}-\\phi_{nn}\\phi_{n-1,k-1}\\text{, para }k=1,2,\\ldots,n-1 \\tag{6.49} \\end{equation}\\] Ejemplo 6.9 (Uso del Algoritmo Durbin-Levinson) Para usar el algoritmo, iniciemos con \\(\\phi_{00}=0, P_1^0=\\gamma(0)\\). Entonces, para \\(n=1\\), \\[\\phi_{11}=\\rho(1)\\text{ y }P_2^1=\\gamma(0)[1-\\phi_{11}^2].\\] Para \\(n=2\\) \\[\\begin{eqnarray*} \\phi_{22} &amp;=&amp; \\frac{\\rho(2)-\\phi_{11}\\rho(1)}{1-\\phi_{11}\\rho(1)}=\\frac{\\rho(2)-\\rho(1)^2}{1-\\rho(1)^2} \\\\ \\phi_{21} &amp;=&amp; \\phi_{11}-\\phi_{22}\\phi_{11}=\\rho(1)[1-\\phi_{22}] \\\\ P_3^2 &amp;=&amp; \\gamma(0)[1-\\phi_{11}^2][1-\\phi_{22}^2] \\end{eqnarray*}\\] Para \\(n=3\\) \\[\\phi_{33}=\\frac{\\rho(3)-\\phi_{21}\\rho(2)-\\phi_{22}\\rho(1)}{1-\\phi_{21}\\rho(1)-\\phi_{22}\\rho(2)}\\] y así sucesivamente. Una consecuencia importante del algoritmo de Durbin-Levinson es la siguiente propiedad. Proposición 6.5 (Solución Iterativa para la PACF) La PACF de un proceso estacionario \\(x_t\\), se puede obtener via iteración de \\[\\phi_{nn}=\\frac{\\rho(n)-\\sum_{k=1}^{n-1}\\phi_{n-1,k}\\rho(n-k)}{1-\\sum_{k=1}^{n-1}\\phi_{n-1,k}\\rho(k)}\\text{, con }P_{n+1}^n=P_n^{n-1}(1-\\phi_{nn}^2)\\] como \\(\\phi_{nn}\\), para \\(n=1,2,\\ldots\\) Ejemplo 6.10 (La PACF de un AR(2)) Del ejemplo 4.4, sabemos que para un AR(2), \\(\\phi_{hh}=0\\) para \\(h&gt;2\\), pero usaremos los resultados del ejemplo 6.8 y la proposición 6.5 para calcular los primeros tres valores de la PACF. Recuerde (Ejemplo 6.4) que para un AR(2), \\(\\rho(1)=\\phi_1/(1-\\phi_2)\\) y en general \\(\\rho(h)-\\phi_1\\rho(h-1)-\\phi_2\\rho(h-2)=0\\) para \\(h\\geq2\\). Entonces \\[\\begin{eqnarray*} \\phi_{11} &amp;=&amp; \\rho(1)=\\frac{\\phi_1}{1-\\phi_2} \\\\ \\phi_{22} &amp;=&amp; \\frac{\\rho(2)-\\rho(1)^2}{1-\\rho(1)^2}=\\frac{\\left[\\phi_1\\left(\\frac{\\phi_1}{1-\\phi_2}\\right)+\\phi_2\\right]-\\left(\\frac{\\phi_1}{1-\\phi_2}\\right)^2}{1-\\left(\\frac{\\phi_1}{1-\\phi_2}\\right)^2}=\\phi_2 \\\\ \\phi_{21} &amp;=&amp; \\phi_1 \\\\ \\phi_{33} &amp;=&amp; \\frac{\\rho(3)-\\phi_1\\rho(2)-\\phi_2\\rho(1)}{1-\\phi_1\\rho(1)-\\phi_2\\rho(2)}=0. \\end{eqnarray*}\\] Hasta ahora nos hemos concentrado en la predicción de un paso, pero la proposición 6.3 nos permite calcular el BLP de \\(x_{n+m}\\) para cada \\(m\\geq1\\). Dado los datos \\(\\{x_1,x_2,\\ldots,x_n\\}\\) el predictor de \\(m\\) pasos es \\[\\begin{equation} x_{n+m}^n=\\phi_{n1}^{(m)}x_n+\\phi_{n2}^{(m)}x_{n-1}+\\cdots+\\phi_{nn}^{(m)}x_1 \\tag{6.50} \\end{equation}\\] donde \\(\\{\\phi_{n1}^{(m)},\\phi_{n2}^{(m)},\\ldots,\\phi_{nn}^{(m)}\\}\\) satisfacen las ecuaciones de predicción \\[\\sum_{j=1}^{n}\\phi_{nj}^{(m)}\\mathbb{E}(x_{n+1-j}x_{n+1-k})=\\mathbb{E}(x_{n+m}x_{n+1-k})\\text{, }k=1,2,\\ldots,n\\] o \\[\\begin{equation} \\sum_{j=1}^{n}\\phi_{nj}^{(m)}\\gamma(k-j)=\\gamma(m+k-1)\\text{, }k=1,2,\\ldots,n \\tag{6.51} \\end{equation}\\] Las ecuaciones de predicción se pueden escribir nuevamente en forma matricial como \\[\\begin{equation} \\Gamma_n\\vec{\\phi}_n^{(m)}=\\vec{\\gamma}_n^{(m)} \\tag{6.52} \\end{equation}\\] donde \\(\\vec{\\gamma}_n^{(m)}=(\\gamma(m),\\ldots,\\gamma(m+n-1))^t\\) y \\(\\vec{\\phi}_n^{(m)}=(\\phi_{n1}^{(m)},\\phi_{n2}^{(m)},\\ldots,\\phi_{nn}^{(m)})^t\\) son vectores \\(n\\times1\\). El error cuadrático medio del predictor de \\(m\\) pasos es \\[\\begin{equation} P_{n+m}^n=\\mathbb{E}(x_{n+m}-x_{n+m}^n)^2=\\gamma(0)-(\\vec{\\gamma}_n^{(m)})^t\\Gamma_n^{-1}\\vec{\\gamma}_n^{(m)} \\tag{6.53} \\end{equation}\\] Otro algoritmo útil para calcular pronósticos es dado por Brockwell &amp; Davis (1996)[]. Este algoritmo se obtiene por aplicación directa del Teorema de Proyección a \\(x_t-x_t^{t-1}\\) para \\(t=1,2,\\ldots,n\\) usando el hecho de que \\(x_t-x_t^{t-1}\\) y \\(x_s-x_s^{s-1}\\) son no-correlacionados para \\(s\\neq t\\). Presentamos el caso en el cual \\(x_t\\) es una serie de tiempo estacionaria de media cero. Proposición 6.6 (Algoritmo de Innovaciones) Los predictores \\(x_{t+1}^t\\) y sus errores cuadráticos medios \\(P_{t+1}^t\\) se pueden calcular iterativamente como \\[x_1^0=0\\text{, }P_1^0=\\gamma(0)\\] \\[\\begin{equation}\\label{} x_{t+1}^t=\\sum_{j=1}^{t}\\theta_{tj}(x_{t+1-j}-x_{t+1-j}^{t-j})\\text{, } t=1,2,\\ldots \\tag{6.54} \\end{equation}\\] \\[\\begin{equation} P_{t+1}^t=\\gamma(0)-\\sum_{j=0}^{t-1}\\theta_{t,t-j}^2P_{j+1}^j\\text{, }t=1,2,\\ldots \\tag{6.55} \\end{equation}\\] donde, para \\(j=0,1,\\ldots,t-1\\), \\[\\begin{equation} \\theta_{t,t-j}=\\left(\\gamma(t-j)-\\sum_{k=0}^{j-1}\\theta_{j,j-k}\\theta_{t,t-k}P_{k+1}^k\\right)\\left(P_{j+1}^j\\right)^{-1} \\tag{6.56} \\end{equation}\\] Dado los datos \\(x_1,x_2,\\ldots,x_n\\) el algoritmo de innovación se puede calcular sucesivamente para \\(t=1\\), luego \\(t=2\\) y así hasta \\(t=n\\), en cuyo caso obtenemos el predictor \\(x_{n+1}^n\\) y el error cuadrático medio \\(P_{n+1}^n\\). El predictor de \\(m\\) pasos y el error cuadrático medio basado en el algoritmo de innovación son dados por \\[\\begin{eqnarray} x_{n+m}^n &amp;=&amp; \\sum_{j=m}^{n+m-1}\\theta_{n+m-1,j}(x_{n+m-j}-x_{n+m-j}^{n+m-j-1})\\tag{6.57} \\\\ P_{n+m}^n &amp;=&amp; \\gamma(0)-\\sum_{j=m}^{n+m-1}\\theta_{n+m-1,j}^2P_{n+m-j}^{n+m-j-1}\\tag{6.58} \\end{eqnarray}\\] donde los \\(\\theta_{n+m-1,j}\\) se obtienen por iteración continua de (6.56). Ejemplo 6.11 (Predicción de un MA(1)) El algoritmo de innovación nos da un buen predictor para un proceso de promedio móvil. Considere un modelo MA(1), \\(x_t=w_t+\\theta x_{t-1}\\). Recuerde que \\(\\gamma(0)=(1+\\theta^2)\\sigma_w^2, \\gamma(1)=\\theta\\sigma_w^2\\) y \\(\\gamma(h)=0\\) para \\(h&gt;1\\). Entonces, usando la proposición 6.6, tenemos, \\[\\begin{eqnarray*} \\theta_{n1} &amp;=&amp; \\theta\\sigma_w^2/P_n^{n-1} \\\\ \\theta_{nj} &amp;=&amp; 0\\text{, para }j=2,3,\\ldots,n \\\\ P_1^0 &amp;=&amp; (1+\\theta^2)\\sigma_w^2 \\\\ P_{n+1}^n &amp;=&amp; (1+\\theta^2-\\theta\\theta_{n1})\\sigma_w^2 \\end{eqnarray*}\\] Finalmente, de (6.54) el predictor de un paso es \\[x_{n+1}^n=\\theta(x_n-x_n^{n-1})\\sigma_w^2/P_n^{n-1}\\] 6.2 Pronósticos para procesos ARMA Las ecuaciones de predicción general (6.39) nos dan una pequeña intuición en el pronóstico de los modelos ARMA en general. Hay diferentes maneras de expresar estos pronósticos, y cada uno ayuda a entender la estructura especial de la predicción ARMA. A través de toda esta sección asumiremos que \\(x_t\\) es un proceso ARMA(p,q) causal e invertible \\(\\phi(B)x_t=\\theta(B)w_t\\) donde \\(w_t\\sim\\text{iid}N(0,\\sigma_w^2)\\). En el caso de media distinto de cero, \\(\\mathbb{E}(x_t)=\\mu\\), reemplazamos \\(x_t\\) por \\(x_t-\\mu\\) en el modelo. Primero consideraremos dos tipos de pronósticos. Escribiremos el mínimo del error cuadrático medio del predictor \\(x_{n+m}\\) como \\(x_{n+m}^n\\) basado en los datos \\(\\{x_n,x_{n-1},\\ldots,x_1\\}\\), esto es \\[x_{n+m}^n=\\mathbb{E}(x_{n+m}|x_n,x_{n-1},\\ldots,x_1).\\] Para un modelo ARMA, es fácil calcular el predictor de \\(x_{n+m}\\) asumiendo que tenemos el historial completo del proceso \\(\\{x_n,x_{n-1},\\ldots\\}\\). Denotaremos el predictor de \\(x_{n+m}\\) basado en infinitos valores pasados como \\[\\tilde{x}_{n+m}=\\mathbb{E}(x_{n+m}|x_n,x_{n-1},\\ldots).\\] La idea aquí, es que para muestra grandes \\(\\tilde{x}_{n+m}\\) proveerá una buena aproximación de \\(x_{n+m}^n\\). Ahora, escribamos \\(x_{n+m}\\) en sus formas causal e invertible \\[\\begin{eqnarray} x_{n+m} &amp;=&amp; \\sum_{j=0}^{\\infty}\\psi_jw_{n+m-j}\\text{, }\\psi_0=1 \\tag{6.59} \\\\ w_{n+m} &amp;=&amp; \\sum_{j=0}^{\\infty}\\pi_jx_{n+m-j}\\text{, }\\pi_0=1 \\tag{6.60} \\end{eqnarray}\\] Entonces, tomando esperanza condicional en (6.59), tenemos \\[\\begin{equation} \\tilde{x}_{n+m}=\\sum_{j=0}^{\\infty}\\psi_j\\tilde{w}_{n+m-j}=\\sum_{j=m}^{\\infty}\\psi_jw_{n+m-j} \\tag{6.61} \\end{equation}\\] ya que por (6.60) \\[\\tilde{w}_t\\equiv\\mathbb{E}(w_t|x_n,x_{n-1},\\ldots)=\\begin{cases} 0,&amp; t&gt;n\\\\ w_t,&amp; t\\leq n \\end{cases}\\] Similarmente, tomando esperanza condicional en (6.60), se tiene \\[0=\\tilde{x}_{n+m}+\\sum_{j=1}^{\\infty}\\pi_j\\tilde{x}_{n+m-j}\\] o \\[\\begin{equation} \\tilde{x}_{n+m}=-\\sum_{j=1}^{m-1}\\pi_j\\tilde{x}_{n+m-j}-\\sum_{j=m}^{\\infty}\\pi_jx_{n+m-j} \\tag{6.62} \\end{equation}\\] usando el hecho de que \\(\\mathbb{E}(x_t|x_n,x_{n-1},\\ldots)=x_t\\) para \\(t\\leq n\\). La predicción se consigue recursivamente usando (6.62) iniciando con un predictor de un paso \\(m=1\\) y continuando para \\(m=2,3,\\ldots\\). Usando (6.62) podemos escribir \\[x_{n+m}-\\tilde{x}_{n+m}=\\sum_{j=0}^{m-1}\\psi_jw_{n+m-j}\\] de modo que el error cuadrático medio de predicción se puede escribir como \\[\\begin{equation} P_{n+m}^n=\\mathbb{E}(x_{n+m}-\\tilde{x}_{n+m})^2=\\sigma_w^2\\sum_{j=0}^{m-1}\\psi_j^2 \\tag{6.63} \\end{equation}\\] También, observe que para una muestra fija de tamaño \\(n\\) los errores de predicción están correlacionados. Esto es, para \\(k\\geq1\\), \\[\\begin{equation} \\mathbb{E}[(x_{n+m}-\\tilde{x}_{n+m})(x_{n+m+k}-\\tilde{x}_{n+m+k})]=\\sigma_w^2\\sum_{j=0}^{m-1}\\psi_j\\psi_{j+k} \\tag{6.64} \\end{equation}\\] Ejemplo 6.12 (Pronóstico a largo plazo) Consideremos el pronóstico para un proceso ARMA de media \\(\\mu\\). Del caso de media cero en (6.61) podemos deducir que el pronóstico de \\(m\\) pasos se puede escribir como \\[\\begin{equation} \\tilde{x}_{n+m}=\\mu+\\sum_{j=m}^{\\infty}\\psi_jw_{n+m-j} \\tag{6.65} \\end{equation}\\] Note que los \\(\\psi\\) pesos decrece a cero de forma exponencial, es claro entonces que \\[\\tilde{x}_{n+m}\\to\\mu\\] exponencialmente (en el sentido de media cuadrado) cuando \\(m\\to\\infty\\). Más aún, por (6.63) el error cuadrático medio de predicción \\[\\begin{equation} P_{n+m}^n\\to\\sigma_w^2\\sum_{j=0}^{\\infty}\\psi_j^2, \\tag{6.66} \\end{equation}\\] exponencialmente cuando \\(m\\to\\infty\\). Es claro de (6.65) y (6.66) que el pronóstico de un proceso ARMA rápidamente se estabiliza a la media con un error de predicción constante a medida que el periodo de pronóstico \\(m\\) crece. Cuando \\(n\\) es pequeño, las ecuaciones generales de predicción (6.39) se puede usar fácilmente. Cuando \\(n\\) es grande, usaremos (6.61) por truncamiento, porque solo tenemos disponibles las observaciones \\(x_1,x_2,\\ldots,x_n\\). En este caso truncamos (6.61) haciendo \\[\\sum_{j=n+m}^{\\infty}\\pi_jx_{n+m-j}=0\\] El predictor truncado se escribe entonces como \\[\\begin{equation} \\tilde{x}_{n+m}^n=-\\sum_{j=1}^{m-1}\\pi_j\\tilde{x}_{n+m-j}^n-\\sum_{j=m}^{n+m-1}\\pi_jx_{n+m-j} \\tag{6.67} \\end{equation}\\] el cual es también calculado recursivamente para \\(m=1,2,\\ldots\\). El error cuadrático medio de predicción, en este caso, se aproxima por (6.63). Para un modelo AR(p) y cuando \\(n&gt;p\\) la ecuación (6.46) nos da el predictor exacto \\(x_{n+m}^n\\) de \\(x_{n+m}\\) y no hay necesidad de aproximación. Esto es, para \\(n&gt;p\\), \\(\\tilde{x}_{n+m}^n=\\tilde{x}_{n+m}=x_{n+m}^n\\). También, en este caso, el error de predicción de un paso es \\(\\mathbb{E}(x_{n+1}-x_{n+1}^n)^2=\\sigma_w^2\\). Para un modelo ARMA(p,q) en general, los predictores truncados para \\(m=1,2,\\ldots\\), son \\[\\begin{equation} \\tilde{x}_{n+m}^n=\\phi_1\\tilde{x}_{n+m}^n+\\cdots+\\phi_p\\tilde{x}_{n+m-p}^n+\\theta_1\\tilde{w}_{n+m-1}^n+\\cdots+\\theta_q\\tilde{w}_{n+m-q}^n \\tag{6.68} \\end{equation}\\] donde \\(\\tilde{x}_t^n=x_t\\) para \\(1\\leq t\\leq n\\) y \\(\\tilde{x}_t^n=0\\) para \\(t\\leq0\\). Los errores de predicción truncados están dados por: \\[\\begin{equation*} \\begin{cases} \\tilde{w}_t^n=0&amp;\\text{ para }t\\leq0\\text{ ó }t&gt;n\\\\ \\tilde{w}_t^n=\\phi(B)\\tilde{x}_t^n-\\theta_1\\tilde{w}_{t-1}^n-\\cdots-\\theta_q\\tilde{w}_{t-q}^n&amp;\\text{ para }1\\leq t\\leq n. \\end{cases} \\end{equation*}\\] Ejemplo 6.13 (Pronóstico para una serie ARMA(1,1)) Dado los datos \\(x_1,x_2,\\ldots,x_n\\) para propósito de pronósticos, escribiremos el modelo como \\[x_{n+1}=\\phi x_n+w_{n+1}+\\theta w_n.\\] Entonces, basado en (6.68), el pronóstico truncado de un paso es \\[\\tilde{x}_{n+1}^n=\\phi x_n+0+\\theta\\tilde{w}_n^n.\\] Para \\(m\\geq2\\), tenemos \\[\\tilde{x}_{n+m}^n=\\phi\\tilde{x}_{n+m-1}^n,\\] el cual puede ser calculado recursivamente para \\(m=1,2,\\ldots\\). Para calcular \\(\\tilde{w}_n^n\\), que se necesitará para iniciar los pronósticos sucesivos, podemos escribir el modelo como \\(w_t=x_t-\\phi x_{t-1}-\\theta w_{t-1}\\) para \\(t=1,2,\\ldots,n\\). Para el pronóstico truncado, usando (6.68) hacemos \\(\\tilde{w}_0^n=0, \\tilde{w}_1^n=x_1\\) y entonces iteramos el error \\[\\tilde{w}_t^n=x_t-\\phi x_{t-1}-\\theta\\tilde{w}_{t-1}^n\\text{, }t=2,3,\\ldots,n.\\] La varianza aproximada del pronóstico se calcula de (6.63) usando los \\(\\psi\\) pesos determinados como en el ejemplo 6.6. En particular, los \\(\\psi\\) pesos satisfacen \\(\\psi_j=(\\phi+\\theta)\\phi^{j-1}\\) para \\(j\\geq1\\). Este resultado nos da \\[\\begin{eqnarray*} P_{n+m}^n &amp;=&amp; \\sigma_w^2\\left[1+(\\phi+\\theta)^2\\sum_{j=1}^{m-1}\\phi^{2(j-1)}\\right] \\\\ &amp;=&amp; \\sigma_w^2\\left[1+\\frac{(\\phi+\\theta)^2(1-\\phi^{2(m-1)})}{(1-\\phi^2)}\\right] \\end{eqnarray*}\\] Para evaluar la precisión de los pronósticos, se calculan los intervalos de predicción junto con el pronóstico. En general, los \\((1-\\alpha)\\) intervalos de predicción son de la forma \\[\\begin{equation} x_{n+m}^n\\pm c_{\\frac{\\alpha}{2}}\\sqrt{P_{n+m}^n} \\tag{6.69} \\end{equation}\\] donde \\(c_{\\alpha/2}\\) se elige de manera de obtener el grado deseado de confidencia. Por ejemplo, si el proceso es gaussiano, entonces elegimos \\(c_{\\alpha/2}=2\\) los cual nos da un intervalo de predicción de aproximadamente 95% para \\(x_{n+m}\\). Si estamos interesados en establecer un intervalos de predicción sobre más de un periodo de tiempo, entonces \\(c_{\\alpha/2}\\) se ajustará apropiadamente, por ejemplo, usando la desigualdad de Bonferroni. (véase Shumway (2006), Capítulo 4) Ejemplo 6.14 (Pronóstico para la serie de nuevos peces) Usando los parámetros estimados como los valores actuales de los parámetros, la figura 6.3 muestra los resultados de la serie de nuevos peces dada en el ejemplo 4.5, sobre un periodo de 24 meses \\(m=1,2,\\ldots,24\\). Los pronósticos actuales se calculan como \\[x_{n+m}^n=6.74+1.35x_{n+m-1}^n-0.46x_{n+m-2}^n\\] para \\(n=453\\) y \\(m=1,2,\\ldots,12\\). Recuerde que \\(x_t^s=x_t\\) cuando \\(t\\leq s\\). Los errores de pronóstico \\(P_{n+m}^n\\) se calculan usando (6.63). Recuerde que \\(\\hat{\\sigma}_w^2=90.31\\), y usando (6.28) del ejemplo 4.5 tenemos \\(\\psi_j=1.35\\psi_{j-1}-0.46\\psi_{j-2}\\) para \\(j\\geq2\\), donde \\(\\psi_0=1\\) y \\(\\psi_1=1.35\\). Entonces para \\(n=453\\) \\[\\begin{eqnarray*} P_{n+1}^n &amp;=&amp; 90.31 \\\\ P_{n+2}^n &amp;=&amp; 90.31(1+1.35^2) \\\\ P_{n+3}^n &amp;=&amp; 90.31(1+1.35^2+[1.35^2-0.46^2]) \\end{eqnarray*}\\] y así sucesivamente. Note como el pronóstico se nivela rápidamente y los intervalos de predicción son amplios, aún cuando en este caso los límites están basados en un solo error estándar; esto es, \\(x_{n+m}^n\\pm\\sqrt{P_{n+m}^n}\\). Figura 6.3: 24 meses de pronósticos para la serie de reclutamientos (nuevos peces) Completaremos está sección con una breve discusión de retroproyección. En retroproyección deseamos predecir \\(x_{1-m}\\), \\(m=1,2,\\ldots\\), basado en los datos \\(\\{x_1,x_2,\\ldots,x_n\\}\\). Escribamos la retroproyección como \\[\\begin{equation} x_{1-m}^n=\\sum_{j=1}^{n}\\alpha_jx_j \\tag{6.70} \\end{equation}\\] Análogamente a (6.51), las ecuaciones de predicción (asumiendo \\(\\mu=0\\)) son \\[\\begin{eqnarray} \\sum_{j=1}^{n}\\alpha_j\\mathbb{E}(x_jx_k) &amp;=&amp; \\mathbb{E}(x_{1-m}x_k)\\text{, }k=1,\\ldots,n\\text{ ó } \\tag{6.71} \\\\ \\sum_{j=1}^{n}\\alpha_j\\gamma(k-j) &amp;=&amp; \\gamma(m+k-1)\\text{, }k=1,\\ldots,n \\tag{6.72} \\end{eqnarray}\\] Estas ecuaciones son precisamente las ecuaciones de predicción para predicción a futuro. Estos es, \\(\\alpha_j\\equiv\\phi_{nj}^{(m)}\\) para \\(j=1,\\ldots,n\\) donde los \\(\\phi_{nj}^{(m)}\\) están dados por (6.52). Finalmente las retroproyecciones están dadas por \\[\\begin{equation} x_{1-m}^n=\\phi_{nj}^{(m)}x_1+\\ldots+\\phi_{nn}^{(m)}x_n\\text{, con }m=1,2,\\ldots \\tag{6.73} \\end{equation}\\] Ejemplo 6.15 (Retroproyección de un proceso ARMA(1,1)) Considere un proceso ARMA(1,1) causal e invertible \\(x_t=\\phi x_{t-1}+\\theta w_{t-1}+w_t\\), llamaremos a este, modelo hacia adelante. Hemos visto que el mejor predictor lineal hacia atrás en el tiempo es el mismo predictor lineal hacia adelante en el tiempo para procesos estacionarios. Dado que estamos suponiendo que el modelo ARMA es gaussiano, tenemos que el mínimo error cuadrático medio de predicción hacia atrás es el mismo que hacia adelante para modelos ARMA. Entonces, el proceso se puede generar equivalentemente por un modelo hacia atrás \\(x_t=\\phi x_{t+1}+\\theta v_{t+1}+v_t\\) donde \\(\\{v_t\\}\\) es un ruido blanco gaussiano con varianza \\(\\sigma_w^2\\). [^nota8] [^nota8:] En el caso estacionario gaussiano (a) la distribución de \\(\\{x_{n+1},x_n,\\ldots,x_1\\}\\) es la misma que (b) la distribución de \\(\\{x_0,x_1,\\ldots,x_n\\}\\). En pronóstico usamos (a) para obtener \\(\\mathbb{E}(x_{n+1}|x_n,\\ldots,x_1)\\); en retroproyección usamos (b) para obtener \\(\\mathbb{E}(x_0|x_1,\\ldots,x_n)\\). Dado que (a) y (b) son iguales, los dos problemas son equivalentes. Escribiremos \\(x_t=\\sum_{j=0}^{\\infty}\\psi_jv_{t+j}\\) donde \\(\\psi_0=1\\); esto significa que \\(x_t\\) es no-correlacionado con \\(\\{v_{t-1},v_{t-2},\\ldots\\}\\), en analogía con el modelo a futuro. Dado los datos \\(\\{x_1,x_2,\\ldots,x_n\\}\\), truncamos \\(v_t=\\mathbb{E}(v_n|x_1,\\ldots,x_n)\\) a cero. Esto es, hacemos \\(\\tilde{v}_n^n=0\\), como aproximación inicial, y entonces generamos los errores \\[\\tilde{v}_t^n=x_t-\\phi x_{t+1}+\\theta\\tilde{v}_{t+1}^n\\text{, con }t=(n-1),(n-2),\\ldots,1.\\] Entonces \\[\\tilde{x}_0^n=\\phi x_1+\\theta\\tilde{v}_1^n+\\tilde{v}_0^n=\\phi x_1+\\theta\\tilde{v}_1^n.\\] porque \\(\\tilde{v}_t^n=0\\) para \\(t\\leq0\\). Continuando, las retroproyecciones truncadas general están dadas por \\[\\tilde{x}_{1-m}^n=\\phi\\tilde{x}_{2-m}^n\\text{, para }m=2,3,\\ldots\\] 6.2 Estimación de parámetros 6.2 Estimación A lo largo de esta sección, supongamos que tenemos \\(n\\) observaciones, \\(x_1,\\ldots,x_n\\), a partir de un proceso ARMA(p,q) gaussiano causal e invertible en el que, inicialmente, los parámetros de orden, \\(p\\) y \\(q\\), son conocidos. Nuestro objetivo es estimar los parámetros, \\(\\phi_1,\\ldots,\\phi_p,\\theta_1,\\ldots,\\theta_q\\) y \\(\\sigma_w^2\\). Vamos a discutir el problema de determinar \\(p\\) y \\(q\\) más adelante en esta sección. Comenzamos con el método de estimación de momentos. La idea detrás de estos estimadores es el de igualar los momentos de la población a los momentos de la muestra y luego resolver para los parámetros en términos de los momentos de la muestra. Inmediatamente vemos que, si \\(\\mathbb{E}(x_t)=\\mu\\), entonces estimador de momentos de \\(\\mu\\) es el promedio de la muestra \\(\\bar{x}\\). Por lo tanto, mientras se discute el método de momentos, vamos a suponer \\(\\mu=0\\). Aunque el método de momentos puede producir buenos estimadores, a veces puede conducir a estimadores subóptimos. En primer lugar, consideremos el caso en el cual el método conduce a un estimador óptimo (eficiente), esto es, un modelo AR(p). Cuando el proceso es AR(p), \\[x_t=\\phi_1x_{t-1}+\\cdots+\\phi_px_{t-p}+w_t,\\] las primeras \\(p+1\\) ecuaciones de (6.34) y (6.35) conducen a la siguiente definición: Definición 6.5 Las ecuaciones de Yule-Walker están dadas por \\[\\begin{eqnarray} \\gamma(h) &amp;=&amp; \\phi_1\\gamma(h-1)+\\cdots\\phi_p\\gamma(h-p),\\quad h=1,2,\\ldots,p \\tag{6.74}\\\\ \\sigma_w^2 &amp;=&amp; \\gamma(0)-\\phi_1\\gamma(1)-\\cdots-\\phi_p\\gamma(p) \\tag{6.75} \\end{eqnarray}\\] En notacion matricial, las ecuaciones de Yule-Walker son: \\[\\begin{equation} \\Gamma_p\\mathbf{\\phi}=\\mathbf{\\gamma}_p, \\sigma_w^2=\\gamma(0)-\\mathbf{\\phi}^t\\mathbf{\\gamma}_p, \\tag{6.76} \\end{equation}\\] donde \\(\\Gamma_p=\\{\\gamma(k-j)\\}_{j,k=1}^p\\) es una matriz de orden \\(p\\times p\\), \\(\\mathbf{\\phi}=(\\phi_1,\\ldots,\\phi_p)^t\\) es un vector \\(p\\times1\\) y \\(\\mathbf{\\gamma}_p=(\\gamma(1),\\ldots,\\gamma(p))^t\\) es un vector \\(p\\times1\\). Usando el método de los momentos, reemplazamos \\(\\gamma(h)\\) en (6.76) por \\(\\hat{\\gamma}(h)\\) y resolvemos \\[\\begin{equation} \\hat{\\mathbf{\\phi}} = \\hat{\\Gamma}_p^{-1}\\hat{\\mathbf{\\gamma}}_p, \\hat{\\sigma}_w^2 = \\hat{\\gamma}(0)-\\hat{\\mathbf{\\gamma}}_p^t\\hat{\\Gamma}_p^{-1}\\hat{\\mathbf{\\gamma}}_p. \\tag{6.77} \\end{equation}\\] Estos estimadores son llamados estimadores de Yule-Walker. Para propósitos de cálculo es a veces más conveniente trabajar con la ACF muestral. Factorizando \\(\\hat{\\gamma}(0)\\) en (6.77) podemos escribir los estimadores de Yule-Walker como: \\[\\begin{equation} \\hat{\\mathbf{\\phi}} = \\hat{\\mathbf{R}}_p^{-1}\\hat{\\mathbf{\\rho}}_p, \\hat{\\sigma}_w^2 = \\hat{\\gamma}(0)\\left[1-\\hat{\\mathbf{\\rho}}_p^t\\hat{\\mathbf{R}}_p^{-1}\\hat{\\mathbf{\\rho}}_p\\right], \\tag{6.78} \\end{equation}\\] donde \\(\\hat{\\mathbf{R}}_p=\\{\\hat{\\rho}(k-j)\\}_{j,k=1}^p\\) es una matriz de orden \\(p\\times p\\) y \\(\\hat{\\mathbf{\\rho}}_p=(\\hat{\\rho}(1),\\ldots,\\hat{\\rho}_p)^t\\) es un vector \\(p\\times1\\). Para un modelo \\(AR(p)\\), si el tamaño de la muestra es grande, los estimadores de Yule-Walker tienen distribución aproximadamente normal y \\(\\hat{\\sigma}_w^2\\) es cercano al valor real de \\(\\sigma_w^2\\). Establecemos este resultado en la proposición 6.7. Proposición 6.7 (Resultado de muestras de tamaño grande para los estimadores de Yule-Walker) El comportamiento asintótico (\\(n\\to\\infty\\)) de los estimadores de Yule-Walker en el caso de un proceso AR(p) causal es como sigue: \\[\\begin{equation} \\sqrt{n}(\\hat{\\mathbf{\\phi}}-\\mathbf{\\phi})\\stackrel{d}{\\to} N(\\mathbf{0},\\sigma_w^2\\Gamma_p^{-1}),\\qquad \\hat{\\sigma}_w^2\\stackrel{p}{\\to}\\sigma_w^2 \\tag{6.79} \\end{equation}\\] El algoritmo de Durbin-Levinson, (6.47) a (6.49), se puede usar para calcular \\(\\hat{\\mathbf{\\phi}}\\) sin invertir \\(\\hat{\\Gamma}_p\\) o \\(\\hat{\\mathbf{R}}_p\\), reemplazando \\(\\gamma(h)\\) por \\(\\hat{\\gamma}(h)\\) en el algoritmo. En la corrida del algoritmo, iterativamente calculamos el \\(h\\times1\\) vector, \\(\\hat{\\mathbf{\\phi}}_h=(\\hat{\\phi}_{h1},\\ldots,\\hat{\\phi}_{hh})^t\\), para \\(h=1,2,\\ldots\\). Por lo tanto, además de obtener el pronóstico deseado, el algoritmo de Durbin-Levinson nos da \\(\\hat{\\phi}_{hh}\\), la PACF muestral. Usando (6.79) se puede demostrar la siguiente propiedad. Proposición 6.8 (Distribución de PACF para muestras grandes) Para un proceso \\(AR(p)\\) causal, asintóticamente (\\(n\\to\\infty\\)) \\[\\begin{equation} \\sqrt{n}\\hat{\\phi}_{hh}\\stackrel{d}{\\to}N(0,1),\\text{ para } h&gt;p. \\tag{6.80} \\end{equation}\\] Ejemplo 6.16 (Estimación de Yule-Walker para un proceso AR(2)) Los datos mostrados en la figura 6.1 son \\(n=144\\) observaciones simuladas de un modelo AR(2) \\[x_t=1.5x_{t-1}-0.75x_{t-2}+w_t,\\] donde \\(w_t\\sim iid N(0,1)\\). Para estos datos, \\(\\hat{\\gamma}(0)=8.434, \\hat{\\rho}(1)=0.834\\), y \\(\\hat{\\rho}(2)0=.476\\). En consecuencia, \\[\\hat{\\mathbf{\\phi}} = \\left( \\begin{array}{c} \\hat{\\phi}_1 \\\\ \\hat{\\phi}_2 \\\\ \\end{array} \\right) = \\left[ \\begin{array}{cc} 1 &amp; 0.834 \\\\ 0.834 &amp; 1 \\\\ \\end{array} \\right]^{-1}\\left( \\begin{array}{c} 0.834 \\\\ 0.476 \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} 1.439 \\\\ -0.725 \\\\ \\end{array} \\right) \\] y \\[\\hat{\\sigma}_w^2 = 8.434\\left[1-(0.834,0.476)\\left( \\begin{array}{c} 1.439 \\\\ -0.725 \\\\ \\end{array} \\right)\\right] = 1.215. \\] Por la proposición 6.7, la matriz de varianza-covarianzas asintótica de \\(\\hat{\\mathbf{\\phi}}\\), \\[\\frac{1}{144}\\frac{1.215}{8.434}\\left[ \\begin{array}{cc} 1 &amp; 0.834 \\\\ 0.834 &amp; 1 \\\\ \\end{array} \\right]^{-1} = \\left[ \\begin{array}{cc} 0.057^2 &amp; -0.003 \\\\ -0.003 &amp; 0.057^2 \\\\ \\end{array} \\right], \\] se puede usar para hallar la región de confianza o hacer inferencias sobre \\(\\hat{\\mathbf{\\phi}}\\) y sus componentes. Por ejemplo, un intervalo de confianza aproximado del 95% para \\(\\phi_2\\) es \\(-0.725\\pm2(0.057)\\) 0 \\((-0.839, -0.611)\\) el cual contiene el valor real de \\(\\phi_2=-0.75\\). Para estos datos, las tres primeras correlaciones muestrales fueron \\(\\hat{\\phi}_{11}=\\hat{\\rho}(1)=0.834, \\hat{\\phi}_{22}=\\hat{\\phi}_2=-0.725\\) y \\(\\hat{\\phi}_{33}=-0.075\\). De acuerdo a la Propiedad~, el error estándar asintótico de \\(\\hat{\\phi}_{33}\\) es \\(1/\\sqrt{144}=0.083\\), y el valor observado es \\(-0.075\\), que esta a menos de una desviación estándar de \\(\\phi_{33}=0\\). Ejemplo 6.17 (Estimación de Yule-Walker para la serie de nuevos peces) Consideremos nuevamente la serie de nuevos peces y ajustemos un modeloa AR(2) usando la estimación de Yule-Walker. Abajo están los resultados de fijar el modelo usando R. Parámetros Valores Media estimada 62.26278 \\(\\phi_1\\) y \\(\\phi_2\\) estimados 1.3315874; -0.4445447 Errores estándar 0.04222637; 0.04222637 Error de varianza estimada 94.79912 Las instrucciones R para realizar la estimación de Yule-Walker y generar la figura 6.4 son: rec=scan(&quot;data/recruit.txt&quot;) rec.yw=ar.yw(rec, order=2) # ----------------------------------------- rec.pr=predict(rec.yw, n.ahead=24) U=rec.pr$pred+rec.pr$se L=rec.pr$pred-rec.pr$se meses=360:453 plot(meses,rec[meses], type=&quot;o&quot;, xlim=c(360,480),ylab=&quot;Nuevos peces&quot;, main=&quot;Estimación de Yule-Walker para la serie de nuevos peces&quot;) lines(rec.pr$pred, col=&quot;red&quot;,type=&quot;o&quot;) lines(U, col=&quot;blue&quot;,lty=&quot;dashed&quot;) lines(L, col=&quot;blue&quot;,lty=&quot;dashed&quot;) Figura 6.4: Estimación de Yule-Walker para la serie de nuevos peces En el caso de los modelos AR(p), los estimadores de Yule-Walker dados en (6.78) son óptimos en el sentido de que la distribución asintótica, (6.79), es la mejor distribución normal asintótica. Esto se debe a que, dadas las condiciones iniciales, los modelos AR(p) son modelos lineales, y los estimadores de Yule-Walker son esencialmente estimadores de mínimos cuadrados. Si utilizamos el método de momentos para los modelos MA o ARMA, no obtendremos estimadores óptimos debido a que tales procesos no son lineales en los parámetros. Ejemplo 6.18 (Estimación por el Método de los Momentos para un proceso MA(1)) Considere la serie de tiempo \\[x_t=w_t+\\theta w_{t-1},\\] donde \\(|\\theta|&lt;1\\). El modelo se puede escribir como \\[x_t=\\sum_{j=1}^{\\infty}(-\\theta)^jx_{t-j}+w_t,\\] el cual es no lineal en \\(\\theta\\). Las primeras dos autocovarianza poblacionales son \\(\\gamma(0)=\\sigma_w^2(1+\\theta^2)\\) y \\(\\gamma(1)=\\sigma_w^2\\theta\\), de modo que la estimación de \\(\\theta\\) se halla resolviendo \\[\\hat{\\rho}(1) = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)} = \\frac{\\hat{\\theta}}{1+\\hat{\\theta}^2}.\\] Existen dos soluciones, por lo que elegimos la invertible. Si \\(|\\hat{\\rho}(1)|\\leq\\frac{1}{2}\\), la solución es real, en cualquier otro caso, no existe solución real. Aún cuando \\(|\\rho(1)|&lt;\\frac{1}{2}\\) para un modelo MA(1), puede pasar que \\(|\\hat{\\rho}(1)|\\geq\\frac{1}{2}\\) porque este es un estimador. Cuando \\(|\\hat{\\rho}(1)|&lt;\\frac{1}{2}\\), la estimación invertible es \\[\\hat{\\theta}=\\frac{1-\\sqrt{1-4\\hat{\\rho}(1)^2}}{2\\hat{\\rho}(1)}.\\] Se puede demostrar que [^nota10] [^nota10:] La notación AN se lee y se define como: Sea \\(\\{x_n\\}\\) una sucesión de variables aleatorias, se dice que \\(\\{x_n\\}\\) que es con media \\(\\mu_n\\) y varianza \\(\\sigma_n^2\\), si cuando \\(n\\to\\infty\\), \\[\\sigma_n^{-1}(x_n-\\mu_n)\\stackrel{d}{\\to}z,\\]donde \\(z\\) tiene distribución normal estándar. \\[\\hat{\\theta} \\sim AN\\left(\\theta,\\frac{1+\\theta^2+4\\theta^4+\\theta^6+\\theta^8}{n(1-\\theta^2)^2}\\right).\\] El estimador de máxima verosimilitud (que discutiremos en la próxima sección) de \\(\\theta\\), en este caso, tiene una varianza asintótica de \\((1-\\theta^2)/n\\). Cuando \\(\\theta=0.5\\), por ejemplo, la relación de la varianza asintótica del estimador por el método de los momentos y el estimador por el método de máxima verosimilitud de \\(\\theta\\) es alrededor de 3.5. Esto es, para muestras grandes, la varianza del estimador por el método de los momentos es alrededor de 3.5 veces mayor que la varianza del estimador por el EMV de \\(\\theta\\) cuando \\(\\theta=0.5\\). 6.2 Estimación por Máxima Verosimilitud y Mínimos Cuadrados Para fijar ideas, primero enfoquemos en un modelo causal AR(1). Sea \\[x_t=\\mu+\\phi(x_{t-1}-\\mu)+w_t,\\] donde \\(|\\phi|&lt;1\\) y \\(w_t\\sim\\text{iid}N(0,\\sigma_w^2)\\). Dado los datos \\(x_1,x_2,\\ldots,x_n\\) buscamos la función de verosimilitud \\[L(\\mu,\\phi,\\sigma_w^2)=f_{\\mu,\\phi,\\sigma_w^2}(x_1,x_2,\\ldots,x_n).\\] En el caso de un modelo AR(1), podemos escribir la función de verosimilitud como \\[L(\\mu,\\phi,\\sigma_w^2)=f(x_1)f(x_2|x_1)\\cdots f(x_n|x_{n-1}),\\] donde hemos eliminado los parámetros en las densidades \\(f(\\cdot)\\) para facilitar la notación. Dado que \\(x_t|x_{t-1}\\sim N(\\mu+\\phi(x_{t-1}-\\mu,\\sigma_w^2)\\) tenemos \\[f(x_t|x_{t-1})=f_w[(x_t-\\mu)-\\phi(x_{t-1}-\\mu)],\\] donde \\(f_w(\\cdot)\\) es la densidad de \\(w_t\\), esto es, la densidad normal con media cero y varianza \\(\\sigma_w^2\\). Podemos escribir la función de verosimilitud como \\[L(\\mu,\\phi,\\sigma_w^2)=f(x_1)\\prod_{t=2}^{n}f_w[(x_t-\\mu)-\\phi(x_{t-1}-\\mu)].\\] Para hallar \\(f(x_1)\\) podemos usar la representación causal \\[x_1=\\mu+\\sum_{j=0}^{\\infty}\\phi^jw_{1-j},\\] para ver que \\(x_1\\) es normal con media \\(\\mu\\) y varianza \\(\\sigma_w^2/(1-\\phi^2)\\). Finalmente, para un AR(1), la verosimilitud es \\[\\begin{equation} L(\\mu,\\phi,\\sigma_w^2)=(2\\pi\\sigma_w^2)^{-n/2}(1-\\phi^2)^{1/2}\\exp\\left[-\\frac{S(\\mu,\\phi)}{2\\sigma_w^2}\\right] \\tag{6.81} \\end{equation}\\] donde \\[\\begin{equation} S(\\mu,\\phi)=(1-\\phi^2)(x_1-\\mu)^2+\\sum_{t=2}^{n}[(x_t-\\mu)-\\phi(x_{t-1}-\\mu)]^2. \\tag{6.82} \\end{equation}\\] Normalmente, \\(S(\\mu,\\phi)\\) se llama suma de cuadrados incondicional. Podemos también considerar la estimación de \\(\\mu\\) y \\(\\phi\\) usando la suma de cuadrados incondicional, esto es, minimizando \\(S(\\mu,\\phi)\\). Tomando la derivada parcial del logaritmo de (6.81) con respecto a \\(\\sigma_w^2\\) e igualando a cero, que para cada valor de \\(\\mu\\) y \\(\\phi\\) en el espacio de parámetros, \\(\\sigma_w^2=n^{-1}S(\\mu,\\phi)\\) maximiza la verosimilitud. Por consiguiente, el estimador de máxima verosimilitud de \\(\\sigma_w^2\\) es \\[\\begin{equation} \\hat{\\sigma}_w^2=n^{-1}S(\\hat{\\mu},\\hat{\\phi}) \\tag{6.83} \\end{equation}\\] donde \\(\\hat{\\mu}\\) y \\(\\hat{\\phi}\\) son los estimadores de máxima verosimilitud de \\(\\mu\\) y \\(\\phi\\) respectivamente. Si reemplazamos \\(n\\) en (6.83) por \\(n-2\\) podemos obtener el estimador de mínimo cuadrado incondicional de \\(\\sigma_w^2\\). Si en (6.81) tomamos logaritmo, reemplazamos \\(\\sigma_w^2\\) por \\(\\hat{\\sigma}_w^2\\), e ignoramos las constantes, \\(\\hat{\\mu}\\) y \\(\\hat{\\phi}\\) son los valores que minimizan la función de criterio \\[\\begin{equation} l(\\mu,\\phi)=\\ln[n^{-1}S(\\mu,\\phi)]-n^{-1}\\ln(1-\\phi^2). \\tag{6.84} \\end{equation}\\] Esto es, \\(l(\\mu,\\phi)\\propto-2\\ln L(\\mu,\\phi,\\hat{\\sigma}_w^2)\\). 9 [^nota11:] La función de criterio a veces es llamada perfil de verosimilitud. Dado que (6.82) o (6.84) son funciones complicadas de los parámetros, la minimización de \\(l(\\mu,\\phi)\\) o \\(S(\\mu,\\phi)\\) se hace numéricamente. En el caso de modelos AR, tenemos la ventaja que, condicionando los valores inicial, ellos son modelos lineales. Esto es, podemos eliminar el término en la verosimilitud que causa la no-linealidad. Condicionando sobre \\(x_1\\) la verosimilitud condicional llega a ser \\[\\begin{eqnarray} L(\\mu,\\phi,\\sigma_w^2|x_1) &amp;=&amp; \\prod_{t=2}^{n}f_w[(x_t-\\mu)-\\phi(x_{t-1}-\\mu)] \\nonumber \\\\ &amp;=&amp; (2\\pi\\sigma_w^2)^{-(n-1)/2}\\exp\\left[-\\frac{S_c(\\mu,\\phi)}{2\\sigma_w^2}\\right] \\tag{6.85} \\end{eqnarray}\\] donde la suma de cuadrados condicional es \\[\\begin{equation} S_c(\\mu,\\phi)=\\sum_{t=2}^{n}[(x_t-\\mu)-\\phi(x_{t-1}-\\mu)]^2. \\tag{6.86} \\end{equation}\\] El estimador de máxima verosimilitud condicional de \\(\\sigma_w^2\\) es \\[\\begin{equation} \\hat{\\sigma}_w^2=S_c(\\hat{\\mu},\\hat{\\phi})/(n-1) \\tag{6.87} \\end{equation}\\] y \\(\\hat{\\mu}\\) y \\(\\hat{\\phi}\\) son los valores que minimizan la suma de cuadrados condicional \\(S_c(\\mu,\\phi)\\). Haciendo \\(\\alpha=\\mu(1-\\phi)\\) la suma de cuadrados condicional se puede escribir como \\[\\begin{equation} S_c(\\mu,\\phi)=\\sum_{t=2}^{n}[x_t-(\\alpha+\\phi x_{t-1})]^2. \\tag{6.88} \\end{equation}\\] El problema ahora es un problema de regresión lineal visto en el Tema 2. Siguiendo los resultados de la estimación de mínimos cuadrados, tenemos \\(\\hat{\\alpha}=\\bar{x}_{(2)}-\\hat{\\phi}\\bar{x}_{(1)}\\) donde \\(\\bar{x}_{(1)}=(n-1)^{-1}\\sum_{t=1}^{n-1}x_t\\) y \\(\\bar{x}_{(2)}=(n-1)^{-1}\\sum_{t=2}^{n}x_t\\) y los estimados condicionales son entonces \\[\\begin{eqnarray} \\hat{\\mu} &amp;=&amp; \\frac{\\bar{x}_{(2)}-\\hat{\\phi}\\bar{x}_{(1)}}{1-\\hat{\\phi}} \\tag{6.89} \\\\ \\hat{\\phi} &amp;=&amp; \\frac{\\sum_{t=2}^{n}(x_t-\\bar{x}_{(2)})(x_{t-1}-\\bar{x}_{(1)})}{\\sum_{t=2}^{n}(x_{t-1}-\\bar{x}_{(1)})^2}. \\tag{6.90} \\end{eqnarray}\\] De (6.89) y (6.90) vemos que \\(\\hat{\\mu}\\approx\\bar{x}\\) y \\(\\hat{\\phi}\\approx\\hat{\\rho}(1)\\). Estos es, los estimadores de Yule-Walker y los estimadores de mínimos cuadrados son aproximadamente los mismos. La única diferencia es la inclusión o exclusión de los términos que envuelven los puntos finales \\(x_1\\) y \\(x_n\\). Podemos también ajustar el estimado de \\(\\sigma_w^2\\) en (6.87) para que sea equivalente al estimador de mínimos cuadrados, esto es, dividimos \\(S_c(\\hat{\\mu},\\hat{\\phi})\\) por \\((n-3)\\) en vez de \\((n-1)\\) en (6.87). Para un modelo general AR(p) los estimadores máxima verosimilitud, mínimos cuadrados incondicionales y mínimos cuadrados condicionales se obtienen de manera análoga al ejemplo de AR(1). Para modelos ARMA en general, es difícil escribir la función de verosimilitud como una función explícita de los parámetros. En vez de eso, es conveniente escribir la verosimilitud en término de las innovaciones o errores de predicción de un paso, \\(x_t-x_t^{t-1}\\). Supóngase que \\(x_t\\) es un proceso ARMA(p,q) causal con \\(w_t\\sim\\text{idd}N(0,\\sigma_w^2)\\). Sea \\(\\pmb{\\beta}=(\\mu,\\phi_1\\ldots,\\phi_p,\\theta_1,\\ldots,\\theta_q)^t\\) un vector de orden \\((p+q+1)\\times1\\) de los parámetros del modelo. La función de verosimilitud se puede escribir como \\[L(\\pmb{\\beta},\\sigma_w^2)=\\prod_{t=1}^{n}f(x_t|x_{t-1},\\ldots,x_1)\\] La distribución condicional de \\(x_t\\) dados \\(x_{t-1},\\ldots,x_1\\) es gaussiana con media \\(x_t^{t-1}\\) y varianza \\(P_t^{t-1}\\). Además, para modelos ARMA, podemos escribir \\(P_t^{t-1}=\\sigma_w^2r_t^{t-1}\\) donde \\(r_t^{t-1}\\) no depende de \\(\\sigma_w^2\\). La función de verosimilitud de la muestra se puede escribir entonces como \\[\\begin{equation} L(\\mathbf{\\beta},\\sigma_w^2)=(2\\pi\\sigma_w^2)^{-n/2}\\left[r_1^0(\\mathbf{\\beta})r_2^1(\\mathbf{\\beta})\\cdot sr_n^{n-1}(\\mathbf{\\beta})\\right]^{1/2}\\exp\\left[-\\frac{S(\\mathbf{\\beta})}{2\\sigma_w^2}\\right] \\tag{6.91} \\end{equation}\\] donde \\[\\begin{equation} S(\\mathbf{\\beta})=\\sum_{t=1}^{n}\\left[\\frac{(x_t-x_t^{t-1}(\\mathbf{\\beta}))^2}{r_t^{t-1}(\\mathbf{\\beta})}\\right]. \\tag{6.92} \\end{equation}\\] Se tiene que \\(x_t^{t-1}\\) y \\(r_t^{t-1}\\) son funciones de \\(\\mathbf{\\beta}\\) y hacemos este hecho explícito en (6.91) y (6.92). Dados los valores para \\(\\mathbf{\\beta}\\) y \\(\\sigma_w^2\\), la verosimilitud se puede evaluar usando las técnicas vistas para Pronósticos. La estimación de máxima verosimilitud ahora procederá maximizando @ref)eq:eq-funcion-verosimilitud-datos) con respecto a \\(\\mathbf{\\beta}\\) y \\(\\sigma_w^2\\). Tenemos entonces \\[\\begin{equation} \\hat{\\sigma}_w^2=n^{-1}S(\\hat{\\mathbf{\\beta}}), \\tag{6.93} \\end{equation}\\] donde \\(\\hat{\\mathbf{\\beta}}\\) es el valor de \\(\\mathbf{\\beta}\\) que minimiza la función de criterio \\[\\begin{equation} l(\\mathbf{\\beta})=\\ln[n^{-1}S(\\mathbf{\\beta})]+n^{-1}\\sum_{t=1}^{n}\\ln r_t^{t-1}(\\mathbf{\\beta}). \\tag{6.94} \\end{equation}\\] Por ejemplo, para el modelo AR(1) discutido arriba, la función genérica \\(l(\\mathbf{\\beta})\\) en (6.94) es \\(l(\\mu,\\phi)\\) en (6.84) y la general \\(S(\\mathbf{\\beta})\\) en (6.92) es \\(S(\\mu,\\phi)\\) dado en (6.82). De (6.82) y (6.84) se ve que \\(x_1^0=\\mu\\) y \\(x_t^{t-1}=\\mu+\\phi(x_{t-1}-\\mu)\\) para \\(t=2,\\ldots,n\\). También \\(r_1^0=1/(1-\\phi^2)\\) y \\(r_t^{t-1}=1\\) para \\(t=2,\\ldots,n\\). Los mínimos cuadrados incondicional se desarrollarán minimizando (6.92) con respecto a \\(\\mathbf{\\beta}\\). La estimación de mínimos cuadrados condicional envuelve minimizar (6.92) con respecto a \\(\\mathbf{\\beta}\\) pero donde, para facilitar la carga computacional, las predicciones y sus errores se obtienen por condicionamiento sobre los valores iniciales de las observaciones. En general, se usan las rutinas numéricas de optimización para obtener las estimaciones y sus errores estándar. Ejemplo 6.19 (Algoritmos de Newton-Raphson y puntuación) Dos rutinas numéricas de optimización comunes para la estimación de máxima verosimilitud son el Newton-Raphson y el de puntuación. Daremos una breve descripción de las ideas matemáticas. La implementación de estos algoritmos es más complicada de lo que discutiremos en este ejemplo. Sea \\(l(\\mathbf{\\beta})\\) una función de criterio de \\(k\\) parámetros \\(\\mathbf{\\beta}=(\\beta_1,\\ldots,\\beta_k)\\) la cual deseamos minimizar respecto a \\(\\mathbf{\\beta}\\). Por ejemplo, considere la función de verosimilitud dada por (6.84) o (6.94). Suponga que \\(l(\\hat{\\mathbf{\\beta}})\\) es el extremo que estamos interesados en hallar, y \\(\\hat{\\mathbf{\\beta}}\\) se halla resolviendo \\(\\partial l(\\mathbf{\\beta})/\\partial\\beta_j=0\\) para \\(j=1,\\ldots,k\\). Denotemos por \\(l^{(1)}(\\mathbf{\\beta})\\) el vector \\(k\\times1\\) de derivadas parciales \\[l^{(1)}(\\mathbf{\\beta})=\\left(\\frac{\\partial l(\\mathbf{\\beta})}{\\partial\\beta_1},\\cdots,\\frac{\\partial l(\\mathbf{\\beta})}{\\partial\\beta_k}\\right)^t\\] Note que \\(l^{(1)}(\\mathbf{\\hat{\\beta}})=\\textbf{0}\\). Sea \\(l^{(2)}(\\mathbf{\\beta})\\) una matriz \\(k\\times k\\) de las segundas derivadas parciales \\[l^{(2)}(\\mathbf{\\beta})=\\left\\{-\\frac{\\partial l^2(\\mathbf{\\beta})}{\\partial\\beta_i\\partial\\beta_j}\\right\\}_{i,j=1}^{k}\\] y supongamos que \\(l^{(2)}(\\mathbf{\\beta})\\) es no singular. Sea \\(\\mathbf{\\beta}_{(0)}\\) un estimador inicial de \\(\\mathbf{\\beta}\\). Entonces, usando el desarrollo de Taylor, tenemos la siguiente aproximación: \\[\\textbf{0}=l^{(1)}(\\mathbf{\\hat{\\beta}})\\approx l^{(1)}(\\mathbf{\\beta}_{(0)})-l^{(2)}(\\mathbf{\\beta}_{(0)})\\left[\\mathbf{\\hat{\\beta}}-\\mathbf{\\beta}_0\\right]\\] Haciendo el lado derecho cero y resolviendo para \\(\\mathbf{\\hat{\\beta}}\\) se tiene \\[\\mathbf{\\beta}_{(1)}=\\mathbf{\\beta}_{(0)}+\\left[l^{(2)(\\mathbf{\\beta}_{(0)}})\\right]^{-1}l^{(1)}(\\mathbf{\\beta}_{(0)})\\] El algoritmo de Newton-Raphson procede iterando este resultado, reemplazando \\(\\mathbf{\\beta}_{(0)}\\) por \\(\\mathbf{\\beta}_{(1)}\\) para obtener \\(\\mathbf{\\beta}_{(2)}\\) y así sucesivamente, hasta que converja. Bajo un conjunto apropiado de condiciones, la sucesión de estimadores \\(\\mathbf{\\beta}_{(1)},\\mathbf{\\beta}_{(2)},\\ldots\\), convergerá a \\(\\mathbf{\\hat{\\beta}}\\) el estimador de máxima verosimilitud para \\(\\mathbf{\\beta}\\). Para la estimación de máxima verosimilitud, la función de criterio usada es \\(l(\\mathbf{\\beta})\\) dada por ((6.94); \\(l^{(1)}(\\mathbf{\\beta})\\) es llamado el vector de puntuación y \\(l^{(2)}(\\mathbf{\\beta})\\) es llamado el Hessiano. En el algoritmo de puntuaciones, reemplazamos \\(l^{(2)}(\\mathbf{\\beta})\\) por \\(\\mathbb{E}[l^{(2)}(\\mathbf{\\beta})]\\), la matriz de información. Bajo condiciones apropiadas, la inversa de la matriz de información es la matriz de varianza-covarianza asintótica del estimador \\(\\mathbf{\\hat{\\beta}}\\). Esta es a veces aproximada por la inversa del Hessiano en \\(\\mathbf{\\hat{\\beta}}\\). Si las derivadas son difíciles de obtener, es posible usar la estimación de verosimilitud cuasi-máxima donde se usan las técnicas numéricas para aproximar las derivadas. Ejemplo 6.20 (EMV para la serie de nuevos peces) En el ejemplo 6.17 fijamos un modelo AR(2) para la serie de nuevos peces usando las ecuaciones de Yule-Walker. El siguiente comando en R fija el modelo AR(2) via máxima verosimilitud. Pueden comparar estos resultados con los obtenidos en el ejemplo 6.17. rec.mle=ar.mle(rec,order=2) rec.mle ## ## Call: ## ar.mle(x = rec, order.max = 2) ## ## Coefficients: ## 1 2 ## 1.351 -0.461 ## ## Order selected 2 sigma^2 estimated as 89.3 6.2 Estimación de mínimos cuadrados para modelos ARMA(p,q) Ahora discutiremos la estimación de mínimos cuadrados para modelos ARMA(p,q) via Gauss-Newton. Sea \\(x_t\\) un proceso ARMA(p,q) gaussiano causal e invertible. Escribimos \\(\\mathbf{\\beta}=(\\phi_1,\\ldots,\\phi_p\\), \\(\\theta_1,\\ldots,\\theta_q)^t\\), para simplificación de la discusión, hacemos \\(\\mu=0\\). Escribimos el modelo en términos de los errores \\[\\begin{equation} w_t(\\mathbf{\\beta})=x_t-\\sum_{j=1}^{p}\\phi_jx_{t-j}-\\sum_{k=1}^{q}\\theta_kw_{t-k}(\\mathbf{\\beta}) \\tag{6.95} \\end{equation}\\] para enfatizar la dependencia de los errores sobre los parámetros. Para mínimos cuadrados condicional, aproximamos la suma residual de cuadrados condicionando por \\(x_1,\\ldots,x_p (p&gt;0)\\) y \\(w_p=w_{p-1}=\\cdots=w_{1-q}=0 (q&gt;0)\\), en cuyo caso podemos evaluar (6.95) para \\(t=p+1,p+2,\\ldots,n\\). Usando estos argumentos condicionales, el error de suma de cuadrados condicional es \\[S_c(\\mathbf{\\beta})=\\sum_{t=p+1}^{n}w_t^2(\\mathbf{\\beta})\\] Minimizando \\(S_c(\\mathbf{\\beta})\\) con respecto a \\(\\mathbf{\\beta}\\) obtenemos los estimados de mínimos cuadrados condicional. Si \\(q=0\\), el problema es una regresión lineal, y no se necesitan técnicas iterativas para minimizar \\(S_c(\\phi_1,\\ldots,\\phi_p)\\). Si \\(q&gt;0\\) el problema es de regresión no-lineal y tenemos que acudir a optimización numérica. Cuando \\(n\\) es grande, condicionando sobre unos pocos valores iniciales tendremos poca influencia sobre los estimados finales de los parámetros. En el caso de muestras de tamaño pequeño a moderado, podemos usar mínimos cuadrados incondicionales. El problema de mínimos cuadrados incondicional es elegir \\(\\mathbf{\\beta}\\) para minimizar la suma de cuadrados incondicional, la cual denotamos por \\(S(\\mathbf{\\beta})\\). La suma de cuadrados incondicional se puede escribir de varias maneras. Una de las maneras es la siguiente forma 10 [^nota12:] Para detalles, véase Box, G.E.P., Jenkins, G.M. and Reinsel, G.C. (1994). Time Series Analysis, Forecasting and Control, 3rd ed. Englewood Cliffs, NJ: Prentice Hall. Apéndice A7.3. \\[S(\\mathbf{\\beta})=\\sum_{t=-\\infty}^{n}\\hat{w}_t^2(\\mathbf{\\beta})\\] donde \\(\\hat{w}_t^2(\\mathbf{\\beta})=\\mathbb{E}(w_t|x_1,\\ldots,x_n)\\). Cuando \\(t\\leq0\\) los \\(\\hat{w}_t(\\mathbf{\\beta})\\) se obtienen por retroproyección. Como una forma práctica, aproximamos \\(S(\\mathbf{\\beta})\\) por medio de iniciar la suma en \\(t=-M+1\\) donde \\(M\\) se elige suficientemente grande para garantizar que \\(\\sum_{t=-\\infty}^{-M}\\hat{w}_t^2(\\mathbf{\\beta})\\approx0\\). En el caso de estimación por mínimos cuadrados incondicional, son necesarias las técnicas de optimización numéricas aún cuando \\(q=0\\). Para emplear Gauss-Newton, sea \\(\\mathbf{\\beta}_{(0)}=(\\phi_1^{(0)},\\ldots,\\phi_p^{(0)},\\theta_1^{(0)},\\ldots,\\theta_q^{(0)})^t\\) un estimado inicial de \\(\\mathbf{\\beta}\\). Por ejemplo, podemos obtener \\(\\mathbf{\\beta}_{(0)}\\) por el método de los momentos. El desarrollo de Taylor de primer orden de \\(w_t(\\mathbf{\\beta})\\) es \\[\\begin{equation} w_t(\\mathbf{\\beta}) \\approx w_t(\\mathbf{\\beta}_{(0)})-\\left(\\mathbf{\\beta}-\\mathbf{\\beta}_{(0)}\\right)^t z_t(\\mathbf{\\beta}_{(0)}) \\tag{6.96} \\end{equation}\\] donde \\[z_t(\\mathbf{\\beta}_{(0)})=\\left(-\\frac{\\partial w_t(\\mathbf{\\beta}_{(0)})}{\\partial\\beta_1},\\cdots,-\\frac{\\partial w_t(\\mathbf{\\beta}_{(0)})}{\\partial\\beta_{p+q}}\\right)^t\\text{, }t=1,\\ldots,n\\] La aproximación lineal de \\(S_c(\\mathbf{\\beta})\\) s \\[\\begin{equation} Q(\\mathbf{\\beta})=\\sum_{t=p+1}^{n}\\left[w_t(\\mathbf{\\beta}_{(0)})-\\left(\\mathbf{\\beta}-\\mathbf{\\beta}_{(0)}\\right)^tz_t(\\mathbf{\\beta}_{(0)})\\right]^2 \\tag{6.97} \\end{equation}\\] y esta es la cantidad que queremos minimizar. Para aproximar los mínimos cuadrados incondicional, iniciaremos la suma en (6.97) en \\(t=-M+1\\) para \\(M\\) grande, y trabajamos con los valores de retroproyección. Usando los resultados de mínimos cuadrados ordinarios, sabemos que \\[\\begin{equation} (\\widehat{\\mathbf{\\beta}-\\mathbf{\\beta}}_{(0)})=\\left(n^{-1}\\sum_{t=p+1}^{n}z_t(\\mathbf{\\beta}_{(0)})z_t^t(\\mathbf{\\beta}_{(0)})\\right)^{-1} \\left(n^{-1}\\sum_{t=p+1}^{n}z_t(\\mathbf{\\beta}_{(0)})w_t(\\mathbf{\\beta}_{(0)})\\right) \\tag{6.98} \\end{equation}\\] minimiza \\(Q(\\mathbf{\\beta})\\). De (6.98) podemos escribir el estimado Gauss-Newton de un paso como \\[\\begin{equation} \\mathbf{\\beta}_{(1)}=\\mathbf{\\beta}_{(0)}+\\Delta(\\mathbf{\\beta}_{(0)}) \\tag{6.99} \\end{equation}\\] donde \\(\\Delta(\\mathbf{\\beta}_{(0)})\\) denota el lado derecho de (6.98). La estimación Gauss-Newton se logra reemplazando \\(\\mathbf{\\beta}_{(0)}\\) por \\(\\mathbf{\\beta}_{(1)}\\) en (6.99). Este procedimiento se repite, iterando para \\(j=2,3,\\ldots\\), para calcular \\[\\mathbf{\\beta}_{(j)}=\\mathbf{\\beta}_{(j-1)}+\\Delta(\\mathbf{\\beta}_{(j-1)})\\] hasta converger. Ejemplo 6.21 (Gauss-Newton para un MA(1)) Considere un proceso MA(1) invertible, \\(x_t=w_t+\\theta w_{t-1}\\). Escribimos el error truncado como \\[\\begin{equation} w_t(\\theta)=x_t-\\theta w_{t-1}(\\theta)\\text{, }t=1,\\ldots,n \\tag{6.100} \\end{equation}\\] donde condicionamos \\(w_0(\\theta)=0\\). Derivando respecto de \\(\\theta\\) \\[\\begin{equation} -\\frac{\\partial w_t(\\theta)}{\\partial\\theta}=w_{t-1}(\\theta)+\\theta\\frac{\\partial w_{t-1}(\\theta)}{\\partial\\theta}\\text{, }t=1,\\ldots,n \\tag{6.101} \\end{equation}\\] donde \\(\\partial w_0(\\theta)/\\partial\\theta=0\\). Usando la notación de (6.96) podemos escribir (6.101) como \\[\\begin{equation} z_t(\\theta)=w_{t-1}(\\theta)-\\theta z_{t-1}(\\theta)\\text{, }t=1,\\ldots,n \\tag{6.102} \\end{equation}\\] donde \\(z_0(\\theta)=0\\). Sea \\(\\theta_{(0)}\\) una estimación inicial de \\(\\theta\\), por ejemplo, el estimado dado en el ejemplo 6.18. Entonces, el procedimiento Gauss-Newton para mínimos cuadrados condicional está dado por \\[\\begin{equation} \\theta_{(j+1)}=\\theta_{(j)}+\\frac{\\sum_{t=1}^{n}z_t(\\theta_{(j)})w_t(\\theta_{(j)})}{\\sum_{t=1}^{n}z_t^2(\\theta_{(j)})}\\text{, }j=0,1,2,\\ldots \\tag{6.103} \\end{equation}\\] donde los valores en (6.103) se calculan recursivamente usando (6.100) y (6.101). Los cálculos se paran cuando \\(|\\theta_{(j+1)}-\\theta_{(j)}|\\) ó \\(|Q(\\theta_{(j+1)})-Q(\\theta_{(j)})|\\) son menor que alguna cantidad prefijada. Ejemplo 6.22 (Ajuste de la serie de varvas glaciares) Consideremos la serie de espesores de varvas glaciares en Massachusetts para \\(n=634\\) años, como analizamos en el ejemplo 3.4.3 (Tema 3) donde ajustamos a un modelo de promedio móvil de primer orden una transformación logarítmica, podemos también a esa serie ajustar una ecuación en diferencia de la transformación logarítmica, como sigue \\[\\nabla[\\ln(x_t)]=\\ln(x_t)-\\ln(x_{t-1})=\\ln\\left(\\frac{x_t}{x_{t-1}}\\right)\\] el cual se puede interpretar como la proporción del porcentaje de cambio en el espesor. En la figura 6.5 mostramos las ACF y PACF muestral, confirmando la tendencia de \\(\\nabla[\\ln(x_t)]\\) de comportarse como proceso de promedio móvil de primer orden ya que la ACF tiene un pico significativa en paso 1 y la PACF decrece exponencialmente. A continuación se muestran 9 iteraciones del procedimiento de Gauss-Newton dado en (6.103), iniciando con \\(\\hat{\\theta}_0=-0.1\\), dando los valores \\[-0.442; -0.624; -0.717;-0.750;-0.763;-0.768;-0.771;-0.772;-0.772;\\] para \\(\\theta_{(1)},\\ldots,\\theta_{(9)}\\), y la varianza estimada del error es \\(\\hat{\\sigma}_w^2=0.236\\). Usando el valor final de \\(\\hat{\\theta}=\\theta_{(9)}=-0.772\\) y el vector \\(z_t\\) de derivadas parciales en (6.102) nos da un error estándar de \\(0.025\\) y un \\(t\\)-valor de \\(-0.772/0.025=-30.88\\) con \\(632\\) grados de libertad (se pierde uno con las diferencias). varva=scan(&quot;data/varve.txt&quot;) dv=log(varva[2:634]/varva[1:633]); par(mfrow=c(2,1)) acf(dv,30) pacf(dv,30) Figura 6.5: ACF y PACF de la serie varvas glaciares En el caso general de un proceso ARMA(p,q) causal e invertible, las estimaciones de máxima verosimilitud, y las estimaciones de mínimos cuadrados condicional e incondicional (y las estimación de Yule-Walker en el caso de modelos AR) dan estimadores óptimos. La prueba de este resultado general se puede hallar en Brockwell y Davis (2006). Denotaremos los coeficientes del proceso ARMA por \\(\\mathbf{\\beta}=(\\phi_1,\\ldots,\\phi_p,\\theta_1,\\ldots,\\theta_q)&#39;\\). Proposición 6.9 (Distribución de los estimadores para muestras grandes) Bajo condiciones apropiadas, para procesos ARMA causal e invertible, los estimadores de máxima verosimilitud, mínimos cuadrados incondicional y condicional, cada uno inicializado por los estimadores dados por el método de los momentos, proveen estimadores óptimos de \\(\\sigma_w^2\\) y \\(\\mathbf{\\beta}\\) en el sentido de que \\(\\hat{\\sigma}_w^2\\) es consistente, y la distribución asintótica de \\(\\mathbf{\\hat{\\beta}}\\) es la mejor distribución normal asintótica. En particular, cuando \\(n\\to\\infty\\) \\[\\begin{equation} \\sqrt{n}\\left(\\mathbf{\\hat{\\beta}}-\\mathbf{\\beta}\\right)\\overset{d}{\\to}N(\\textbf{0},\\sigma_w^2\\Gamma_{p,q}^{-1}) \\tag{6.104} \\end{equation}\\] En (6.104) la matriz de varianza-covarianza del estimador \\(\\mathbf{\\hat{\\beta}}\\) es la inversa de la matriz de transformación. En este caso, la matriz \\(\\Gamma_{p,q}\\) de orden \\((p+q)\\times(p+q)\\), que tiene la forma \\[\\begin{equation} \\Gamma_{p,q}=\\left( \\begin{array}{cc} \\Gamma_{\\phi\\phi} &amp; \\Gamma_{\\phi\\theta} \\\\ \\Gamma_{\\theta\\phi} &amp; \\Gamma_{\\theta\\theta} \\\\ \\end{array} \\right) \\tag{6.105} \\end{equation}\\] La \\(p\\times p\\) matriz \\(\\Gamma_{\\phi\\phi}\\) es dada por (6.76), esto es, el \\(ij\\)-ésimo elemento de \\(\\Gamma_{\\phi\\phi}\\) para \\(i,j=1,\\ldots,p\\) es \\(\\gamma_x(i-j)\\) de un proceso AR(p) \\(\\phi(B)x_t=w_t\\). Similarmente, \\(\\Gamma_{\\theta\\theta}\\) es una matriz \\(q\\times q\\) con el \\(ij\\)-ésimo elemento para \\(i,j=1,\\ldots,q\\) igual a \\(\\gamma_y(i-j)\\) de un proceso AR(q) \\(\\theta(B)y_t=w_t\\). La \\(p\\times q\\) matriz \\(\\Gamma_{\\phi\\theta}=\\{\\gamma_{xy}(i-j)\\}\\) para \\(i=1,\\ldots,p; j=1,\\ldots,q\\); estos es, el \\(ij\\)-ésimo elemento es la covarianza cruzada entre dos procesos AR dados por \\(\\phi(B)x_t=w_t\\) y \\(\\theta(B)y_t=w_t\\). Finalmente, \\(\\Gamma_{\\theta\\phi}=\\Gamma_{\\phi\\theta}&#39;\\) es de orden \\(q\\times p\\). Ejemplo 6.23 (Algunas distribuciones asintóticas específicas) Las siguientes distribuciones son algunos casos de la proposición 6.9 AR(1): \\(\\gamma_x(0)=\\sigma_w^2/(1-\\phi^2)\\), de esta manera \\(\\sigma_w^2\\Gamma_{1,0}^{-1}=(1-\\phi^2)\\). Entonces \\[\\begin{equation} \\hat{\\phi}\\sim AN[\\phi,n^{-1}(1-\\phi^2)] \\tag{6.106} \\end{equation}\\] AR(2): Pueden verificar que \\(\\gamma_x(0)=\\left(\\frac{1-\\phi_2}{1+\\phi_2}\\right)\\frac{\\sigma_w^2}{(1-\\phi_2)^2-\\phi_1^2}\\) y \\(\\gamma_x(1)=\\phi_1\\gamma_x(0)+\\phi_2\\gamma_x(1)\\). De este hecho, podemos calcular \\(\\Gamma_{2,0}^{-1}\\). En particular, tenemos \\[\\begin{equation} \\left( \\begin{array}{c} \\hat{\\phi}_1 \\\\ \\hat{\\phi}_2 \\\\ \\end{array} \\right)\\sim AN\\left[\\left( \\begin{array}{c} \\phi_1 \\\\ \\phi_2 \\\\ \\end{array} \\right), n^{-1}\\left( \\begin{array}{cc} 1-\\phi_2^2 &amp; -\\phi_1(1+\\phi_2) \\\\ \\text{sym} &amp; 1-\\phi_2^2 \\\\ \\end{array} \\right)\\right] \\tag{6.107} \\end{equation}\\] MA(1): En este caso, escribimos \\(\\theta(B)y_t=w_t\\) ó \\(y_t+\\theta y_{t-1}=w_t\\). Entonces, análogamente al caso AR(1), \\(\\gamma_t(0)=\\sigma_w^2/(1-\\theta^2)\\), de este modo \\(\\sigma_w^2\\Gamma_{0,1}^{-1}=(1-\\theta^2)\\). Entonces, \\[\\begin{equation} \\hat{\\theta}\\sim AN[\\theta,n^{-1}(1-\\theta^2)] \\tag{6.108} \\end{equation}\\] MA(2): Escribiendo \\(y_t+\\theta_1y_{t-1}+\\theta_2y_{t-2}=w_t\\), así, análogamente al caso AR(2), tenemos \\[\\begin{equation} \\left( \\begin{array}{c} \\hat{\\theta}_1 \\\\ \\hat{\\theta}_2 \\\\ \\end{array} \\right)\\sim AN\\left[\\left( \\begin{array}{c} \\theta_1 \\\\ \\theta_2 \\\\ \\end{array} \\right), n^{-1}\\left( \\begin{array}{cc} 1-\\theta_2^2 &amp; -\\theta_1(1+\\theta_2) \\\\ \\text{sym} &amp; 1-\\theta_2^2 \\\\ \\end{array} \\right)\\right] \\tag{6.109} \\end{equation}\\] ARMA(1,1): Para calcular \\(\\Gamma_{\\phi\\theta}\\) debemos hallar \\(\\gamma_{xy}(0)\\), donde \\(x_t-\\phi x_{t-1}=w_t\\) y \\(y_t+\\theta y_{t-1}=w_t\\). Tenemos \\[\\begin{eqnarray*} \\gamma_{xy}(0) &amp;=&amp; \\text{cov}(x_t,y_t)=\\text{cov}(\\phi x_{t-1}+w_t,-\\theta y_{t-1}+w_t \\\\ &amp;=&amp; -\\phi\\theta\\gamma_{xy}(0)+\\sigma_w^2 \\end{eqnarray*}\\] Resolviendo, hallamos \\(\\gamma_{xy}(0)=\\sigma_w^2/(1+\\phi\\theta)\\). Entonces, \\[\\begin{equation} \\left( \\begin{array}{c} \\hat{\\phi} \\\\ \\hat{\\theta} \\\\ \\end{array} \\right)\\sim AN\\left[\\left( \\begin{array}{c} \\phi \\\\ \\theta \\\\ \\end{array} \\right), n^{-1}\\left[ \\begin{array}{cc} (1-\\phi^2)^{-1} &amp; (1+\\phi\\theta)^{-1} \\\\ \\text{sym} &amp; (1-\\theta^2)^{-1} \\\\ \\end{array} \\right]^{-1}\\right] \\tag{6.110} \\end{equation}\\] Puede resultar sorprendente, que las distribuciones asintóticas de \\(\\hat{\\phi}\\) de un AR(1) [ecuación (6.106)] y \\(\\hat{\\theta}\\) de un MA(1) [ecuación (6.108)] sean de la misma forma. Es posible explicar este resultado heurístico inesperado usando la intuición de regresión lineal. Esto es, para el modelo de regresión normal presentado en la Sección 3.3 del Tema 3 sin término de intercepción \\(x_t=\\beta z_t+w_t\\), sabemos que \\(\\hat{\\beta}\\) es normalmente distribuido con media \\(\\beta\\), y de (3.16) (Tema 3) \\[\\text{var}\\left\\{\\sqrt{n}\\left(\\hat{\\beta}-\\beta\\right)\\right\\}=n\\sigma_w^2\\left(\\sum_{t=1}^{n}z_t^2\\right)^{-1}=\\sigma_w^2\\left(n^{-1}\\sum_{t=1}^{n}z_t^2\\right)^{-1}\\] Para el modelo AR(1) causal dado por \\(x_t=\\phi x_{t-1}+w_t\\), la intuición de regresión nos dice que debemos esperar que para \\(n\\) grande \\[\\sqrt{n}(\\hat{\\phi}-\\phi)\\] es aproximadamente normal con media cero y varianza dada por \\[\\sigma_w^2\\left(n^{-1}\\sum_{t=1}^{n}x_{t-1}^2\\right)^{-1}\\] Ahora, \\(n^{-1}\\sum_{t=2}^{n}x_{t-1}^2\\) es la varianza muestral (recuerde que la media de \\(x_t\\) es cero) de \\(x_t\\), de modo que cuando \\(n\\) se hace grande podemos esperar que esta se aproxime a \\(\\text{var}(x_t)=\\gamma(0)=\\sigma_w^2/(1-\\phi^2)\\). Entonces, la varianza muestral grande de \\(\\sqrt{n}(\\hat{\\phi}-\\phi)\\) es \\[\\sigma_w^2\\gamma_x(0)^{-1}=\\sigma_w^2\\left(\\frac{\\sigma_w^2}{1-\\phi^2}\\right)^{-1}=(1-\\phi^2)\\] esto es, (6.106) vale. En el caso de un MA(1), podemos usar la discusión del ejemplo 6.21 para escribir un modelo de regresión aproximado para el MA(1). Esto es, considere la aproximación (6.102) como el modelo de regresión \\[z_t(\\hat{\\theta})=-\\theta z_{t-1}(\\hat{\\theta})+w_{t-1}\\] donde ahora, \\(z_{t-1}(\\hat{\\theta})\\) se define como en el ejemplo 6.21, jugando el papel de regresor. Continuando con la analogía, podemos esperar que la distribución asintótica de \\(\\sqrt{n}(\\hat{\\phi}-\\phi)\\) sea normal con media cero y varianza aproximada \\[\\sigma_w^2\\left(n^{-1}\\sum_{t=1}^{n}z_{t-1}^2(\\hat{\\theta})\\right)^{-1}\\] Como en el caso AR(1), \\(n^{-1}\\sum_{t=1}^{n}z_{t-1}^2(\\hat{\\theta})\\) es la varianza muestral de \\(z_t(\\hat{\\theta})\\), de modo que para \\(n\\) grande, esta debería ser \\(\\text{var}\\{z_t(\\theta)\\}=\\gamma_z(0)\\). Pero, note que, como se ve de (6.102), \\(z_t(\\theta)\\) es aproximadamente un proceso AR(1) con parámetro \\(-\\theta\\). Por la tanto, \\[\\sigma_w^2\\gamma_X(0)^{-1}=\\sigma_w^2\\left(\\frac{\\sigma_w^2}{1-(-\\theta)^2}\\right)^{-1}=(1-\\theta^2)\\] lo cual concuerda con (6.108). Finalmente, la distribución asintótica de los parámetros estimados de un AR y de un MA son de la misma forma, porque en el caso MA, los regresores son las diferencias del proceso \\(z_t(\\theta)\\) que tienen estructura AR, y es esta estructura la que determina la varianza asintótica de los estimadores. En el ejemplo 3.31 el error estándar estimado de \\(\\hat{\\theta}\\) fue \\(0.025\\). En el ejemplo, este valor se calculó como la raíz cuadrada de \\[s_w^2\\left(n^{-1}\\sum_{t=2}^{n}z_{t-1}^2(\\hat{\\theta})\\right)^{-1}\\] donde \\(n=633, s_w^2=0.236\\) y \\(\\hat{\\theta}=-0.772\\). Usando (6.108), también pudimos haber calculado este valor usando la aproximación asintótica, como la raíz cuadrada de \\((1-0.772^2)/633\\) lo cual también nos da \\(0.025\\). El comportamiento asintótico de los estimadores de los parámetros nos da una información adicional sobre el problema de ajuste de los modelos ARMA a los datos. Por ejemplo, supongamos que una serie de tiempo sigue un proceso AR(1) y decidimos fijar un modelo AR(2) a los datos. ¿Habrá algún problema si hacemos esto? Más generalmente, ¿por qué no fijamos un modelo AR de orden grande para asegurar que capturamos toda la dinámica del proceso? Después de todo, si el proceso es realmente un AR(1), los otros parámetros autoregresivos no serán significativos. La respuesta es que si sobre ajustamos el modelo, podemos perder eficiencia. Por ejemplo, si fijamos un modelo AR(1) a un proceso AR(1), para \\(n\\) grande, \\(\\text{var}(\\hat{\\phi})\\approx n^{-1}(1-\\phi_1^2)\\). Pero si fijamos un modelo AR(2) a un proceso AR(1), para \\(n\\) grande, \\(\\text{var}(\\hat{\\phi})\\approx n^{-1}(1-\\phi_2^2)=n^{-1}\\) porque \\(\\phi_2=0\\). En consecuencia, la varianza de \\(\\phi_1\\) ha sido aumentada, haciendo del estimador menos preciso. Sin embargo, diremos que el sobre ajuste lo podemos usar como una herramienta de diagnóstico. Por ejemplo,, si fijamos un modelo AR(2) a los datos y estos se satisfacen con el modelo, entonces, agregando un parámetro más y fijando un modelo AR(3) debería darnos aproximadamente el mismo modelo como en el ajuste AR(2). Discutiremos los modelos de diagnóstico con más detalle más adelante. Si \\(n\\) es pequeño o si los parámetros están cerca de los bordes o cotas, la aproximación asintótica puede ser un poco pobre. La técnica de bootstrap puede ser útil en este caso. Para una explicación ampliada de bootstrap véase Efron y Tibshirani (1994). Daremos un ejemplo simple de bootstrap para un proceso AR(1) Ejemplo 6.24 (Bootstrap para un AR(1)) Consideremos un modelo AR(1) con coeficiente de regresión cerca a la cota de causalidad y un error del proceso que es simétrico pero no normal. Específicamente, considere el modelo estacionario y causal \\[\\begin{equation}\\label{} x_t=\\mu+\\phi(x_{t-1}-\\mu)+w_t \\tag{6.111} \\end{equation}\\] donde \\(\\mu=50, \\phi=0.95\\) y \\(w_t\\) son iid doble exponencial con localización cero, y parámetro de escala \\(\\beta=2\\). La densidad de \\(w_t\\) está dada por \\[f_{w_t}(w)=\\frac{1}{2\\beta}\\exp[-|w|/\\beta]\\text{ con }-\\infty&lt;w&lt;\\infty\\] En este ejemplo, \\(\\mathbb{E}(w_t)=0\\) y \\(\\text{var}(w_t)=2\\beta^2=8\\). La figura 6.6 muestra \\(n=100\\) observaciones simuladas de este proceso. boot=scan(&quot;data/ar1boot.txt&quot;) plot(boot,type=&quot;b&quot;,xlab=&quot;Tiempo&quot;) Figura 6.6: Modelo causal estacionario, n=100 Esta realización en particular es interesante, ya que los datos lucen como si fuesen generados de un proceso no-estacionario con tres diferentes niveles de media. De hecho, los datos fueron generados por un modelo estacionario y causal de buen comportamiento, aunque no normal. Para mostrar las ventajas del bootstrap, procederemos como si no conociéramos la distribución del error y procederemos como si este fuera normal; por supuesto, esto significa, por ejemplo, que los EMV de \\(\\phi\\) basados en una normal no serán los EMV reales porque los datos no son normales. Usando los datos mostrado en la figura @ref{fig:grafico-modelo-estacionario-causal-n-100), obtenemos los estimadores de Yule-Walker \\(\\hat{\\mu}=40.0483, \\hat{\\phi}=0.9572\\) y \\(s_w^2=15.55\\), donde \\(s_w^2\\) es el estimado de \\(\\text{var}(w_t)\\). m=mean(boot) m ## [1] 40.05 fit=ar.yw(boot,order=1) fit ## ## Call: ## ar.yw.default(x = boot, order.max = 1) ## ## Coefficients: ## 1 ## 0.957 ## ## Order selected 1 sigma^2 estimated as 15.6 phi=fit$ar Basándonos en la proposición 6.9, diremos que \\(\\hat{\\phi}\\) es aproximadamente normal con media \\(\\phi\\) y varianza \\((1-\\phi^2)/100\\), la cual es aproximada por \\((1-0.957^2)/100=0.029^2\\). Para evaluar la distribución muestral finita de \\(\\hat{\\phi}\\) cuando \\(n=100\\), simularemos 1000 realizaciones de este proceso AR(1) y estimaremos los parámetros vía Yule-Walker. La densidad muestral finita del estimador Yule-Walker de \\(\\phi\\) basado en 1000 simulaciones se muestra en la figura 6.7. Claramente la distribución muestral no está cerca a la normalidad para este tamaño muestral. La media de la distribución mostrada es \\(0.8638\\) y la varianza es \\(0.122^2\\) estos valores son muy distintos de los valores asintóticos. # Densidad del estimador de Yule-Walker de phi phi.est=0 x.sim=boot[1] wt=rexp(100,1/2) for (i in 1:1000) { for (j in 2:100) {x.sim[j]=50+0.95*(x.sim[j-1]-50)+wt[j]} fit.est=ar.yw(x.sim,order=1) phi.est[i]=fit.est$ar} plot(density(phi.est),main=&quot;Densidad muestral finita de los estimadores de Yule-Walker de phi&quot;) Figura 6.7: Densidad muestral finita de los estimadores de Yule-Walker de phi Algunos de los cuantiles de la distribución muestral son: Cuantil 5% 10% 25% 50% 75% 90% 95% Valor 0.6747 0.6957 0.7587 0.8638 0.9689 1.0320 1.0530 Antes de discutir la técnica de bootstrap, estudiemos el proceso de innovación muestral \\(x_t-x_t^{t-1}\\) con la correspondiente varianza \\(P_t^{t-1}\\). Para el modelo AR(1) de este ejemplo \\[x_t^{t-1}=\\mu+\\phi(x_{t-1}-\\mu)\\text{, }t=2,\\ldots,100\\] De aquí, se sigue que \\[P_t^{t-1}=\\mathbb{E}(x_t-x_t^{t-1})^2=\\sigma_w^2\\text{, }t=2,\\ldots,100\\] Cuando \\(t=1\\), tenemos \\[x_1^0=\\mu\\text{ y }P_1^0=\\sigma_w^2/(1-\\phi^2)\\] Entonces, las innovaciones tiene media cero pero varianzas distintas; a fin de que todas las innovaciones tengan la misma varianza \\(\\sigma_w^2\\), las escribiremos como \\[\\begin{eqnarray} \\epsilon_1 &amp;=&amp; (x_1-\\mu)\\sqrt{(1-\\phi^2)} \\nonumber \\\\ \\epsilon_t &amp;=&amp; (x_t-\\mu)-\\phi(x_{t-1}-\\mu)\\text{, para }t=2,\\ldots,100 \\tag{6.112} \\end{eqnarray}\\] De estas ecuaciones, podemos escribir el modelo en término de las innovaciones \\(\\epsilon_t\\) como \\[\\begin{eqnarray} x_1 &amp;=&amp; \\mu+\\epsilon_1/\\sqrt{(1-\\phi^2)} \\nonumber\\\\ x_t &amp;=&amp; \\mu+\\phi(x_{t-1}-\\mu)+\\epsilon_t\\text{, para }t=2,\\ldots,100 \\tag{6.113} \\end{eqnarray}\\] A continuación, reemplazamos los parámetros con sus estimados en (6.112), esto es, \\(n=100, \\hat{\\mu}=40.048\\) y \\(\\hat{\\phi}=0.957\\) y denotamos los resultados de las innovaciones muestrales como \\(\\{\\hat{\\epsilon}_1,\\ldots,\\hat{\\epsilon}_{100}\\}\\). Para obtener una muestra bootstrap, primero escogemos una muestra aleatoria con reemplazo con \\(n=100\\) del conjunto de innovaciones muestral, llamemos a esta muestra \\(\\{\\epsilon_1^*,\\ldots,\\epsilon_{100}^*\\}\\). Ahora, generamos un conjunto de datos bootstrap secuencialmente haciendo \\[\\begin{eqnarray} x_1^* &amp;=&amp; 40.048+\\epsilon_1^*/\\sqrt{(1-0.957^2)} \\nonumber\\\\ x_t^* &amp;=&amp; 40.048+0.957(x_{t-1}^*-40.048)+\\epsilon_t^*\\text{, }t=2,\\ldots,n \\tag{6.114} \\end{eqnarray}\\] A continuación, estimamos los parámetros como si los datos fueran \\(x_t^*\\). Llamamos a estos estimados \\(\\hat{\\mu}(1),\\hat{\\phi}(1)\\) y \\(s_w^2(1)\\). Repetimos este proceso un número grande \\(N\\) de veces, generando una colección de parámetros estimados bootstrap \\(\\{\\hat{\\mu}(k),\\hat{\\phi}(k),s_w^2(k),k=1,\\ldots,N\\}\\). Podemos entonces aproximar la distribución muestral finita de un estimador de los valores del parámetro obtenido con bootstrap. Por ejemplo, podemos aproximar la distribución de \\(\\hat{\\phi}-\\phi\\) por la distribución empírica de \\(\\hat{\\phi}(k)-\\hat{\\phi}\\) para \\(k=1,\\ldots,N\\). La figura 6.8 muestra un histograma bootstrap de 200 estimaciones de \\(\\phi\\) hechas con bootstrap usando los datos en la figura @ref(fig:grafico-modelo-estacionario-causal-n-100}. En particular, la media de la distribución de \\(\\hat{\\phi}(k)\\) es \\(0.8750\\) con varianza \\(0.0556^2\\). Algunos cuantiles de esta distribución son: Cuantil 5% 10% 25% 50% 75% 90% 95% Valor 0.7833 0.8014 0.8412 0.8762 0.9135 0.9455 0.9672 # Booststrap nboot=200 resids=fit$resid resids=resids[2:100] boot.star=boot phi.star=matrix(0,nboot,1) for (i in 1:nboot){ resid.star=sample(resids,replace=TRUE) for (t in 1:99){ boot.star[t+1]=boot+phi*(boot.star[t]-boot)+resid.star[t] } phi.star[i]=ar.yw(boot.star,order=1)$ar } # Histograma hist(phi.star,breaks=15,col = &quot;lightblue&quot;, main=&quot;Histograma de frecuencia para phi estimado con bootstrap&quot;) Figura 6.8: Histograma bootstrap de phi basado en 200 iteraciones. 6.2 Modelos ARIMA En este capítulo examinaremos el problema de encontrar un modelo apropiado para un conjunto determinado de observaciones \\(\\{x_1,\\ldots,x_n\\}\\) que no son necesariamente generados por una serie de tiempo estacionaria. Si los datos (a) no muestran desviaciones aparentes de la estacionariedad y (b) tienen una función de autocovarianza en rápida disminución, intentamos ajustar un modelo ARMA a los datos medios corregidos utilizando las técnicas desarrolladas en el capítulo de modelos ARMA. De lo contrario, buscamos primero una transformación de los datos que genere una nueva serie con las propiedades (a) y (b). Esto puede lograrse frecuentemente mediante la diferenciación, lo que nos lleva a considerar la clase de modelos ARIMA (siglas en inglés: autoregressive integrated moving-average). En muchas situaciones, las series de tiempo pueden pensarse o ver como la composición de dos componentes, una componente de tendencia no estacionaria y una componente estacionaria de media cero. Por ejemplo, consideremos el modelo \\[\\begin{equation} x_t = \\mu_t+y_t, \\tag{6.115} \\end{equation}\\] donde \\(\\mu_t=\\beta_0+\\beta_1t\\) y \\(y_t\\) es estacionario. Si diferenciamos este proceso, obtenemos un proceso estacionario, en efecto \\[\\begin{eqnarray*} \\nabla x_t &amp;=&amp; x_t-x_{t-1} = (\\mu_t+y_t)-(\\mu_{t-1}+y_{t-1}) \\\\ &amp;=&amp; (\\beta_0+\\beta_1t+y_t)-(\\beta_0+\\beta_1(t-1)+y_{t-1}) \\\\ &amp;=&amp; \\beta_1 + y_t-y_{t-1} \\\\ &amp;=&amp; \\beta_1+\\nabla y_t. \\end{eqnarray*}\\] El cual claramente es estacionario. Otro modelo que lleva a la primera diferenciación es el caso en el cual \\(\\mu_t\\) en la ecuación (6.115), es un proceso estocástico y que varía lentamente de acuerdo a un paseo laeatorio. Esto es, \\[\\mu_t=\\mu_{t-1}+v_t,\\] donde \\(v_t\\) es estacionario. Tenemos entonces \\[x_t=\\mu_{t-1}+v_t+y_t.\\] En este caso, \\[\\begin{eqnarray*} \\nabla x_t &amp;=&amp; x_t-x_{t-1} \\\\ &amp;=&amp; (\\mu_{t-1}+v_t+y_t)-(\\mu_{t-2}+v_{t-1}+y_{t-1}) \\\\ &amp;=&amp; v_t+\\nabla y_t, \\end{eqnarray*}\\] es estacionario. Si \\(\\mu_t\\) en (6.115) es un polinomio de grado \\(k\\), \\(\\mu_t=\\sum_{i=0}^k\\beta_it^i\\), entonces la serie diferenciada \\(\\nabla^ky_t\\) es estacinaia. Por ejemplo, sea \\(\\mu_t=\\beta_0+\\beta_1t+\\beta_2t^2\\), luego \\[x_t=\\mu_t+y_t = \\beta_0+\\beta_1t+\\beta_2t^2+y_t.\\] Diferenciando una vez se tiene \\[\\begin{eqnarray*} \\nabla x_t &amp;=&amp; (\\mu_t+y_t)-(\\mu_{t-1}+y_{t-1}) \\\\ &amp;=&amp; (\\beta_0+\\beta_1t+\\beta_2t^2+y_t) - (\\beta_0+\\beta_1(t-1)+\\beta_2(t-1)^2+y_{t-1}) \\\\ &amp;=&amp; \\beta_0+\\beta_1t+\\beta_2t^2+y_t-\\beta_0-\\beta_1t-\\beta_1-\\beta_2(t^2-2t+1)-y_{t-1} \\\\ &amp;=&amp; \\beta_1+\\beta_2+2\\beta_2t+\\nabla y_t. \\end{eqnarray*}\\] Volvemos a diferenciar \\[\\begin{eqnarray*} \\nabla(\\nabla x_t) &amp;=&amp; (\\beta_1+\\beta_2+2\\beta_2t+(y_t-y_{t-1})) - (\\beta_1+\\beta_2+2\\beta_2(t-1)+(y_{t-1}-y_{t-2})) \\\\ &amp;=&amp; \\beta_1+\\beta_2+2\\beta_2t+y_t-y_{t-1}-\\beta_1-\\beta_2-2\\beta_2t+2\\beta_2-y_{t-1}+y_{t-2} \\\\ &amp;=&amp; 2\\beta_2+\\nabla(\\nabla y_t) \\\\ &amp;=&amp; 2\\beta_2+\\nabla^2y_t. \\end{eqnarray*}\\] El cual es estacionario. Los modelos estocásticos con tendencia llevan a diferenciaciones de orden superior. Por ejemplo, supongamos \\[\\mu_t=\\mu_{t-1}+v_t\\text{ y }v_t=v_{t-1}+e_t\\] donde \\(e_t\\) es estacionario. Entonces \\(\\nabla x_t = v_t+\\nabla y_t\\) no es estacionario, pero \\[\\nabla^2x_t = e_t+\\nabla^2y_t,\\] si es estacionario. Los modelos ARMA integrados o modelos ARIMA, es una extensión de los modelos ARMA que incluyen diferenciación. Formalmente, tenemos la siguiente definición. Definición 6.6 Un proceso \\(x_t\\) es un proceso ARIMA(p,d,q) si \\[\\nabla^dx_t = (1-B)^dx_t\\] es un proceso ARMA(p,q). En general, escribimos el modelo como \\[\\begin{equation} \\phi(B)(1-B)^dx_t = \\theta(B)w_t. \\tag{6.116} \\end{equation}\\] Si \\(\\mathbb{E}(\\nabla^dx_t)=\\mu\\), escribimos el modelo como \\[\\phi(B)(1-B)^dx_t=\\delta+\\theta(B)w_t,\\] donde \\(\\delta=\\mu(1-\\phi_1-\\cdots-\\phi_p)\\). Debido a la no estacionaridad, debemosser cuidadosos cuando realizamos predicciones. 6.2 Construcción de modelos ARIMA Hay algunos pasos básicos para ajustar modelos ARIMA a series de tiempo: Gráfico de los datos. Posible transformación de los datos (diferenciación, logaritmo, etc.). Identificación del orden de dependencia del modelo. Estimación del (los) parámetro(s). Diagnóstico. Elección del modelo. Primero, como en todo análisis de datos, debemos realizar un gráfico de serie de tiempo de los datos e inspeccionar el mismo para ver cualquier anomalía. Si, por ejemplo, la variabilidad crece en el tiempo, será necesario que transformemos los datos para estabilizar la varianza. En este caso, las transformaciones de potencia de la clase Box-Cox resultan útiles. Por ejemplo, supongamos un proceso que evoluciona como un cambio porcentual bastante pequeño, como una inversión. Supongamos que \\[x_t=(1+p_t)x_{t-1},\\] donde \\(x_t\\) es el valor de la inversión en tiempo \\(t\\) y \\(p_t\\) es el porcentaje de cambio del periodo \\(t-1\\) al \\(t\\), el cual puede ser negativo. Tomando logaritmo tenemos \\[\\ln(x_t) = \\ln(1+p_t)+\\ln(x_{t-1})\\] o \\[\\nabla\\ln(x_t) = \\ln(1+p_t).\\] Si el porcentaje de cambio \\(p_t\\) se mantiene relativamente pequeño en magnitud, entonces \\(\\ln(1+p_t)\\approx p_t\\) 11 y entonces \\[\\nabla\\ln(x_t)\\approx p_t,\\] será un proceso relativamente estable. A menudo \\(\\nabla\\ln(x_t)\\) se llama el retorno o tasa de crecimiento. Después de una transformación apropiada de los datos, el siguiente paso es identificar los valores preliminares del orden autoregresivo \\(p\\), el orden de diferenciación \\(d\\) y el orden de promedio móvil \\(q\\). Ya hemos abordado en parte el problema de la elección del orden de diferenciación. El gráfico de la serie de tiempo nos ayudará a deterinar si hace falta una diferenciación. Si se requiere una diferenciación, entonces diferenciamos los datos una vez \\((d=1)\\), e inspeccionamos el gráfico de \\(\\nabla x_t\\). Si hace falta otra diferenciación, entonces volvemos a diferenciar \\((d=2)\\) e inspeccionamos nuevamente el gráfico, esta vez el de \\(\\nabla^2x_t\\). Debemos tener cuidado de no sobre diferenciar pues esto puede introducir dependencia donde no la hay. Por ejemplo, \\(x_t=w_t\\) es no correlacionado, pero \\(\\nabla x_t=w_t-w_{t-1}\\) es un proceso \\(MA(1)\\). Además del gráfico de la serie de tiempo, la ACF muestral nos puede ayudar a ver si es necesaria una diferenciación. Dado que el polinomio \\(\\phi(z)(1-z)^d\\) tiene raíz unitaria, la ACF muestral \\(\\hat{\\rho}(h)\\), no decaerá a cero tan rápido cuando \\(h\\) crece. Entonces, un lento decaimiento en \\(\\hat{\\rho}(h)\\) es un indicativo de que se necesitará una diferenciación. Una vez que hemos establecido el valor preliminar de \\(d\\), el siguiente paso es ver la ACF y PACF muestrales de \\(\\nabla^dx_t\\) para el valor de \\(d\\) elegido. Usando la tabla resumen para la elección de modelos ARMA (véase la tabla al final de la sección Propiedades de los modelos ARMA(p,q)) como guía, escogemos los valores preliminares de \\(p\\) y \\(q\\). Dado que estamos trabajando con estimaciones, dos modelos que luzcan diferentes pueden ser de hecho bastante similares, por lo tanto, nodebemos preocuparnos, de momento, en ser muy precisos al ajustar un modelo. En este punto, unos pocos valores preliminares de \\(p,d\\) y \\(q\\) serán suficientes y nos permitirán iniciar las estimaciones de los parámetros. A continuación daremos un ejemplo de uso de los pasos previamente descritos. Ejemplo 6.25 (Análisis de datos G.N.P.) Consideremos los datos del Producto Nacional Bruto trimestral de EE.UU en miles de millónes de dólares, desde el primer trimestre de 1947 hasta el tercer trimestre de 2002. Son \\(n=223\\) observaciones, losdatos han sido ajustados estacionalmente. El archivo de datos es “gnp96.txt”, y fueron obtenidos del Federal Reserve Bank of St. Louis (http://research.stlouisfed.org/). El gráfico 6.9 muestra la serie de tiempo correspondiente. # Lectura de los datos gnp=read.table(&quot;data/gnp96.txt&quot;) # Grafico de la serie plot(gnp,type=&quot;l&quot;, xlab=&quot;Años&quot;,ylab=&quot;G.N.P.&quot;,col=&quot;blue&quot;) Figura 6.9: Producto Nacional Bruto trimestral de EE.UU (en miles de millónes de dólares), desde el 1er trimestre de 1947 hasta el 3er trimestre de 2002. Dado que la serie presenta una fuerte tendencia creciente, no es claro si la varianza crece con el tiempo. Por lo tanto para propósito de demostrar como usar la ACF muestral, en la figura 6.10 mostramos la misma. Como el decaimiento es lento, esto nos sugiere que una diferenciación es posible. # ACF muestral de GNP acf(gnp[,2], 50) Figura 6.10: ACF muestral para la serie de datos G.N.P. La figura 6.11 muestra la primera diferenciación, allí podemos observar que la variabilidad en la segunda mitad de datos es mayor que en la primera mitad. Además, parece que la tendencia creciente todavía está presente, porlo tanto, tomando en cuenta los pasos descritos al inicio de esta sección primero transformamos losdatos y luego diferenciamos, así obtenemos \\(y_t=\\nabla\\ln(x_t)\\). # Primera diferencia gnpdif=diff(gnp[,2]) plot(gnp[2:223,1],gnpdif, type=&quot;l&quot;,xlab=&quot;Años&quot;,ylab=&quot;diff(G.N.P.)&quot;, col=&quot;blue&quot;) Figura 6.11: Primera diferenciación de la serie de tiempo G.N.P. La figura 6.12 muestra la serie transformada y diferenciada, podemos ver allí que el proceso parece ser estable. Más aún, podemos interpretar los valores de \\(y_t\\) como el porcentaje de crecimiento trimestral del Producto Nacional Bruto de EE.UU. # Transformacion y primera diferencia gnpgr = diff(log(gnp[,2])) plot(gnp[2:223,1],gnpgr,type = &quot;l&quot;,xlab=&quot;Años&quot;,ylab=&quot;diff(G.N.P.)&quot;, col=&quot;blue&quot;) Figura 6.12: Serie de tiempo de G.N.P. transformada (log) y diferenciada una vez Graficamos ahora las ACF y PACF muestral de \\(y_t\\). Observando las ACF y PACF parece que la ACF se corta en paso 2 y la PACF decae, lo que nos sugiere un modelo \\(MA(2)\\) para la tasa de crecimiento del P.N.B., o un modelo ARIMA(0,1,2) para \\(y_t\\). Pero en lugar de enfocarnos en un solo modelo, si detallamos las ACF y PACF muestral, parece sugerir que la ACF decrece y la PACF se corta en paso 1, lo que sugiere un modelo \\(AR(1)\\) para la tasa de cambio o un modelo ARIMA(1,1,0) para \\(y_t\\). Podemos decir entonces que un modelo ARIMA(1,1,2) es una primera elección de ajuste. # ACF y PACF de la transformacion par(mfrow=c(2,1)) acf(gnpgr, 24) pacf(gnpgr,24) Figura 6.13: ACF y PACF muestrales para las serie de tiempo G.N.P., transformada y diferenciada A modo de entrenamiento vamos a realizar primeramente los ajustes de los modelos \\(MA(2)\\) y \\(AR(1)\\) por separado. Para ello nos valemos de R, usaremos la función ‘sarima’ del paquete ‘astsa’. Iniciamos con el modelo \\(AR(1)\\). gnpgr.ar=arima(gnpgr, c(1, 0, 0)) # AR(1) El modelo \\(AR(1)\\) estimado es \\(x_t=\\mu(1-\\phi_1)+\\phi_1x_{t-1}+\\hat{w}_t\\) \\[\\begin{equation} x_t = 0.0083_{(0.001)}(1-0.3467)+0.3467_{(0.063)}x_{t-1}+\\hat{w}_t \\tag{6.117} \\end{equation}\\] donde \\(\\hat{\\sigma}_w=0.0095\\) con 220 grados de libertad; note que la constante en (6.117) es \\(0.0083(1-0.3467)=0.005\\). Los valores entre parentesis son los errores estándar estimados. Ahora usando EMV fijamos un modelo \\(MA(2)\\) para la tasa de crecimiento \\(x_t\\), siendo el modelo estimado \\[\\begin{equation} x_t = 0.008_{(0.001)}+0.303_{(0.065)}\\hat{w}_{t-1}+0204_{(0.064)}\\hat{w}_{t-2}+\\hat{w}_t \\tag{6.118} \\end{equation}\\] donde \\(\\hat{\\sigma}_w=0.0094\\) con 219 grados de libertad. Los valores entre parentesis corresponden a los errores estándar estimados. Aunque la constante es muy pequeña, su valor es significativo, no incluir una constante lleva a conclusiones erróneas sobre la naturaleza de la economía estadounidense. Si no incluimos una constane, asumiríamos que la tasa de crecimiento trimestral promedio es cero, mientras que en realidad la tasa de crecimiento trimestral promedio del P.N.B. de EE.UU es de alrededor del 1% (véase la gráfica 6.9). gnpgr.ma2=arima(gnpgr, c(0, 0, 2)) # MA(2) # Psi-pesos ARMAtoMA(ar=.3467, ma=0, 10) # prints psi-weights ## [1] 3.467e-01 1.202e-01 4.167e-02 1.445e-02 5.009e-03 ## [6] 1.737e-03 6.021e-04 2.088e-04 7.237e-05 2.509e-05 El siguiente paso en el ajuste de modelos es el diagnóstico. Esta investigación incluye el análisis de residuales así como la comparación de modelos. De nuevo, el primer paso envuelve un gráfico de las innovaciones (o residuales) \\(x_t-\\hat{x}_t^{t-1}\\) o de las innovaciones estandarizadas \\[\\begin{equation} e_t = (x_t-\\hat{x}_t^{t-1})/\\sqrt{\\hat{P}_t^{t-1}} \\tag{6.119} \\end{equation}\\] donde \\(\\hat{x}_t^{t-1}\\) es la predicción de un paso de \\(x_t\\) basado en el modelo ajustado y \\(\\hat{P}_t^{t-1}\\) es varianza del error estimado de un paso. Si el modelo se ajusta bien, el residual estandarizado debe comportarse como una sucesión iid de media cero y varianza uno, así que debemos observar bien el gráfico de la serie de tiempo para ver si hay desviación evidente de esta suposición. Por ejemplo, es posible en el caso no-gaussiano tener un proceso no-correlacionado para el cual valores contiguos en tiempo sean altamente dependientes. Como ejemplo, podemos mencionar la familia de modelos GARCH que discutiremos en el capítulo siguiente. Para determinar o investigar sobre la normaidad marginal nos valemos del histograma de los residuales, así visualmente podemos ver si el mismo se parece o ajusta a la curva de densidad normal. Además de esto, un gráfico de probabilidad normal o un gráfico de cuantiles (qq-plot) nos puede ayudar a identificar la desviación de la normalidad. También podemos inspeccionar la autocorrelación muestral de los residuales \\(\\hat{\\rho}_e(h)\\), para ver algún patrón o valores grandes. Recordemos que, para un ruido blanco, las autocorrelaciones muestrales son aproximadamente independientes y normalmente distribuidas con media cero y varianza \\(1/n\\). Por consiguiente, una buena forma de inspeccionar la estructura de correlación de los residuales es graficar \\(\\hat{\\rho}_e(h)\\) vs \\(h\\) junto con las cotas de error \\(\\pm2/\\sqrt{n}\\). Tome en cuenta, sin embargo, que los residuales de un modelo ajustado, no tendrán necesariamente las propiedades de un ruido blanco y la varianza de \\(\\hat{\\rho}_e(h)\\) puede ser mucho menor que \\(1/n\\). Además de graficar \\(\\hat{\\rho}_e(h)\\), podemos realizar una prueba de hipótesis general que tome en consideración las magnitudes de \\(\\hat{\\rho}_e(h)\\) como grupo. Por ejemplo, puede ser el caso que individualmente cada \\(\\hat{\\rho}_e(h)\\) sea pequeño en magnitud, digamos menr que \\(2/\\sqrt{n}\\) en magnitud, es decir \\(|\\hat{\\rho}_e(h)|&lt;2/\\sqrt{n}\\), pero colectivamente, los valores sean grandes. El estadístico de Ljung-Box-Pierce dado por \\[\\begin{equation} Q = n(n+2)\\sum_{h=1}^H\\frac{\\hat{\\rho}_e^2(h)}{n-h} \\tag{6.120} \\end{equation}\\] es útil para realizar esta prueba de hipótesis. El valor \\(H\\) en (6.120) se elige de manera arbitraria, en general se usa \\(H=20\\). Bajo la hipótesis nula de que el modelo es adecuado, asintóticamente, (cuando \\(n\\to\\infty\\)), \\(Q\\) se distribuye como una chi-cuadrado con \\(H-p-q\\) grados de libertad, esto es \\(Q\\sim\\chi_{H-p-q}^2\\). Entonces, rechazamos la hipótesis nula a nivel \\(\\alpha\\) si el valor de \\(Q\\) es mayor que la \\(\\chi_{H-p-q}^2(1-\\alpha)\\). Ejemplo 6.26 (Diagnóstico para la tasa de crecimiento del P.N.B) Enfoquémonos en el modelo \\(MA(2)\\) ajustado del ejemplo 6.25, el análisis de los residuales de \\(AR(1)\\) es similar. La figura ?? muestra el gráfico de los residuales estandarizados (parte superior), la ACF de los residuales (parte media izquierda) (Note que R incluye la correlación en paso cero que siempre es uno) y los valores del estadístico \\(Q\\), dado en (6.120) desde paso \\(H=1\\) hasta \\(H=20\\) (parte inferior). tsdiag(gnpgr.ma2,gof.lag=20) Figura 6.14: Diagnóstico de los residuales con el modelo MA(2) de la serie P.N.B. Observando el gráfico de los residuales estandarizados en 6.14, no muestran un patrón obvio. Note que no hay valores atípicos pero si algunos pocos valores mayores que 3 desviaciones estándar. La ACF de los residuales no muestra una aparente desviación de la suposición del modelo, y el estadístico \\(Q\\) no es significativo para los primeros 20 pasos calculados. Finalmente la figura 6.15 muestra un histograma de los residuales (parte superior) y un gráfico qq-plot de los residuales (parte inferior). En la misma podemos observar que los residuales están cercanos a la normalidad excepto para algunos valores extremos en la cola. par(mfrow=c(2,1)) hist(gnpgr.ma2$resid,br=12) qqnorm(gnpgr.ma2$resid) Figura 6.15: Histograma de los residuales para P.N.B. (parte superior), y qq-plot de los residuales (parte inferior), para el modelo MA(2) Para concluir, realizamos la prueba de Shapiro-Wilk (referencia), la cual nos da un \\(p\\)-valor de 0.003, lo que indica que los residuales no son normal. Por lo tanto, el modelo parece ajustarse bien salvo que debemos usar una distribución con una cola más pesada que la distribución normal. El comando en R para la prueba de Shapiro-Wilk es shapiro.test(gnpgr.ma2$resid) ## ## Shapiro-Wilk normality test ## ## data: gnpgr.ma2$resid ## W = 0.98, p-value = 0.003 Podemos hacer lo mismo ahora para el modelo ARIMA(1,1,2) gnpgr.arima=arima(gnpgr,order=c(1,1,2)) tsdiag(gnpgr.arima,gof.lag=20) Figura 6.16: Diagnóstico para el modelo ARIMA de B.N.P Nuevamente, observando el gráfico de los residuales estandarizados, estos no muestran un patrón obvio. Note que tampoco hay valores atípicos pero si algunos pocos valores mayores que 3 desviaciones estándar. La ACF de los residuales no muestra una aparente desviación de la suposición del modelo, y el estadístico \\(Q\\) no es significativo para los primeros 20 pasos calculados. par(mfrow=c(2,1)) hist(gnpgr.arima$resid) qqnorm(gnpgr.arima$resid) Figura 6.17: Histograma de los residuales para P.N.B. (parte superior), y qq-plot de los residuales (parte inferior), para el modelo ARIMA(1,1,2) shapiro.test(gnpgr.arima$resid) ## ## Shapiro-Wilk normality test ## ## data: gnpgr.arima$resid ## W = 0.98, p-value = 0.001 La figura 6.17 muestra un histograma de los residuales (parte superior) y un gráfico qq-plot de los residuales (parte inferior). En la misma, de manera similar al modelo \\(MA(2)\\) podemos observar que los residuales están cercanos a la normalidad excepto para algunos valores extremos en la cola. Por último, realizamos la prueba de Shapiro-Wilk, la cual nos da un \\(p\\)-valor de 0.001, lo que indica que los residuales no son normales. Por lo tanto, de nuevo, este modelo parece también ajustarse bien salvo que debemos usar una distribución con una cola más pesada que la distribución normal. Como explicamos previamente, debemos tener cuidado con sobreajustar un modelo; no siempre es el caso que más es mejor. Sobreajustar nos lleva a estimadores menos preciso, y agregar más parámetros puede ajustar mejor los datos pero puede llevar a malas predicciones. Este resultado se ilustra en el ejemplo siguiente. Ejemplo 6.27 (Un problema de sobreajuste) La figura ??, muestra la población de los EE.UU., según el censo oficial cada 10 años de 1910 hasta 1990 (puntos azules). Si usamos estos nueve puntos para predecir la población a futuro de los EE.UU. podemos usar un polinomio de grado 8 para ajustar las 9 observaciones; lo cual como se observa en la gráfica es perfecta. El modelo en este caso es \\[x_t=\\beta_0+\\beta_1t+\\beta_2t^2+\\cdots+\\beta_8t^8+w_t\\] El modelo fijado, el cual es graficado hasta el año 2010, (linea continua roja), pasa a través de los 9 puntos. El modelo predice que la población de los EE.UU. estará cercana a cero en el año 2000, y cruzará el cero en algún mes del año 2002, lo cual es falso. {rfig.cap=&quot;Población de los EE.UU (puntos azules) y modelos ajustado (linea roja continua) de 1910 hasta 2010&quot;,fig-sobreajuste-poblacion-usa} uspop=read.table(&quot;data/USPOP2.txt&quot;, header = TRUE) fit.usp=lm(Pob~t+I(t^2)+I(t^3)+I(t^4)+I(t^5)+I(t^6)+I(t^7)+I(t^8), data=uspop) plot(uspop,type=&quot;p&quot;,lty=19,col=&quot;blue&quot;,xlim=c(1910,2010)) lines(x = uspop$t, y=predict(fit.usp), col = &quot;red&quot;, lwd = 2) —- El paso final en el ajuste de modelos es la elección del modelo. Esto es, debemos decidir que modelo mantendremos para la predicción. La técnica más popular es calcular los índices AIC, AICc y SIC (BIC), descritos en las definiciones 4.3, 4.4 y 4.5. Ejemplo 6.28 (Elección del modelo para la serie P.N.B. de EE.UU.) Volviendo al análisis del P.N.B. de EE.UU., visto en los ejemplos 6.25 y 6.26, recordemos que los modelos son AR(1), MA(2) y ARIMA(1,1,2). Para escoger el modelo final, comparemos los valores del AIC, AICc y SIC para los 3 modelos n=length(gnpgr) # Modelos AR kar=length(gnpgr.ar$coef) sar=gnpgr.ar$sigma2 # Modelos MA kma=length(gnpgr.ma2$coef) sma=gnpgr.ma2$sigma2 # Modelo ARIMA karima=length(gnpgr.arima$coef) sarima=gnpgr.arima$sigma2 # AIC log(sar)+(n+2*kar)/n ## [1] -8.294 log(sma)+(n+2*kma)/n ## [1] -8.298 log(sarima)+(n+2*karima)/n ## [1] -8.285 # AICc log(sar)+(n+kar)/(n-kar-2) ## [1] -8.285 log(sma)+(n+kma)/(n-kma-2) ## [1] -8.288 log(sarima)+(n+karima)/(n-karima-2) ## [1] -8.275 # BIC (SIC) log(sar)+kar*log(n)/n ## [1] -9.264 log(sma)+kma*log(n)/n ## [1] -9.252 log(sarima)+karima*log(n)/n ## [1] -9.239 6.2 Modelos SARIMA En esta sección vamos a introducir diversas modificaciones a los modelos ARIMA para que se ajusten a comportamiento estacional y no-estacionario. A menudo, la dependencia del pasado tiende a ocurrir más fuertemente en múltiplos de algún paso estacinal \\(s\\) oculto. Por ejemplo, con datos económicos mensuales, existe una fuerte componente anual con pasos que son múltiplos de \\(s=12\\), debido a la fuerte conexión de todas las actividades al calendario anual. Los datos tomados trimestralmente exhibirán un período repetitivo para \\(s=4\\). Los fenómenos naturales tales como temperatura, lluvia, etc., también presentan una fuerte componente correspondiente a la estación del año. Por consiguiente, la variabilidad natural de muchos fenómenos físicos, biológicos y procesos económicos tienden a comportarse según las fluctuaciones estacionales. Debido a esto, es apropiado introducir polinomios autorregresivo de promedio móvil que se identifiquen con los rezagos estacionales. Definición 6.7 El modelo autorregresivo de promedio móvil estacional puro denotado \\(ARMA(P,Q)_s\\) tiene la forma \\[\\begin{equation} \\Phi_P(B^s)x_t=\\Theta_Q(B^s)w_t, \\tag{6.121} \\end{equation}\\] donde los operadores \\[\\begin{equation} \\Phi_P(B^s) = 1-\\Phi_1B^s-\\Phi_2B^{2s}-\\cdots-\\Phi_PB^{Ps} \\tag{6.122} \\end{equation}\\] y \\[\\begin{equation} \\Theta_Q(B^s) = 1+\\Theta_1B^s+\\Theta_2B^{2s}+\\cdots+\\Theta_QB^{Qs}, \\tag{6.123} \\end{equation}\\] son los operadores autorreggresivo estacional y de promedio móvil estacional de ordenes \\(P\\) y \\(Q\\) respectivamente, con período estacional \\(s\\). Análogo a las propiedades de los modelos \\(ARMA\\) no-estacionales, el modelo \\(ARMA(P,Q)_s\\) puro es causal sólo cuando las raíces de \\(\\Phi_P(z^s)\\) están fuera del círculo unitario y es invertible cuando las raíces de \\(\\Theta_Q(z^s)\\) están fuera del círculo unitario. Ejemplo 6.29 (Una serie AR estacional) Una serie autorregresiva estacional de primer orden que podría durar meses la podemos escribir como \\[(1-\\Phi B^{12})x_t=w_t\\] o \\[x_t=\\phi x_{t-12}+w_t.\\] Este modelo exhibe la serie \\(x_t\\) en términos de saltos o rezagos múltiplos del periodo estacional anual \\(s=12\\) meses. De la forma anterior se desprende cláramente que la estimación y el pronóstico para tal proceso sólo implica modificaciones directas del cso de rezago unitario que ya tratamos. En particular, la condición causal requiere \\(|\\Phi|&lt;1\\). Simulamos 3 años de datos de este modelo con \\(\\Phi=0.9\\) y mostramos las ACF y PACF teóricas del modelo. Véase la figura 6.18 set.seed(666) phi=c(rep(0,11),0.9) sAR=arima.sim(list(order=c(12,0,0),ar=phi),n=37) sAR = ts(sAR, freq=12) layout(matrix(c(1,1,2, 1,1,3), nc=2)) par(mar=c(3,3,2,1), mgp=c(1.6,.6,0)) plot(sAR, axes=FALSE, main=&#39;Serie AR(1) estacional&#39;, xlab=&quot;años&quot;, type=&#39;c&#39;) Months = c(&quot;E&quot;,&quot;F&quot;,&quot;M&quot;,&quot;A&quot;,&quot;M&quot;,&quot;J&quot;,&quot;J&quot;,&quot;A&quot;,&quot;S&quot;,&quot;O&quot;,&quot;N&quot;,&quot;D&quot;) points(sAR, pch=Months, cex=1.25, font=4, col=1:4) axis(1, 1:4); abline(v=1:4, lty=2, col=gray(.7)) axis(2); box() ACF = ARMAacf(ar=phi, ma=0, 100) PACF = ARMAacf(ar=phi, ma=0, 100, pacf=TRUE) plot(ACF,type=&quot;h&quot;, xlab=&quot;Rezago&quot;, ylim=c(-.1,1)); abline(h=0) plot(PACF, type=&quot;h&quot;, xlab=&quot;Rezago&quot;, ylim=c(-.1,1)); abline(h=0) Figura 6.18: Datos generados de un modelo estaconal AR(1), con s=12 y las funciones ACF y PACF del modelo x_t=0.9x_{t-12}+w_t Para un modelo \\(MA(1)\\) estacional con \\(s=12\\), \\(x_t=w_t+\\Theta w_{t-12}\\), es fácil verificar que \\[\\begin{eqnarray*} \\gamma(0) &amp;=&amp; (1+\\Theta^2)\\sigma^2 \\\\ \\gamma(\\pm12) &amp;=&amp; \\Theta\\sigma^2 \\\\ \\gamma(h) &amp;=&amp; 0 \\text{, cualquier otro caso.} \\end{eqnarray*}\\] Entonces, la única correlación no cero, aparte del paso 0 es \\[\\rho(\\pm12) = \\Theta/(1+\\Theta^2).\\] Para un modelo \\(AR(1)\\) estacional con \\(s=12\\), usando las técnicas para el modelo \\(AR(1)\\) no-estacional, tenemos \\[\\begin{eqnarray*} \\gamma(0) &amp;=&amp; \\sigma^2/(1-\\Phi^2) \\\\ \\gamma(\\pm12) &amp;=&amp; \\sigma^2\\Phi^k/(1-\\Phi^2), k=1,2,\\ldots \\\\ \\gamma(h) &amp;=&amp; 0 \\text{, cualquier otro caso.} \\end{eqnarray*}\\] En este caso, las únicas correlaciones no cero son \\[\\rho(\\pm12) = \\Phi^k, k=0,1,2,\\ldots.\\] Estos resultados se pueden verificar usando el resultado general \\[\\gamma(h) = \\Phi\\gamma(h-12)\\text{, para }h\\geq1.\\] Por ejemplo, cuando \\(h=1\\), \\(\\gamma(1)=\\Phi\\gamma(11)\\), pero para \\(h=11\\), se tiene que \\(\\gamma(11)=\\Phi\\gamma(1)\\), lo que implica que \\(\\gamma(1)=\\gamma(11)=0\\). Adicional a estos resultados, la PACF tiene extensión análoga del modelo no-estacional al estacional. Estos resultados se observan en la figura 6.18. Como un criterio de diagnóstico inicial, podemos usar las propiedades de una serie autorregresiva de promedio móvil estacional puro que se muestran en la tabla siguiente. Estas propiedades las podemos considerar como una generalización de las propiedades para modelos no estacionales que presentamos en la sección (referencia) AR(P)s MA(Q)s ARMA(P,Q)s ACF* Cola fuera en Corte después Cola fuera en paso k, k=1,2,… de paso Q paso k PACF* Corte después Cola fuera en Cola fuera en de paso P paso k, k=1,2,… paso k *Los valores en paso no-estacional \\(h\\neq k\\), para \\(k=1,2,\\ldots\\) son cero. En general, podemos combinar los operadores no estacionales y estacionales en un solo modelo. Definición 6.8 Un modelo multiplicativo autorregresivo de promedio móvil estacional denotado por \\(ARMA(p,q)\\times(P,Q)_s\\) tiene la forma \\[\\begin{equation} \\Phi_P(B^s)\\phi(B)x_t = \\Theta_Q(B^s)\\theta(B)w_t \\tag{6.124} \\end{equation}\\] Aunque las propiedades de diagnóstico en la tabla anterior no son estrictamente ciertas para el modelo general mixto, el comportamiento de las ACF y PACF tienden a mostrar patrones aproximados de la forma indicada. De hecho, para modelos mixtos, podemos ver una mezcla de las propiedades listadas en las tablas mencionadas. Al ajustar tales modelos, nos centraremos primero en los componentes estacionales autorregresivo de promedio móvil estacional, lo que en general nos conduce a resultados más satisfactorios. Ejemplo 6.30 (Un modelo estacional mixto) Consideremos un modelo \\(ARMA(0,1)\\times(1,0)_{12}\\) \\[x_t = \\Phi x_{t-12}+w_t+\\theta w_{t-1},\\] donde \\(|\\Phi|&lt;1\\) y \\(|\\theta|&lt;1\\). Entonces, dado que \\(x_{t-12}, w_t\\) y \\(w_{t-1}\\) son no-correlacionados, y \\(x_t\\) es estacionario \\(\\gamma(0)=\\Phi^2\\gamma(0)+\\sigma_w^2+\\theta^2\\\\sigma_w^2\\) o \\[\\gamma(0) = \\frac{1+\\theta^2}{1-\\Phi^2}\\sigma_w^2.\\] Además, multiplicando el modelo por \\(x_{t-h}, h&gt;0\\), y tomando valor esperado, tenemos \\(\\gamma(1)=\\Phi\\gamma(11)+\\theta\\sigma_w^2\\), y \\(\\gamma(h)=\\Phi\\gamma(h-12)\\) para \\(h\\geq2\\). Entonces, la ACF para este modelo es \\[\\begin{eqnarray*} \\rho(12h) &amp;=&amp; \\Phi^h, h=1,2,\\ldots\\\\ \\rho(12h-1) &amp;=&amp; \\rho(12h+1) = \\frac{\\theta}{1+\\theta^2}\\Phi^h, h=0,1,2,\\ldots \\\\ \\rho(h) &amp;=&amp; 0 \\text{, en otro caso.} \\end{eqnarray*}\\] Las ACF y PACF para este modelo, con \\(\\Phi=0.8\\) y \\(\\theta=-0.5\\) se muestran en la Figura 6.19. Los comandos en \\(R\\) para reproducir la Figura 6.19 son los siguientes. phi = c(rep(0,11),.8) ACF = ARMAacf(ar=phi, ma=-.5, 50)[-1] # [-1] remueve el rezago 0 PACF = ARMAacf(ar=phi, ma=-.5, 50, pacf=TRUE) par(mfrow=c(1,2)) plot(ACF, type=&quot;h&quot;, xlab=&quot;Rezago&quot;, ylim=c(-.4,.8)); abline(h=0) plot(PACF, type=&quot;h&quot;, xlab=&quot;Rezago&quot;, ylim=c(-.4,.8)); abline(h=0) Figura 6.19: ACF y PACF de un modelo ARMA estacional mixto La persistencia estacional ocurre cuando el proceso es casi periódico en la temporada. Por ejemplo, con promedios de temperaturas mensuales sobre los años, cada enero será aproximadamente igual, cada febrero será aproximadamente el mismo, y así sucesivamente. En este caso, podemos pensar que la temperatura promedio mensual \\(x_t\\) es modelada como \\[x_t = S_t+w+t,\\] donde \\(S_t\\) es una componente estacional que varia poco de un año a otro de acuerdo a un paseo aleatorio \\[S_t = S_{t-12}+v_t.\\] En este modelo, \\(w_t\\) y \\(v_t\\) son ruidos blancos no-correlacionados. La tendencia de los datos que sigue este tipo de modelos se exhibe en la ACF muestral que es grande y decae muy lentamente en los rezagos \\(h=12k\\) para \\(k=1,2,\\ldots\\). Si sustraemos el efecto de años sucesivos el uno del otro, encontramos que \\[(1-B^{12})x_t = x_t-x_{t-12} = v_t+w_t-w_{t-12}.\\] Este modelo es un modelo \\(MA(1)_{12}\\) estacionario y su ACF tendrá un pico solo en paso 12. En general, la diferenciación estacional puede ser indicada cuando la ACF decae lentamente en múltiplos de algún período estacional \\(s\\), pero es despreciable entre los períodos. Definición 6.9 La diferencia estacional de orden \\(D\\) se define como \\[\\begin{equation} \\nabla_s^Dx_t=(1-B^2)^Dx_t \\tag{6.125} \\end{equation}\\] Normalmente, \\(D=1\\), es suficiente para obtener estacionaridad estacional. incorporando estas ideas al modelo general nos lleva a la siguiente definición. Definición 6.10 Un modelo autorregresivo integrado de promedio móvil estacional multiplicativo o modelo \\(SARIMA\\) está dado por \\[\\begin{equation} \\Phi_P(B^s)\\phi(B)\\nabla_s^D\\nabla^dx_t = \\delta+\\Theta_Q(B^s)\\theta(B)w_t, \\tag{6.126} \\end{equation}\\] donde \\(w_t\\) es un ruido blanco gaussiano. El modelo general es denotado como \\(ARIMA(p,d,q)\\times(P,D,Q)_s\\). Las componentes autorregresiva y de promedio móvil ordinarias son representadas por \\(\\phi(B)\\) y \\(\\theta(B)\\) de órdenes \\(p\\) y \\(q\\) respectivamente, y las componentes autorregresivas y de promedio móvil estacionales por \\(\\Phi_P(B^s)\\) y \\(\\Theta_Q(B^s)\\) de órdenes \\(P\\) y \\(Q\\), y las componentes de diferencias ordinarias y estacionales \\(\\nabla^d=(1-B)^d\\) y \\(\\nabla_s^D=(1-B^s)^D\\). Ejemplo 6.31 (Un modelo SARIMA) Consideremos el siguiente modelo, el cual a menudo provee una representación razonable para seires econométricas estacionales y no estacionarias. Mostramos la ecuación para el modelo, denotado por \\(ARIMA(0,1,1)\\times(0,1,1)_{12}\\) en la notación de la definición anterior (Definición 6.10), donde las fluctuaciones estacionales ocurren cada 12 meses. Entonces con \\(\\delta=0\\), el modelo (6.126) llega a ser \\[\\nabla_{12}\\nabla x_t=\\Theta(B^{12})\\theta(B)w_t,\\] o \\[\\begin{equation} (1-B^{12})(1-B)x_t = (1+\\Theta B^{12})(1+\\theta B)w_t. \\tag{6.127} \\end{equation}\\] Expandiendo ambos lados de (6.127), obtenemos la representación \\[(1-B-B^{12}+B^{13})x_t = (1+\\theta B+\\Theta B^{12}+\\Theta\\theta B^{13})w_t,\\] o en la forma de ecuaciones en diferencias \\[x_t = x_{t-1}+x_{t-12}-x_{t-13}+w_t+\\theta w_{t-1}+\\Theta w_{t-12}+\\Theta\\theta w_{t-13}.\\] Note que la naturaleza multiplicativa del modelo implica que el coeficiente de \\(w_{t-13}\\) es el producto de los coeficientes de \\(w_t\\) y \\(w_{t-12}\\), en lugar de un parámetro libre. El supuesto del modelo multiplicativo parece funcionar bien con muchos conjuntos de datos de series de tiempo estacionales a la vez que reduce el número de parámetros que debemos estimar. Seleccionar el modelo apropiado para un conjunto de datos dado entre todos los posibles modelos representados por la ecuación (6.126) es una tarea desalentadora, y generalmente pensamos primero en términos de encontrar operadores de diferencia que producen una serie más o menos estacionaria y luego en términos de encontrar un modelo autorregresivo de promedio móvil simple o un modelo ARMA multiplicativo estacional para adaptarlo a la serie de residuales resultante. Primero aplicamos operaciones de diferenciación y luego construimos los residuos a partir de una serie de tamaño reducido. A continuación, evaluamos las ACF y PACF de estos residuos. Los picos que aparecen en estas funciones a menudo pueden eliminarse fjando o ajustando una componente autorregresiva o una componente de promedio móvil de acuerdo con las propiedades de las Tablas para las funciones ACF y PACF. Al considerar si el modelo es satisfactorio podemos aplicar las técnicas de diagnóstico discutidas en la Sección Construcción de modelos ARIMA. Ejemplo 6.32 (Pasajeros aéreos) Consideremos el conjunto de datos de \\(R\\) “AirPassengers”, que son los totales mensuales de pasajeros de lineas aereas internacionales de 1949 a 1960. En la Figura 6.20 mostramos la serie de datos así como 3 transformaciones de los mismos. Primero una transformación logarítmica, luego una diferenciación de un paso sobre esta, y finalmente una diferenciación adicional de orden 12. Las instrucciones en \\(R\\) son: x = AirPassengers lx = log(x); dlx = diff(lx); ddlx = diff(dlx, 12) plot.ts(cbind(x,lx,dlx,ddlx), main=&quot;&quot;) Figura 6.20: Serie de tiempo AirPassengers, x, (parte superior), el cual es los totales mensuales de pasajeros de lineas aéreas internacionales de 1949 a 1960; y los datos transformados lx=log x_t (segundo cuadro); dlx=diff(log x_t) (tercer cuadro) y ddlx=diff_12 diff(log x_t) (cuadro inferior) Observe que la serie original \\(x\\) muestra tendencia y varianza crecientes; en \\(lx\\) están los datos transformados logarítmicamente y en estso la varianza se estabiliza. Luego diferenciamos la serie transformada para eliminar la tendencia, y la guardamos en \\(dlx\\). Se observa claramente la persistencia estacional (i.e., \\(dlx_t\\approx dlx_{t-12}\\)) de modo que aplicamos una diferenciación de orden 12. Los datos transformados parecen ser estacionarios, así que estamos listos para fijar un modelo inicial a los mismos. Las ACF y PACF muestrales de \\(ddlx (\\nabla_{12}\\nabla\\log x_t)\\) los mostramos en la Figura 6.21. acf2(ddlx,50) Figura 6.21: ACF y PACF muestrales de ddlx ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## ACF -0.34 0.11 -0.20 0.02 0.06 0.03 -0.06 0.00 ## PACF -0.34 -0.01 -0.19 -0.13 0.03 0.03 -0.06 -0.02 ## [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] ## ACF 0.18 -0.08 0.06 -0.39 0.15 -0.06 0.15 -0.14 ## PACF 0.23 0.04 0.05 -0.34 -0.11 -0.08 -0.02 -0.14 ## [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] ## ACF 0.07 0.02 -0.01 -0.12 0.04 -0.09 0.22 -0.02 ## PACF 0.03 0.11 -0.01 -0.17 0.13 -0.07 0.14 -0.07 ## [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] ## ACF -0.1 0.05 -0.03 0.05 -0.02 -0.05 -0.05 0.20 ## PACF -0.1 -0.01 0.04 -0.09 0.05 0.00 -0.10 -0.02 ## [,33] [,34] [,35] [,36] [,37] [,38] [,39] [,40] ## ACF -0.12 0.08 -0.15 -0.01 0.05 0.03 -0.02 -0.03 ## PACF 0.01 -0.02 0.02 -0.16 -0.03 0.01 0.05 -0.08 ## [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] ## ACF -0.07 0.10 -0.09 0.03 -0.04 -0.04 0.11 -0.05 ## PACF -0.17 0.07 -0.10 -0.06 -0.03 -0.12 -0.01 -0.05 ## [,49] [,50] ## ACF 0.11 -0.02 ## PACF 0.09 0.13 Componente estacional: Parece que en la estacionalidad, la ACF se corta en paso \\(1s\\) (\\(s=12\\)), mientras que la PACF se rezaga en pasos \\(1s,2s,3s,4s,\\ldots\\). Estos resultados implican un \\(SMA(1), P=0,Q=1\\), en la componente estacional (s=12). Componente no-estacional: Inspeccionando las ACf y PACF muestrales en los primeros pasos, parece que ambas colas decaen. Esto sugiere un modelo \\(ARMA(1,1)\\), dentor de las estaciones, \\(p=q=1\\). Entonces podemos empear con el modelo \\(ARIMA(1,1,1)\\times(0,1,1)_{12}\\) sobre la serie \\(lx\\). sarima(lx, 1,1,1, 0,1,1,12) ## initial value -3.085211 ## iter 2 value -3.225399 ## iter 3 value -3.276697 ## iter 4 value -3.276902 ## iter 5 value -3.282134 ## iter 6 value -3.282524 ## iter 7 value -3.282990 ## iter 8 value -3.286319 ## iter 9 value -3.286413 ## iter 10 value -3.288141 ## iter 11 value -3.288262 ## iter 12 value -3.288394 ## iter 13 value -3.288768 ## iter 14 value -3.288969 ## iter 15 value -3.289089 ## iter 16 value -3.289094 ## iter 17 value -3.289100 ## iter 17 value -3.289100 ## iter 17 value -3.289100 ## final value -3.289100 ## converged ## initial value -3.288388 ## iter 2 value -3.288459 ## iter 3 value -3.288530 ## iter 4 value -3.288649 ## iter 5 value -3.288753 ## iter 6 value -3.288781 ## iter 7 value -3.288784 ## iter 7 value -3.288784 ## iter 7 value -3.288784 ## final value -3.288784 ## converged ## $fit ## ## Call: ## arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), ## include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, ## REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ma1 sma1 ## 0.196 -0.578 -0.564 ## s.e. 0.247 0.213 0.075 ## ## sigma^2 estimated as 0.00134: log likelihood = 244.9, aic = -481.9 ## ## $degrees_of_freedom ## [1] 128 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.1960 0.2475 0.7921 0.4298 ## ma1 -0.5784 0.2132 -2.7127 0.0076 ## sma1 -0.5643 0.0747 -7.5544 0.0000 ## ## $AIC ## [1] -3.679 ## ## $AICc ## [1] -3.677 ## ## $BIC ## [1] -3.591 Sin embargo, el parámetro \\(AR\\) no es significativo, así que intentamos eliminando un parámetro de la parte dentro de las estaciones. En este caso probaremos con los modelos \\(ARIMA(0,1,1)\\times(0,1,1)_{12}\\) y \\(ARIMA(1,1,0)\\times(0,1,1)_{12}\\). sarima(lx, 0,1,1, 0,1,1,12) ## initial value -3.086228 ## iter 2 value -3.267980 ## iter 3 value -3.279950 ## iter 4 value -3.285996 ## iter 5 value -3.289332 ## iter 6 value -3.289665 ## iter 7 value -3.289672 ## iter 8 value -3.289676 ## iter 8 value -3.289676 ## iter 8 value -3.289676 ## final value -3.289676 ## converged ## initial value -3.286464 ## iter 2 value -3.286855 ## iter 3 value -3.286872 ## iter 4 value -3.286874 ## iter 4 value -3.286874 ## iter 4 value -3.286874 ## final value -3.286874 ## converged Figura 6.22: Análisis de residuales para el modelo ARIMA(0,1,1)x(0,1,1)_12 ajustado a la serie lx de pasajeros aéreos ## $fit ## ## Call: ## arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), ## include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, ## REPORT = 1, reltol = tol)) ## ## Coefficients: ## ma1 sma1 ## -0.402 -0.557 ## s.e. 0.090 0.073 ## ## sigma^2 estimated as 0.00135: log likelihood = 244.7, aic = -483.4 ## ## $degrees_of_freedom ## [1] 129 ## ## $ttable ## Estimate SE t.value p.value ## ma1 -0.4018 0.0896 -4.482 0 ## sma1 -0.5569 0.0731 -7.619 0 ## ## $AIC ## [1] -3.69 ## ## $AICc ## [1] -3.689 ## ## $BIC ## [1] -3.624 sarima(lx, 1,1,0, 0,1,1,12) ## initial value -3.085211 ## iter 2 value -3.259459 ## iter 3 value -3.262637 ## iter 4 value -3.275171 ## iter 5 value -3.277007 ## iter 6 value -3.277205 ## iter 7 value -3.277208 ## iter 8 value -3.277209 ## iter 8 value -3.277209 ## iter 8 value -3.277209 ## final value -3.277209 ## converged ## initial value -3.279535 ## iter 2 value -3.279580 ## iter 3 value -3.279586 ## iter 3 value -3.279586 ## iter 3 value -3.279586 ## final value -3.279586 ## converged Figura 6.23: Análisis de residuales para el modelo ARIMA(1,1,0)x(0,1,1)_12 ajustado a la serie lx de pasajeros aéreos ## $fit ## ## Call: ## arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), ## include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, ## REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 sma1 ## -0.340 -0.562 ## s.e. 0.082 0.075 ## ## sigma^2 estimated as 0.00137: log likelihood = 243.7, aic = -481.5 ## ## $degrees_of_freedom ## [1] 129 ## ## $ttable ## Estimate SE t.value p.value ## ar1 -0.3395 0.0822 -4.130 1e-04 ## sma1 -0.5619 0.0748 -7.511 0e+00 ## ## $AIC ## [1] -3.675 ## ## $AICc ## [1] -3.675 ## ## $BIC ## [1] -3.61 Todos los criterios de información prefieren el modelo \\(ARIMA(0,1,1)\\times(0,1,1)_{12}\\). En la Figura 6.22 mostramos los diagnósticos para los residuales y excepto para uno o dos datos atípicos, el modelo parece ajustarse bien. Finalmente, el pronóstico para 12 meses de los datos con la transformación logarítmica lo mostramo en la Figura 6.24. sarima.for(lx, 12, 0,1,1, 0,1,1,12) Figura 6.24: Pronóstico de 12 meses usando el modelo ARIMA(0,1,1)x(0,1,1)_12 de los datos transformados de pasajeros aéreos ## $pred ## Jan Feb Mar Apr May Jun Jul Aug ## 1961 6.110 6.054 6.172 6.199 6.233 6.369 6.507 6.503 ## Sep Oct Nov Dec ## 1961 6.325 6.209 6.063 6.168 ## ## $se ## Jan Feb Mar Apr May Jun ## 1961 0.03672 0.04278 0.04809 0.05287 0.05725 0.06132 ## Jul Aug Sep Oct Nov Dec ## 1961 0.06513 0.06873 0.07216 0.07543 0.07856 0.08157 6.2 Modelos ARCH y GARCH Antes de comenzar, es necesario precisar el concepto de volatilidad en el contexto del análisis financiero. Se denomina volatilidad a la tasa relativa a la que un activo experimenta una drástica disminución o aumento de su precio dentro de un período predeterminado de tiempo. La volatilidad se determina mediante el cálculo de la desviación estándar anualizada de la variación diaria del precio. Si el precio de la acción aumenta y disminuye rápidamente durante cortos períodos de tiempo, entonces se dice que tiene una volatilidad alta. Si el precio se mantiene casi siempre en el mismo valor entonces se dice que tiene volatilidad baja. Los inversores evalúan la volatilidad de las acciones antes de tomar una decisión en, la compra de una oferta de acciones nuevas, la adquisición de acciones adicionales de un activo ya presente en una cartera, o en la venta de acciones que actualmente est án en poder del inversionista. La idea detrás de la comprensión del comportamiento de la volatilidad de los activos es organizar las inversiones para obtener el máximo rendimiento con el mínimo de oportunidades de pérdida. En esta sección se discutirán algunos de los modelos estadísticos y econométricos mas importantes para la modelización de la volatilidad de series de tiempo de rentabilidades de activos. A diferencia del análisis de series de tiempo tradicional, el cual se enfoca principalmente en la modelización del momento condicional de primer orden, los denominados modelos de heterocedasticidad condicional buscan captar la dependencia dentro del momento condicional de segundo orden, en otras palabras, el objetivo ahora es modelizar la volatilidad. La incertidumbre o riesgo constituye uno de los temas de investigación principales en el análisis financiero. Como se mencion´o, la volatilidad es un factor importante en las finanzas puesto que proporciona un método simple para calcular el valor en riesgo de una situación financiera en la gestión de riesgos. Por otra parte, la modelización de la volatilidad de una serie de tiempo puede mejorar la eficiencia en la estimación de parámetros y la exactitud en los intervalos de predicción. En esta sección se discutirán los modelos univariados de la volatilidad entre los que se incluyen el modelo autorregresivo de heterocedasticidad condicional (ARCH) de Engle (1982), el modelo generalizado ARCH (GARCH) de Bollerslev (1986), entre otros. La volatilidad tiene la particularidad de que no es posible su observación directa. Aún cuando esto no es posible, la volatilidad tiene algunas características que pueden ser observadas en las series de rentabilidad de activos entre los que se pueden destacar, Agrupamiento de la volatilidad (cluster). En otras palabras, períodos de volatilidades altas y períodos de volatilidades bajas. Evolución continua de la volatilidad en el tiempo. Las variaciones de la volatilidad se presentan en un rango fijo, es decir, no diverge al infinito. En términos estadísticos, se puede decir que la volatilidad es a menudo estacionaria. La volatilidad parece reaccionar de manera diferente a un incremento elevado de los precios o una disminución sustancial de los precios. Este efecto es conocido con el nombre de apalancamiento o efecto palanca. Tales propiedades descritas anteriormente juegan un papel importante en el desarrollo de los modelos usados para caracterizar la volatilidad. 6.2 Estructura de los Modelos Como se analizó en secciones previas, más precisamente en la sección de modelos lineales, una serie de tiempo \\(x_t\\) se puede escribir como la suma de dos componentes, \\[\\begin{equation} x_t=\\mu_t+w_t = \\mathbb{E}(x_t|\\mathcal{F}_{t-1}) \\tag{6.128} \\end{equation}\\] donde \\(\\mathcl{F}_{t-1}\\) representa la información disponible hasta el tiempo \\(t-1\\). Usualmente, \\(\\mathcal{F}_{t-1}\\) consiste de todas la funciones lineales del pasado de \\(x_t\\). El objetivo de los proceso descritos por (6.128) es la modelización de \\(\\mu_t=\\mathbb{E}(x_t|\\mathcal{F}_{t-1})\\), con la suposición de que \\(w_t\\) sea un ruido blanco condicionalmente homocedástico, es decir, \\[\\begin{equation} \\mathbb{E}(w_t^2) = \\mathbb{E}(w_t^2|\\mathcal{F}_{t-1}) = \\sigma_w^2. \\tag{6.129} \\end{equation}\\] Los modelos de heterocedasticidad condicional suponen que el segundo momento condicional depende del tiempo, es decir, \\[\\begin{equation} \\sigma_t^2=Var(x_t|\\mathcal{F}_{t-1})=\\mathbb{E}((x_t-\\mu_t)^2|\\mathcal{F}_{t-1})=\\mathbb{E}(w_t^2|\\mathcal{F}_{t-1})=h_t, \\tag{6.130} \\end{equation}\\] siendo \\(h_t\\) una función no negtiva, \\(h_t=h_t(\\mathcal{F}_{t-1})\\). A través de este capítulo discutiremos algunas de las posibles funciones para \\(h_t\\). La forma en que \\(h_t\\) evoluciona respecto del tiempo distinguirá una forma de otra. Ya que nuestro objetivo esel estudio de modelos que nos permitan caracterizar series de tiempo financieras, consideraremos de forma general que \\(x_t\\) representa la serie de rentabilidades de activos. Así mismo, haremos referencia de \\(w_t\\) como la rentabilidad corregida en media o impulso del activo. Los modelos de heterocedasticidad condicional los podemos clasificar en dos categorías generales La primera categoría, agrupa los modelos que usan una función exacta que rige la evolución de \\(\\sigma_t^2=h_t\\). La segunda categoría, agrupa los modelos que usan una ecuación estocástica para describir \\(\\sigma_t^2=h_t\\). Los modelos GARCH pertenecen a la primera categoría, mientras que los modelos de volatilidad estocástica están en la segunda categoría. 6.2 Modelos ARCH El primer modelo que proporciona un enfoque sistemático para el modelado de la volatilidad es el modelo Autorregresivo de Heterocedasticidad Condicional denotado por sus sigla en inglés \\(ARCH\\) (Autoregressive Conditional Heteroscedasticity), introducido por Engle (1982). un modelo \\(ARCH(p)\\) tiene la forma \\[\\begin{eqnarray} w_t^2 &amp;=&amp; \\nu_t\\sqrt{h_t} \\nonumber \\\\ h_t &amp;=&amp; \\alpha_0+\\alpha_1w_{t-1}^2+\\cdots+\\alpha_pw_{t-p}^2. \\tag{6.131} \\end{eqnarray}\\] Donde \\(\\{\\nu_t\\}\\) es una sucesión de variables aleatorias iid con media 0 y varianza 1, \\(\\alpha_0&gt;0,\\alpha_p&gt;0\\) y \\(\\alpha_i\\geq0, i=1,\\ldots,p-1\\). La condición de no negatividad sobre los coeficinetes \\(\\alpha_i\\) garantizan que la varianza condicional \\(h_t\\) sea positiva. Nota. Algunos autores usan \\(\\sigma_t^2\\) para denotar la varianza condicional en la ecuación (??) en lugar de \\(h_t\\) tal como lo denotamos. Así pues, el modelo \\(ARCH(p)\\) también lo podemos escirbir de la siguiente manera: \\[\\begin{eqnarray*} w_t &amp;=&amp; \\nu_t\\sigma_t \\\\ \\sigma_t^2 &amp;=&amp; \\alpha_0+\\alpha_1w_{t-1}^2+\\cdots+\\alpha_pw_{t-p}^2. \\end{eqnarray*}\\] Sin embargo, en lo que sigue y por razones prácticas, usareos la primera notación que describimos en la ecuación (??). El modelo \\(ARCH(P)\\) lo podemos escribir como un modelo \\(AR(p)\\) para \\(w_t^2\\). En efecto, \\[\\begin{equation} w_t^2=\\alpha_0+\\alpha_1w_{t-1}^2+\\cdots+\\alpha_pw_{t-p}^2+\\eta_t, \\tag{6.132} \\end{equation}\\] donde \\(\\eta_t=w_t^2-h_t\\). Recordando la teoría de los modelos \\(AR\\), si las raíces de la ecuación característica del proceso \\(AR\\) están fuera del círculo unitario, entonces el proceso es estacionario y además podemos calcular la varianza incondicional de \\(w_t\\), como \\[Var(w_t^2) = \\sigma_w^2 = \\mathbb{E}(w_t^2) = \\frac{\\alpha_0}{1-\\alpha_1-\\cdots-\\alpha_p}\\] siempre y cuando \\(1-\\alpha_1-\\cdots-\\alpha_p&gt;0\\). Tomando en cuenta la ecuación (6.131), podemos ver la razón por la cual los modelos \\(ARCH\\) pueden describir el agrupamiento de la volatilidad. El mmodelo establece que la varianza condicional \\(h_t\\) es una función creciente de \\(w_{t-1}^2\\) para \\(i=1,\\ldots,p\\). Por lo tanto, valores grandes de \\(w_{t-1}\\) (en módulo) dan lugar a valores grandes de \\(h_t\\). Por consiguiente, \\(w_t\\) también tiende a asumirvalores grandes (en módulo). Además de capturar el agrupamiento de la volatilidad, los modelos \\(ARCH\\) tambie’n reflejan el exceso de kurtosis estándar de las series de rentabilidad. Para estudiar esta y otras propiedades, consideraremos por simplicidad el modelo \\(ARCH(1)\\), que asume la forma siguiente: \\[\\begin{eqnarray*} w_t &amp;=&amp; \\nu_t\\sqrt{h_t} \\nonumber\\\\ h_t &amp;=&amp; \\alpha_0+\\alpha_1w_{t-1}^2. \\tag{6.133} \\end{equation} Entonces, tenemos que $$\\mathbb{E}(w_t) = \\mathbb{E}[\\mathbb{E}(w_t|\\mathcal{F}_{t-1})] = \\mathbb{E}(\\sqrt{h_t}\\mathbb{E}(\\nu_t)) = 0.$$ Por otro lado, suponiendo estacionaridad de la serie, la varianza incondicional de $w_t$ es $$\\sigma_w^2=\\mathbb{E}(w_t^2) = \\frac{\\alpha_0}{1-\\alpha_1},$$ con $0\\leq\\alpha_1&lt;1$. Suponiendo normalidad en $\\nu_t$, tenemos $$\\mathbb{E}(w_t^4|\\mathcal{F}_{t-1}) = 3(\\alpha_0+\\alpha_1w_{t-1}^2)^2,$$ y por lo tanto $$\\mathbb{E}(w_t^4) = \\mathbb{E}(\\mathbb{E}(w_t^4|\\mathcal{F}_{t-1})) = 3\\mathbb{E}(\\alpha_0^2+2\\alpha_0\\alpha_1w_{t-1}^2+\\alpha_1^2w_{t-1}^4).$$ Entonces si $w_t$ es estacionario de cuarto orden con $\\mu_4=\\mathbb{E}(w_t^4)$, tenemos que $$\\mu_4 = 3(\\alpha_0^2+2\\alpha_0\\alpha_1Var(w_t)+\\alpha_1^2\\mu_4) = 3\\alpha_0^2\\left(1+2\\frac{\\alpha_1}{1-\\alpha_1}\\right)+3\\alpha_1^2\\mu_4.$$ Despejando, obtenemos $$\\mu_4 = \\frac{3\\alpha_0^2(1+\\alpha_1)}{(1-\\alpha_1)(1-3\\alpha_1^2)}.$$ Con la condición $0\\leq\\alpha_1^2&lt;\\frac{1}{3}$, para asegurar que $\\mu_4&gt;0$. or otra parte, la kurtosis incondicional de $w_t$ es $$k = \\frac{\\mathbb{E}(w_t^4)}{[Var(w_t)]^2} = 3\\frac{1-\\alpha_1^2}{1-3\\alpha_1^2} &gt; 3.$$ En esta última ecuación vemos reflejado el exceso de kurtosis de $w_t$. El modelo $ARCH$ tiene múltiles propiedades que en cierta forma pueden mejorar el modelado de series de tiempo financieras, en epsecial si queremos modeloar o simular la volatilidad.Sin embargo, este modelo como los ya vistos presentan limitaciones a la hora de modelar series de rentabilidad de activos financieros. Es habitual que períodos de rentabilidades negativas sean seguidos por períodos de gran volatilidad. Así, los modelos $ARCH$ no tienen la capacidad de captar esta característica debido a que la volatilidad responde igualmente ante impulsos negativos y positivos, pues dependen del cuadrado de los mismos. Por otro lado, las condiciones para la existencia de momentos de orden mayor, implica colocar restricciones muy estrictas sobre los parámetros del modelo. Como ya mencionamos, para un modelo $ARCH(1)$ con momento de cuarto orden finito exigimos que $0\\leq\\alpha_1^2&lt;1/3$, de modo que para un modelo $ARCH$ de mayor orden las restricciones tienden a complicarse. ### Estimación de un Modelo ARCH(p) ### Predicción con modelos ARCH ## Modelos GARCH ### Estimación de un Modelo GARCH ### Predicción con modelos GARCH &lt;!--chapter:end:306-Modelos-ARCH-GARCH.Rmd--&gt; # Análisis Espectral La representación espectral de un proceso estacionario $x_t$ esencialmente descompone $x_t$ en suma de componentes senosoidales con coeficientes no correlacionados. En relación con esta descomposición existe una correspondiente descomposición en senosoidales de la función de autocovarianza de $x_t$. La descomposición espectral es así una analogía para procesos estocásticos estacionarios de la conocida representación de Fourier para funciones determinísticas. El análisis de procesos estacionarios por medio de su representación espectral es usualmente referido como el análisis en el *dominio de frecuencias* de la serie de tiempo. Este es equivalente al análisis en el *dominio de tiempo* basado en la función de autocovarianza, pero provee una manera alternativa de ver el proceso para el cual en algunas aplicaciones puede ser más significativo. Por ejemplo en el diseño de una estructura sujeta a una fluctuación de carga aleatoria es importante tener cuidado con la presencia en la fuerza de carga de una gran armónica con frecuencia particular para asegurar que la frecuencia en cuestión no sea una frecuencia resonante de la estructura. El punto de vista espectral es particularmente ventajoso en el análisis de procesos estacionarios multivariantes y en el análisis de conjuntos de datos grandes, para los cuales los cálculos numéricos se pueden realizar rápidamente usando la *Transformada Rápida de Fourier (FFT)*. ## Comportamiento Cíclico y Periodicidad Ya hemos visto la noción de periodicidad en varios ejemplos de los capítulos anteriores. La noción general de periodicidad se puede hacer con más precisión introduciendo algunas terminologías. De interés descriptivo es el período de una serie temporal, definido como el número de puntos en un ciclo, es decir, \\begin{equation} T=\\frac{1}{\\omega}. \\tag{6.134} \\end{equation} De manera de definir la tasa de cambio a la cual una serie oscila, primero definiremos un ciclo como un periodo completo de una función seno o de coseno sobre un intervalo de tiempo de longitud $2\\pi$. Consideremos el siguiente proceso periódico \\begin{equation} x_t=A\\cos(2\\pi\\omega t+\\phi) \\tag{6.135} \\end{equation} para $t=0,\\pm1,\\pm2,\\ldots$, donde $\\omega$ es un índice de frecuencias, definida en ciclos por unidad de tiempo con $A$ la altura o **amplitud** de la función y $\\phi$ la **fase** la cual determina el punto de inicio de la función coseno. Podemos introducir una variación aleatoria en esta serie de tiempo haciendo que la *amplitud* o la *fase* varíen aleatoriamente. De esta manera es fácil usar identidad trigonométrica [^nota10] y escribir \\@ref(eq:eq-proceso-periodico) como [^nota10]: $\\cos(\\alpha\\pm\\beta)=\\cos(\\alpha)\\cos(\\beta)\\mp\\sin(\\alpha)\\sin(\\beta)$ \\begin{equation} x_t=U_1\\cos(2\\pi\\omega t)+U_2\\sin(2\\pi\\omega t) \\tag{6.136} \\end{equation} donde $U_1=A\\cos\\phi$ y $U_2=-A\\sin\\phi$ son en general tomados de manera que sean variables aleatorias normalmente distribuidas. En este caso la amplitud es $A=\\sqrt{U_1^2+U_2^2}$ y la fase es $\\phi=\\arctan(-U_2/U_1)$. De este hecho se puede demostrar que si y solo si, en \\@ref(eq:eq-proceso-periodico), $A$ y $\\phi$ son variables aleatorias independientes, donde $A^2$ es una chi-cuadrado con 2 grados de libertad, y $\\phi$ es uniforme en $(-\\pi,\\pi)$, entonces $U_1$ y $U_2$ son variables aleatorias normal estándar independientes. Considere una generalización de \\@ref(eq:eq-proceso-periodico-2) que nos permita mezclas de series periódicas con multiples frecuencias y amplitudes \\begin{equation} x_t=\\sum_{k=1}^{q}\\left[U_{k1}\\cos(2\\pi\\omega_kt)+U_{k2}\\sin(2\\pi\\omega_kt)\\right] \\tag{6.137} \\end{equation} donde $U_{k1},U_{k2}$ para $k=1,2,\\ldots,q$, son variables aleatorias independientes con media cero y varianza $\\sigma_k^2$ y las $\\omega_k$ son distintas frecuencias. Note que \\@ref(eq:eq-proceso-periodico-general) muestra el proceso como una suma de componentes independientes, con varianza $\\sigma_k^2$ para frecuencia $\\omega_k$. Usando la independencia de $Us$ e identidad trigonométrica, es fácil demostrar que la función de autocovarianza del proceso es \\begin{equation} \\gamma(h)=\\sum_{k=1}^{q}\\sigma_k^2\\cos(2\\pi\\omega_kh) \\tag{6.138} \\end{equation} Note que la función de autocovarianza es la suma de componentes periódicas con pesos proporcionales a la varianza $\\sigma_k^2$. Por consiguiente, $x_t$ es un proceso estacionario de media cero con varianza \\begin{equation} \\gamma(0)=\\mathbb{E}(x_t^2)=\\sum_{k=1}^{q}\\sigma_k^2 \\tag{6.139} \\end{equation} que muestra la variación total como la suma de las varianzas de cada una de las componentes. \\BeginKnitrBlock{example}\\iffalse{-91-85-110-97-32-115-101-114-105-101-32-112-101-114-105-243-100-105-99-97-93-}\\fi{}&lt;div class=&quot;example&quot;&gt;&lt;span class=&quot;example&quot; id=&quot;exm:ejem-serie-periodica&quot;&gt;&lt;strong&gt;(\\#exm:ejem-serie-periodica) \\iffalse (Una serie periódica) \\fi{} &lt;/strong&gt;&lt;/span&gt; La Figura \\@ref(fig:fig-componentes-periodicas) muestra un ejemplo de mezcla \\@ref(eq:eq-proceso-periodico-general) con $q=3$ construido de la siguiente manera. Primero para $t=1,\\ldots,100$ generamos tres series \\begin{eqnarray*} x_{t1} &amp;=&amp; 2\\cos(2\\pi t6/100)+3\\sin(2\\pi t6/100) \\\\ x_{t2} &amp;=&amp; 4\\cos(2\\pi t10/100)+5\\sin(2\\pi t10/100) \\\\ x_{t3} &amp;=&amp; 6\\cos(2\\pi t40/100)+7\\sin(2\\pi t40/100) \\end{eqnarray*}\\] Estas tres series se muestran en la Figura 6.25 junto con las correspondientes frecuencias y amplitudes cuadrada. Por ejemplo, la amplitud cuadrada de \\(x_{t1}\\) es \\(2^3+3^2=13\\). Por consiguiente, los valores máximos y mínimos de la serie \\(x_{t1}\\) están restringidos a \\(\\pm\\sqrt{13}=\\pm3.61\\). Finalmente construimos la serie \\[x_t=x_{t1}+x_{t2}+x_{t3}\\] esta serie también se muestra en la Figura 6.25. Note que la serie \\(x_t\\) parece tener el comportamiento de alguna de las series periódicas vistas en los Capítulos Características de series de tiempo y Modelos de series de tiempo. La clasificación sistemática de los componentes esenciales de frecuencia en una serie de tiempo, incluyendo sus contribuciones relativas, constituye uno de los principales objetivos del análisis espectral. x1=2*cos(2*pi*1:100*6/100)+3*sin(2*pi*1:100*6/100) x2=4*cos(2*pi*1:100*10/100)+5*sin(2*pi*1:100*10/100) x3=6*cos(2*pi*1:100*40/100)+7*sin(2*pi*1:100*40/100) xt=x1+x2+x3 par(mfrow=c(2,2)) plot.ts(x1, ylim=c(-10,10), main=expression(omega==6/100~~~A^2==13)) plot.ts(x2, ylim=c(-10,10), main=expression(omega==10/100~~~A^2==41)) plot.ts(x3, ylim=c(-10,10), main=expression(omega==40/100~~~A^2==85)) plot.ts(xt, ylim=c(-10,10), main=&quot;suma&quot;) Figura 6.25: Componentes periódicas y su suma como se describe en el Ejemplo Ejemplo 6.33 (Periodograma escalado para el ejemplo anterior) En el Ejemplo 2.7 introdujimos el periodograma como una manera de descubrir las componentes periódicas de una serie de tiempo. Recuerde que el periodograma escalado está dado por \\[\\begin{equation} P(j/n)=\\left(\\frac{2}{n}\\sum_{t=1}^{n}x_t\\cos(2\\pi tj/n)\\right)^2+\\left(\\frac{2}{n}\\sum_{t=1}^{n}x_t\\sin(2\\pi tj/n)\\right)^2 \\tag{6.140} \\end{equation}\\] y se podia considerar como una medida de la correlación cuadrada de los datos con las oscilaciones senosoidales a frecuencia \\(\\omega_j=j/n\\) o \\(j\\) ciclos en \\(n\\) puntos de tiempo. El periodograma escalado de los datos \\(x_t\\) simulados en el Ejemplo ?? se muestran en la Figura 6.26 y claramente se identifican las tres componentes \\(x_{t1},x_{t2}\\) y \\(x_{t3}\\) de \\(x_t\\). Más aún, los pesos del periodograma escalado mostrados en la Figura 6.26 son \\[P(6/100)=13\\text{, }P(10/100)=41\\text{, }P(40/100)=85\\text{ y } P(j/n)=0 \\text{ en otro caso.}\\] Estos son exactamente las amplitudes al cuadrado de las componentes generadas en el Ejemplo ??. Este resultado sugiere que el periodograma puede proporcionar una idea de la varianza de los componentes, (6.139), de un conjunto real de los datos. Las instrucciones en R para calcular el Periodograma y graficarlo son: P=abs(2*fft(xt)/100)^2 f=0:50/100 plot(f,P[1:51], type=&quot;o&quot;, xlab=&quot;Frecuencia&quot;, ylab=&quot;periodograma&quot;) Figura 6.26: Periodograma de los datos generados en el Ejemplo … Si consideramos los datos \\(x_t\\) en el Ejemplo ?? como un color (forma de onda) hecho con colores primarios \\(x_{t1},x_{t2},x_{t3}\\) en varias intensidades (amplitudes), entonces podemos considerar el periodograma como un prisma que descompone el color \\(x_t\\) en sus colores primarios (espectro). Por consiguiente el término análisis espectral. Otro hecho que podemos usar para entender el concepto de periodograma es que para cada muestra \\(x_1,\\ldots,x_n\\) de una serie temporal donde \\(n\\) es impar, podemos escribir, exactamente \\[\\begin{equation} x_t=a_0 + \\sum_{j=1}^{(n-1)/2}\\left[a_j\\cos(2\\pi tj/n) + b_j\\sin(2\\pi tj/n)\\right], \\tag{6.141} \\end{equation}\\] para \\(t=1,\\ldots,n\\) y coeficientes convenientemente elegidos. Si \\(n\\) es par, la representación () se puede modificar sumando hasta \\((n/2-1)\\) y añadiendo una componente adicional dada por \\(a_{n/2}\\cos(2\\pi t1/2)=a_{n/2}(-1)^t\\). El punto crucial aquí es que (6.141) es exacto para cada muestra. Dado que (6.137) se puede pensar como una aproximación de (6.141), la idea es que muchos de los coeficientes en (6.141) pueden estar cerca de cero. Recuerde del Ejemplo 3.4.5 que \\[\\begin{equation} P(j/n) = a_j^2+b_j^2 \\tag{6.142} \\end{equation}\\] de modo que el periodograma escalado indica cuales componentes periódicas en (6.141) son grandes y cuales componentes son pequeñas. 6.3 La Densidad Espectral La idea de que una serie de tiempo está formada por componentes periódicos, apareciendo en proporción a sus varianzas subyacentes es fundamental en la representación espectral dada por los siguientes Teoremas: Teorema 6.1 Una función \\(\\gamma(h)\\) para \\(h=0,\\pm1,\\pm2,\\dots\\) es Hermitiana no-negativa definida si y solo si se puede expresar como \\[\\begin{equation} \\gamma(h)=\\int_{-1/2}^{1/2}\\exp(2\\pi i\\omega h)dF(\\omega) \\tag{6.143} \\end{equation}\\] donde \\(F(\\cdot)\\) es monótona no-decreciente. La función \\(F(\\cdot)\\) es continua a la derecha, acotada en \\([-1/2,1/2]\\) y únicamente determinada por las condiciones \\(F(-1/2)=0,F(1/2)=\\gamma(0)\\). Demostración. Para demostrar el resultado, note primero que si \\(\\gamma(h)\\) tiene la representación de arriba \\[\\begin{eqnarray*} \\sum_{s=1}^{n}\\sum_{t=1}^{n}\\bar{a}_s\\gamma(s-t)a_t &amp;=&amp; \\int_{-1/2}^{1/2}\\bar{a}_s\\gamma(s-t)a_te^{2\\pi i\\omega(s-t)}dF(\\omega) \\\\ &amp;=&amp; \\int_{-1/2}^{1/2}\\left|\\sum_{s=1}^{n}a_se^{-2\\pi i\\omega s}\\right|^2dF(\\omega) \\\\ &amp;=&amp; \\geq 0 \\end{eqnarray*}\\] y \\(\\gamma(h)\\) es no-negativa definida. Recíprocamente, suponga que \\(\\gamma(h)\\) es una función no-negativa definida, y definamos la función no-negativa \\[\\begin{eqnarray} f_n(\\omega) &amp;=&amp; \\frac{1}{n}\\sum_{s=1}^{n}\\sum_{t=1}^{n}e^{-2\\pi i\\omega s}\\gamma(s-t)e^{2\\pi i\\omega t} \\nonumber\\\\ &amp;=&amp; \\frac{1}{n}\\sum_{u=-(n-1)}^{(n-1)}(n-|u|)e^{-2\\pi i\\omega u}\\gamma(u) \\tag{6.144} \\\\ &amp;=&amp; \\geq 0. \\nonumber \\end{eqnarray}\\] Ahora, sea \\(F_n(\\omega)\\) la función de distribución correspondiente a \\(f_n(\\omega)I_{(-1/2,1/2]}\\) donde \\(I_{(\\cdot)}\\) denota la función indicatriz del intervalo en el subíndice. Note que \\(F_n(\\omega)=0, \\omega\\leq-1/2\\) y \\(F_n(\\omega)=F_n(1/2)\\) para \\(\\omega\\geq1/2\\). Entonces \\[\\begin{eqnarray*} \\int_{-1/2}^{1/2}e^{2\\pi i\\omega u}dF_n(\\omega) &amp;=&amp; \\int_{-1/2}^{1/2}e^{2\\pi i\\omega u}f_n(\\omega)d\\omega \\\\ &amp;=&amp; \\begin{cases} (1-|u|/n)\\gamma(u)&amp;\\text{, }|u|&lt;n\\\\ 0&amp;\\text{, en otro caso} \\end{cases} \\end{eqnarray*}\\] También tenemos \\[\\begin{eqnarray*} F_n(1/2) &amp;=&amp; \\int_{-1/2}^{1/2}f_n(\\omega)d\\omega \\\\ &amp;=&amp; \\int_{-1/2}^{1/2}\\sum_{|u|&lt;n}(1-|u|/n)\\gamma(u)e^{-2\\pi i\\omega u}d\\omega \\\\ &amp;=&amp; \\gamma(0). \\end{eqnarray*}\\] Ahora, por el primer teorema de convergencia de Helly 12, existe una subsucesión \\(F_{n_k}\\) convergente a \\(F\\) y por el lema de Helly-Bray, esto implica que \\[\\int_{-1/2}^{1/2}e^{2\\pi i\\omega u}dF_{n_k}(\\omega)\\to\\int_{-1/2}^{1/2}e^{2\\pi i\\omega u}dF(\\omega)\\] y del lado derecho de la ecuación anterior \\[(1-|u|/n_k)\\gamma(u)\\to\\gamma(u)\\] cuando \\(n_k\\to\\infty\\), y se obtiene el resultado requerido. Ahora presentamos una versión del Teorema de Representación Espectral en términos de un proceso estacionario de media cero \\(x_t\\). Esta versión nos permite pensar en un proceso estacionario como un proceso generado (aproximadamente) por sumas aleatorias de senos y cosenos tal como se describe en (6.137). Teorema 6.2 Si \\(x_t\\) es un proceso estacionario de media cero, con distribución espectral \\(F(\\omega)\\) como la dada en el Teorema 6.1, entonces existe un proceso estocástico a valores complejos \\(z(\\omega)\\) en el intervalo \\(\\omega\\in[-1/2,1/2]\\) con incrementos estacionarios no-correlacionados, tal que \\(x_t\\) se puede escribir como la integral estocástica \\[x_t=\\int_{-1/2}^{1/2}\\exp(-2\\pi it\\omega)dz(\\omega)\\] donde, para \\(-1/2\\leq\\omega_1\\leq\\omega_2\\leq1/2\\) \\[\\text{var}\\{z(\\omega_2)-z(\\omega_1)\\}=F(\\omega_2)-F(\\omega_1).\\] Este resultado es muy técnico porque envuelve integración estocástica; es decir, integración respecto a un proceso estocástico. En términos no técnico, el Teorema 6.2 dice que (6.137) es aproximadamente verdadero para cada serie de tiempo estacionaria. En otras palabras, cada serie de tiempo estacionaria se puede pensar, aproximadamente, como una superposición aleatoria de senos y cosenos oscilando a distintas frecuencias. Dado que (6.137) es aproximadamente cierta para toda serie de tiempo estacionaria, la siguiente pregunta es si una representación significativa para la función de autocovarianza, como la dada por (6.138), también existirá. La respuesta es sí, y su representación es dada por el Teorema 6.1. El siguiente ejemplo, nos ayudará a explicar estos resultados. Ejemplo 6.34 (Un proceso estacionario periódico) Considere un proceso aleatorio estacionario periódico dado por (6.136), con frecuencia fija \\(\\omega_0\\) \\[x_t=U_1\\cos(2\\pi\\omega_0t)+U_2\\sin(2\\pi\\omega_0t)\\] donde \\(U_1\\) y \\(U_2\\) son variables aleatorias independientes de media cero y varianza igual \\(\\sigma^2\\). El número de periodos de tiempo necesario para que la serie de arriba complete un ciclo es exactamente \\(1/\\omega_0\\), y el proceso hace exactamente \\(\\omega_0\\) ciclos por puntos para \\(t=0,\\pm1,\\pm2,\\ldots\\). Es fácil demostrar que 13 \\[\\begin{eqnarray*} \\gamma(h) &amp;=&amp; \\sigma^2\\cos(2\\pi\\omega_0h)=\\frac{\\sigma^2}{2}e^{-2\\pi i\\omega_0h}+\\frac{\\sigma^2}{2}e^{2\\pi i\\omega_0h} \\\\ &amp;=&amp; \\int_{-1/2}^{1/2}e^{2\\pi i\\omega h}dF(\\omega) \\end{eqnarray*}\\] usando la integración de Riemann-Stieltjes, donde \\(F(\\omega)\\) es la función definida por \\[F(\\omega)=\\begin{cases} 0,&amp;\\omega&lt;-\\omega_0\\\\ \\sigma^2/2,&amp;-\\omega_0\\leq\\omega&lt;\\omega_0\\\\ \\sigma^2,&amp;\\omega\\geq\\omega_0 \\end{cases}.\\] La función \\(F(\\omega)\\) se comporta como una función de distribución acumulada para una variable aleatoria discreta, excepto que \\(F(\\infty)=\\sigma^2=\\gamma_x(0)\\) en vez de uno. De hecho, \\(F(\\omega)\\) es una función de distribución acumulada, no una probabilidad, sino más bien de varianza asociada con la frecuencia \\(\\omega_0\\) en un análisis de varianza, siendo \\(F(\\infty)\\) la varianza total del proceso \\(x_t\\). Por lo tanto, llamamos a \\(F(\\omega)\\) la función de distribución espectral. El Teorema 6.1 establece que una representación como la dada en el Ejemplo 6.34 siempre existirá para un proceso estacionario. En particular, si \\(x_t\\) es estacionario con autocovarianza \\(\\gamma(h)=\\mathbb{E}[(x_{t+h}-\\mu)(x_t-\\mu)]\\), entonces existe una única función monótona creciente \\(F(\\omega)\\), llamada la función de distribución espectral, que es acotada, con \\(F(-\\infty)=F(-1/2)=0\\) y \\(F(\\infty)=F(1/2)=\\gamma(0)\\) tal que \\[\\begin{equation} \\gamma(h)=\\int_{-1/2}^{1/2}e^{2\\pi i\\omega h}dF(\\omega). \\tag{6.145} \\end{equation}\\] Una situación más importante que usaremos repetidamente es cubierta por el Teorema 6.3, donde se muestra que, sujeto a la sumabilidad absoluta de la autocovarianza, la función de distribución espectral es absolutamente continua con \\(dF(\\omega)=f(\\omega)d\\omega\\) y la representación (6.145) motiva la propiedad que sigue. Teorema 6.3 Si \\(\\gamma(h)\\) es la función de autocovarianza de un proceso estacionario \\(x_t\\) con \\[\\begin{equation} \\sum_{h=-\\infty}^{\\infty}|\\gamma(h)|&lt;\\infty \\tag{6.146} \\end{equation}\\] entonces la densidad espectral de \\(x_t\\) está dada por \\[\\begin{equation} f(\\omega)=\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-2\\pi i\\omega h}. \\tag{6.147} \\end{equation}\\] Proposición 6.10 (La Densidad Espectral) Si la función de autocovarianza \\(\\gamma(h)\\) de un proceso estacionario satisface \\[\\begin{equation} \\sum_{h=-\\infty}^{\\infty}|\\gamma(h)|&lt;\\infty \\tag{6.148} \\end{equation}\\] entonces esta tiene representación espectral \\[\\begin{equation} \\gamma(h)=\\int_{-1/2}^{1/2}e^{2\\pi i\\omega h}f(\\omega)d\\omega\\text{ para }h=0,\\pm1,\\pm2,\\ldots \\tag{6.149} \\end{equation}\\] como la transformación inversa de la densidad espectral, la cual tiene la representación \\[\\begin{equation} f(\\omega)=\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-2\\pi i\\omega h}\\text{ con }-1/2\\leq\\omega\\leq1/2. \\tag{6.150} \\end{equation}\\] La densidad espectral definida en la Proposición 6.10 es análoga a la función de densidad de probabilidad; el hecho de que \\(\\gamma(h)\\) es no negativa definida asegura que \\[f(\\omega)\\geq0\\] para todo \\(\\omega\\). Se sigue inmediatamente de (6.149) y (6.150) que \\[f(\\omega)=f(-\\omega)\\] y \\[f(\\omega+1)=f(\\omega)\\] verificando que la densidad espectral es una función par de periodo uno. Debido a que \\(f(\\omega)\\) es una función par, normalmente se graficará solo \\(f(\\omega)\\) para \\(\\omega\\geq0\\). Adicionalmente, haciendo \\(h=0\\) en (6.149) se obtiene \\[\\gamma(0)=\\text{var}(x_t)=\\int_{-1/2}^{1/2}f(\\omega)d\\omega\\] lo cual expresa la varianza total como la integral de la densidad espectral sobre todas las frecuencias. Demostraremos luego, que un filtro lineal puede aislar la varianza en ciertos intervalos de frecuencias o bandas. Análogo a la teoría de probabilidades, \\(\\gamma(h)\\) en (6.149) es la función característica de la densidad espectral \\(f(\\omega)\\) en (6.150). Estos hechos deben dejar claro que, cuando la condición de la Proposición 6.10 es satisfecha, la función de autocovarianza \\(\\gamma(h)\\) y la función de densidad espectral \\(f(\\omega)\\) contienen la misma información. Esta información, sin embargo, es expresada de distintas maneras. La función de autocovarianza expresa la información en términos de pasos o saltos, mientras que la densidad espectral expresa la misma información en término de ciclos. Algunos de los problemas son más fáciles de trabajar cuando consideramos la información de pasos o saltos y tendemos a manejar los problemas en el dominio del tiempo. Sin embargo, otros problemas son más fáciles de trabajar teniendo en cuenta la información periódica y tendemos a manejar los problemas en el dominio espectral o de frecuencias. También debemos mencionar, que hasta ahora nos hemos enfocado en la frecuencia \\(\\omega\\) expresada en ciclos por puntos de tiempo, en lugar de la más común (en estadística) alternativa \\(\\lambda=2\\pi\\omega\\) que nos da radianes por puntos. Finalmente, la condición de sumabilidad absoluta (6.148) no es satisfecha por (6.138), el ejemplo que introdujimos para dar las ideas de representación espectral. La condición, sin embargo, es satisfecha para modelos ARMA. Note que la función de autocovarianza \\(\\gamma(h)\\) en (6.149) y la densidad espectral \\(f(\\omega)\\) en (6.150) son pares de transformadas de Fourier. En general, tenemos la siguiente definición: Definición 6.11 Para una función general \\(\\{a_t;t=0,\\pm1,\\pm2,\\ldots\\}\\) que satisface la condición de sumabilidad absoluta \\[\\begin{equation} \\sum_{t=-\\infty}^{\\infty}|a_t|&lt;\\infty, \\tag{6.151} \\end{equation}\\] definimos el par de transformadas de Fourier de la forma \\[\\begin{equation}\\label{} A(\\omega)=\\sum_{t=-\\infty}^{\\infty}a_te^{-2\\pi i\\omega t} \\tag{6.152} \\end{equation}\\] y \\[\\begin{equation}\\label{} a_t=\\int_{-1/2}^{1/2}A(\\omega)e^{2\\pi i\\omega t}d\\omega \\tag{6.153} \\end{equation}\\] El uso de (6.149) y (6.150) como par de transformadas de Fourier es fundamental en el estudio de procesos estacionarios a tiempo discreto. Bajo la condición de sumabilidad, el par de transformadas de Fourier (6.149) y (6.150) existirá y esta relación es única. Si \\(f(\\omega)\\) y \\(g(\\omega)\\) son dos densidades espectrales para lo cual \\[\\begin{equation} \\int_{-1/2}^{1/2}f(\\omega)e^{2\\pi i\\omega h}d\\omega=\\int_{-1/2}^{1/2}g(\\omega)e^{2\\pi i\\omega h}d\\omega \\tag{6.154} \\end{equation}\\] para todo \\(h=0,\\pm1,\\pm2,\\ldots\\), entonces \\[\\begin{equation} f(\\omega)=g(\\omega) \\tag{6.155} \\end{equation}\\] casi siempre. Ejemplo 6.35 (Serie de ruido blanco) Como un ejemplo sencillo, consideremos el espectro de potencias teórica de una sucesión de variables aleatorias no correlacionadas \\(w_t\\) con varianza \\(\\sigma_w^2\\). Dado que la función de autocovarianza fue calculada en (??) como \\(\\gamma_w(h)=\\sigma_w^2\\) para \\(h=0\\) y cero en cualquier otro caso, se sigue de (6.150) que \\[f_w(\\omega)=\\sigma_w^2\\] para \\(-1/2\\leq\\omega\\leq1/2\\) con la misma potencia para todas las frecuencias. Esta propiedad se ve en la realización, el cual parece contener todas las diferentes frecuencias en proporciones similares. La figura 6.27 (parte superior) muestra la gráfica del espectro de un ruido blanco con \\(\\sigma_w^2=1\\). Ejemplo 6.36 (Un promedio móvil simple) Una serie que no tiene una proporción igual de frecuencias es la serie de ruido blanco suavizada que se muestra en la parte inferior de la primera Figura del Ejemplo 3.4. Específicamente construimos una serie de promedio móvil de tres puntos definida por \\[v_t=\\frac{1}{3}(w_{t-1}+w_t+w_{t+1}).\\] Es claro de la realización del ejemplo que la serie tiene menos frecuencias altas, calculando su espectro de potencias se verifica este hecho. En el Ejemplo 3.6 calculamos su función de autocovarianza, obteniendo \\[\\gamma_v(h)=\\frac{\\sigma_w^2}{9}(3-|h|)\\] para \\(|h|\\leq2\\) y \\(\\gamma_v(h)=0\\) para \\(|h|&gt;2\\). Entonces, usando (6.150) nos da \\[\\begin{eqnarray*} f_v(\\omega) &amp;=&amp; \\sum_{h=-2}^{2}\\gamma_v(h)e^{-2\\pi i\\omega h} \\\\ &amp;=&amp; \\frac{\\sigma_w^2}{9}(e^{-4\\pi i\\omega}+e^{4\\pi i\\omega})+\\frac{2\\sigma_w^2}{9}(e^{-2\\pi i\\omega}+e^{2\\pi\\omega})+\\frac{3\\sigma_w^2}{9} \\\\ &amp;=&amp; \\frac{\\sigma_w^2}{9}[3+4\\cos(2\\pi\\omega)+2\\cos(4\\pi\\omega)] \\end{eqnarray*}\\] Graficando el espectro para \\(\\sigma_w^2=1\\), como en la Figura 6.27, se muestra que las frecuencias cercanas a cero tiene mayor potencia y las energías más grandes, \\(\\omega&gt;0.2\\) tienen menor potencia. Ejemplo 6.37 (Una serie autoregresiva de segundo orden) Consideremos el espectro de una serie AR(2) de la forma \\[x_t-\\phi_1x_{t-1}-\\phi_2x_{t-2}=w_t\\] para el caso especial \\(\\phi_1=1\\) y \\(\\phi_2=-0.9\\). Recuerde el Ejemplo 3.6 el cual muestra una realización de este proceso con \\(\\sigma_w^2=1\\). Note que los datos exhiben una fuerte componente periódica de un ciclo cada seis puntos. Primero, calculemos la función de autocovarianza del lado derecho e igualemos este a la autocovarianza de la parte izquierda \\[\\begin{eqnarray*} \\gamma_w(h) &amp;=&amp; \\mathbb{E}[(x_{t+h}-\\phi_1x_{t+h-1}-\\phi_2x_{t+h-2})(x_t-\\phi_1x_{t-1}-\\phi_2x_{t-2})] \\\\ &amp;=&amp; [1+\\phi_1^2+\\phi_2^2]\\gamma_x(h)+(\\phi_1\\phi_2-\\phi_1)[\\gamma_x(h+1)+\\gamma_x(h-1)]-\\phi_2[\\gamma_x(h+2)+\\gamma_x(h-2)] \\\\ &amp;=&amp; 2.81\\gamma_x(h)-1.9[\\gamma_x(h+1)+\\gamma_x(h-1)]+0.9[\\gamma_x(h+2)+\\gamma_x(h-2)], \\end{eqnarray*}\\] hemos sustituido los valores de \\(\\phi_1=1\\) y \\(\\phi_2=-0.9\\) en la ecuación. Ahora, sustituyendo la representación espectral para \\(\\gamma_x(h)\\) en la ecuación anterior, se tiene \\[\\begin{eqnarray*} \\gamma_w(h) &amp;=&amp; \\int_{-1/2}^{1/2}[2.81-1.90(e^{2\\pi i\\omega}+e^{-2\\pi i\\omega})+0.90(e^{4\\pi i\\omega}+e^{-4\\pi i\\omega})]e^{2\\pi i\\omega h}f_x(\\omega)d\\omega \\\\ &amp;=&amp; \\int_{-1/2}^{1/2}[2.81-3.80\\cos(2\\pi\\omega)+1.80\\cos(4\\pi\\omega)]e^{2\\pi i\\omega h}f_x(\\omega)d\\omega. \\end{eqnarray*}\\] Si el espectro del proceso de ruido blanco es \\(g_w(\\omega)\\), la unicidad de la transformada de Fourier nos permite identificar \\[g_w(\\omega)=[2.81-3.80\\cos(2\\pi\\omega)+1.80\\cos(4\\pi\\omega)]f_x(\\omega).\\] Pero, como ya hemos visto, \\(g_w(\\omega)=\\sigma_w^2\\) de donde se deduce que \\[f_x(\\omega)=\\frac{\\sigma_w^2}{2.81-3.80\\cos(2\\pi\\omega)+1.80\\cos(4\\pi\\omega)}\\] es el espectro de la serie autoregresiva. Haciendo \\(\\sigma_w^2=1\\) se tiene el espectro \\(f_x(\\omega)\\) mostrado en la Figura 6.27, y donde muestra una componente de potencia fuerte alrededor de \\(\\omega=0.16\\) ciclos por puntos o un periodo entre seis y siete ciclos por puntos y potencias muy pequeñas en las otras frecuencias. En este caso, modificando la serie de ruido blanco aplicando un operador AR de orden dos ha concentrado la potencia o varianza de la serie resultante en una banda de frecuencia bastante estrecha. n=100 sigma2=1 w=seq(0,0.5,length=n) # Calculo de las densidades espectrales fw=numeric(n) fv=numeric(n) fx=numeric(n) for (i in 1:n){ fw[i]=sigma2 fv[i]=(sigma2/9)*(3+4*cos(2*pi*w[i])+2*cos(4*pi*w[i])) fx[i]=sigma2/(2.81-3.80*cos(2*pi*w[i])+1.80*cos(4*pi*w[i])) } # Graficos par(mfrow=c(3,1)) plot(w,fw,type=&quot;l&quot;, main=&quot;Ruido blanco&quot;, ylab=&quot;Potencia&quot;) plot(w,fv,type=&quot;l&quot;, main=&quot;Promedio movil del ruido blanco&quot;, ylab=&quot;Potencia&quot;) plot(w,fx,type=&quot;l&quot;, main=&quot;AR(2)&quot;, ylab=&quot;Potencia&quot;, xlab=&quot;Frecuencia&quot;) Figura 6.27: Espectros teóricos de un ruido blanco (superior), promedio móvil de ruido blanco (medio) y proceso AR(2) (inferior) Los ejemplos anteriores han sido dados para motivar el uso de los espectros de potencias para describir las fluctuaciones de la varianza teórica de una serie estacionaria. Es más, la interpretación de la función de densidad espectral como la varianza de la serie de tiempo sobre una banda de frecuencia dada nos da una explicación intuitiva del significado físico. La gráfica de la función \\(f(\\omega)\\)sobre el argumento de frecuencia \\(\\omega\\) puede ser pensado como un análisis de varianza, en el cual las columnas o bloques efectivos son las frecuencias indexadas por \\(\\omega\\). 6.4 Periodograma y Transformada Discreta de Fourier Ahora estamos listos para unir el periodograma, que es el concepto basada en la muestra presentado en la sección [Comportamiento Cíclico y Periodicidad], con la densidad espectral, que es el concepto basado en la población descrito en la sección La Densidad Espectral. Definición 6.12 Dado los datos \\(x_1,x_2,\\ldots,x_n\\), definimos la Transformada Discreta de Fourier (TDF) como \\[\\begin{equation} d(\\omega_j)=n^{-1/2}\\sum_{t=1}^{n}x_te^{-2\\pi i\\omega_jt} \\tag{6.156} \\end{equation}\\] para \\(j=0,1,\\ldots,n-1\\), donde las frecuencias \\(\\omega_j=j/n\\) son llamadas las frecuencias de Fourier o frecuencias fundamentales. Si \\(n\\) es un número altamente compuesto (i.e., tiene muchos factores), la TDF se puede calcular usando la Transformada Rápida de Fourier (FFT). A veces es útil explotar el resultado de inversión para TDF que muestra que la transformación lineal es de uno a uno. Para la inversa de TDF, tenemos \\[\\begin{equation} x_t=n^{-1/2}\\sum_{j=0}^{n-1}d(\\omega_j)e^{2\\pi i\\omega_jt} \\tag{6.157} \\end{equation}\\] para \\(t=1,2,\\ldots,n\\). Definición 6.13 Dados los datos \\(x_1,x_2,\\ldots,x_n\\) definimos el periodograma como \\[\\begin{equation} I(\\omega_j)=|d(\\omega_j)|^2 \\tag{6.158} \\end{equation}\\] para \\(j=0,1,2,\\ldots,n-1\\). Note que \\(I(0)=n\\bar{x}^2\\), donde \\(\\bar{x}\\) es la media muestral. Además, dado que \\(\\sum_{t=1}^{n}\\exp(-2\\pi i\\omega_jt)=0\\) para \\(j\\neq0\\), 14 podemos escribir la TDF como \\[\\begin{equation} d(\\omega_j)=n^{-1/2}\\sum_{t=1}^{n}(x_t-\\bar{x})e^{-2\\pi i\\omega_jt} \\tag{6.159} \\end{equation}\\] para \\(j\\neq0\\). Entonces, para \\(j\\neq0\\), \\[\\begin{eqnarray} I(\\omega_j)=|d(\\omega_j)|^2 &amp;=&amp; n^{-1}\\sum_{t=1}^{n}\\sum_{s=1}^{n}(x_t-\\bar{x})(x_s-\\bar{x})e^{-2\\pi i\\omega_j(t-s)} \\nonumber\\\\ &amp;=&amp; n^{-1}\\sum_{t=1}^{n}\\sum_{s=1}^{n}(x_{t+|h|}-\\bar{x})(x_t-\\bar{x})e^{-2\\pi i\\omega_jh} \\nonumber \\\\ &amp;=&amp; \\sum_{h=-(n-1)}^{n-1}\\hat{\\gamma}(h)e^{-2\\pi i\\omega_jh} \\tag{6.160} \\end{eqnarray}\\] donde hemos hecho \\(h=t-s\\) con \\(\\hat{\\gamma}(h)\\) 15. Recuerde que \\(P(\\omega_j)=(4/n)I(\\omega_j)\\) donde \\(P(\\omega_j)\\) es el periodograma escalado definido en (6.140). Por consiguiente, trabajaremos con \\(I(\\omega_j)\\) en vez de \\(P(\\omega_j)\\). A veces es útil trabajar con las partes real e imaginarias de la TDF individualmente, de donde tenemos la siguiente definición: Definición 6.14 Dados las datos \\(x_1,x_2,\\ldots,x_n\\) definimos la transformada de cosenos como \\[\\begin{equation} d_c(\\omega_j)=n^{-1/2}\\sum_{t=1}^{n}x_t\\cos(2\\pi\\omega_jt) \\tag{6.161} \\end{equation}\\] y la transformada de senos como \\[\\begin{equation} d_s(\\omega_j)=n^{-1/2}\\sum_{t=1}^{n}x_t\\sin(2\\pi\\omega_jt) \\tag{6.162} \\end{equation}\\] donde \\(\\omega_j=j/n\\) para \\(j=0,1,2,\\ldots,n-1\\). Note que \\(d(\\omega_j)=d_c(\\omega_j)-id_s(\\omega_j)\\) y por lo tanto \\[\\begin{equation} I(\\omega_j)=d_c^2(\\omega_j)+d_s^2(\\omega_j) \\tag{6.163} \\end{equation}\\] Ejemplo 6.38 (ANOVA espectral) Sea \\(x_1,x_2,\\ldots,x_n\\) una muestra de tamaño \\(n\\), donde para simplificar \\(n\\) es impar. Entonces, recordando el Ejemplo 6.33, se tiene \\[\\begin{equation} x_t=a_0+\\sum_{j=1}^{m}[a_j\\cos(2\\pi\\omega_jt)+b_j\\sin(2\\pi\\omega_jt)] \\tag{6.164} \\end{equation}\\] donde \\(m=(n-1)/2\\) es exacto para \\(t=1,2,\\ldots,n\\). En particular, usando la fórmula de regresión multiples, tenemos \\(a_0=\\bar{x}\\) \\[\\begin{eqnarray*} a_j &amp;=&amp; \\frac{2}{n}\\sum_{t=1}^{n}x_t\\cos(2\\pi\\omega_jt)=\\frac{2}{\\sqrt{n}}d_c(\\omega_j) \\\\ b_j &amp;=&amp; \\frac{2}{n}\\sum_{t=1}^{n}x_t\\sin(2\\pi\\omega_jt)=\\frac{2}{\\sqrt{n}}d_s(\\omega_j) \\end{eqnarray*}\\] Por consiguiente, podemos escribir \\[(x_t\\bar{x})=\\frac{2}{\\sqrt{n}}\\sum_{j=1}^{m}[d_c(\\omega_j)\\cos(2\\pi\\omega_jt)+d_s(\\omega_j)\\sin(2\\pi\\omega_jt)]\\] para \\(t=1,2,\\ldots,n\\). Elevando al cuadrado ambos miembros y sumando tenemos 16 \\[\\sum_{t=1}^{n}(x_t-\\bar{x})^2=2\\sum_{j=1}^{m}\\left[d_c^2(\\omega_j)+d_s^2(\\omega_j)\\right]=2\\sum_{j=1}^{m}I(\\omega_j)\\] En consecuencia, hemos particionado la suma de cuadrados en componentes armónicas representadas por las frecuencias \\(\\omega_j\\) con el periodograma \\(I(\\omega_j)\\) siendo la regresión cuadrada media. Esto nos lleva a la tabla ANOVA Fuente g.l. SC MS \\(\\omega_1\\) 2 \\(2I(\\omega_1)\\) \\(I(\\omega_1)\\) \\(\\omega_2\\) 2 \\(2I(\\omega_2)\\) \\(I(\\omega_2)\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\omega_m\\) 2 \\(2I(\\omega_m)\\) \\(I(\\omega_m)\\) Total \\(n-1\\) \\(\\sum_{t=1}^{n}(x_t-\\bar{x})^2\\) Esta descomposición significa que si los datos contienen alguna componente periódica fuerte, entonces los valores del periodograma correspondientes a estas frecuencias (o cercano a estas frecuencias) serán grandes. Por otra parte, los valores del periodograma serán pequeños para componentes periódicas no presentes en los datos. Ahora estamos listos para presentar algunas propiedades de muestras grandes del periodograma. Primero, sea \\(\\mu\\) la media de un proceso estacionario \\(x_t\\) con función de autocovarianza absolutamente sumable \\(\\gamma(h)\\) y densidad espectral \\(f(\\omega)\\). Podemos usar el mismo argumento como en (6.160) reemplazando \\(\\bar{x}\\) por \\(\\mu\\) en (6.159) para escribir \\[\\begin{equation} I(\\omega_j)=n^{-1}\\sum_{h=-(n-1)}^{n-1}\\sum_{t=1}^{n-|h|}(x_{t+|h|}-\\mu)(x_t-\\mu)e^{-2\\pi i\\omega_jh} \\tag{6.165} \\end{equation}\\] donde \\(\\omega_j\\) es una frecuencia fundamental no cero. Tomando esperanza en (6.165) obtenemos \\[\\begin{equation} \\mathbb{E}[I(\\omega_j)]=\\sum_{h=-(n-1)}^{n-1}\\left(\\frac{n-|h|}{n}\\right)\\gamma(h)e^{-2\\pi i\\omega_jh}. \\tag{6.166} \\end{equation}\\] Para cada \\(\\omega\\neq0\\) dado, elegimos una frecuencia fundamental \\(\\omega_{j:n}\\to\\omega\\) cuando \\(n\\to\\infty\\) 17 de lo cual se sigue por (6.166) que \\[\\begin{equation} \\mathbb{E}(I(\\omega_{j:n})]\\to f(\\omega)=\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-2\\pi ih\\omega} \\tag{6.167} \\end{equation}\\] cuando \\(n\\to\\infty\\). 18 En otras palabras, bajo la sumabilidad absoluta de \\(\\gamma(h)\\), la densidad espectral es la media a largo plazo del periodograma. Para examinar la distribución asintótica del periodograma, note que si \\(x_t\\) es una serie de tiempo normal, las transformadas de senos y cosenos serán conjuntamente normal, porque sus combinaciones lineales son variables aleatorias conjuntamente normal \\(x_1,x_2,\\ldots,x_n\\). En este caso, la suposición de que la función de covarianza satisface la condición \\[\\begin{equation} \\theta=\\sum_{h=-\\infty}^{\\infty}|h||\\gamma(h)|&lt;\\infty \\tag{6.168} \\end{equation}\\] es suficiente para obtener aproximaciones de muestras grandes simples de la varianza y la covarianza. Usando el mismo argumento para desarrollar (6.166) tenemos \\[\\begin{eqnarray} \\text{cov}[d_c(\\omega_j),d_c(\\omega_k)] &amp;=&amp; n^{-1}\\sum_{s=1}^{n}\\sum_{t=1}^{n}\\gamma(s-t)\\cos(2\\pi\\omega_js)\\cos(2\\pi\\omega_kt) \\tag{6.169} \\\\ \\text{cov}[d_c(\\omega_j),d_s(\\omega_k)] &amp;=&amp; n^{-1}\\sum_{s=1}^{n}\\sum_{t=1}^{n}\\gamma(s-t)\\cos(2\\pi\\omega_js)\\sin(2\\pi\\omega_kt) \\tag{6.170} \\\\ \\text{cov}[d_s(\\omega_j),d_s(\\omega_k)] &amp;=&amp; n^{-1}\\sum_{s=1}^{n}\\sum_{t=1}^{n}\\gamma(s-t)\\sin(2\\pi\\omega_js)\\sin(2\\pi\\omega_kt) \\tag{6.171} \\end{eqnarray}\\] donde los términos de la varianza se obtienen haciendo \\(\\omega_j=\\omega_k\\) en (6.169) y (6.171). Se puede demostrar que los términos en (6.169) y (6.171) tienen propiedades interesantes bajo la suposición (6.168), por ejemplo, para \\(\\omega_j,\\omega_k\\neq0\\) o 1/2. \\[\\begin{eqnarray} \\text{cov}[d_c(\\omega_j),d_c(\\omega_k)] &amp;=&amp; \\begin{cases}f(\\omega_j)/2+\\epsilon_n, &amp; \\omega_j=\\omega_k\\\\ \\epsilon_n,&amp; \\omega_j\\neq\\omega_k \\end{cases} \\tag{6.172} \\\\ \\text{cov}[d_s(\\omega_j),d_s(\\omega_k)] &amp;=&amp; \\begin{cases}f(\\omega_j)/2+\\epsilon_n, &amp; \\omega_j=\\omega_k\\\\ \\epsilon_n,&amp; \\omega_j\\neq\\omega_k \\end{cases} \\tag{6.173} \\end{eqnarray}\\] y \\[\\begin{equation} \\text{cov}[d_c(\\omega_j),d_s(\\omega_k)] = \\epsilon_n \\tag{6.174} \\end{equation}\\] donde el término de error \\(\\epsilon_n\\) en la aproximación se puede acotar por \\[\\begin{equation} |\\epsilon_n|\\leq\\theta/n \\tag{6.175} \\end{equation}\\] y \\(\\theta\\) está dado por (6.168). Si \\(\\omega_j=\\omega_k=0\\) o 1/2 en (6.172) el múltiplo 1/2 desaparece; note que \\(d_s(0)=d_s(1/2)=0\\), de modo que (6.173) no aplica. Ejemplo 6.39 (Covarianzas de senos y cosenos para un proceso MA) Para la serie de promedio móvil de tres puntos del Ejemplo 6.36, el espectro teórico se mostraba en la Figura 6.27. Para \\(n=256\\) puntos, la matriz de covarianza teórica del vector \\[\\textbf{d}=(d_c(\\omega_{26}),d_s(\\omega_{26}),d_c(\\omega_{27}),d_s(\\omega_{27}))^t\\] es \\[\\text{cov}(\\textbf{d})=\\left( \\begin{array}{cccc} 0.3752 &amp; -0.0009 &amp; -0.0022 &amp; -0.0010 \\\\ -0.0009 &amp; 0.3777 &amp; -0.0009 &amp; 0.0003 \\\\ -0.0022 &amp; -0.0009 &amp; 0.3667 &amp; -0.0010 \\\\ -0.0010 &amp; 0.0003 &amp; -0.0010 &amp; 0.3692 \\\\ \\end{array} \\right)\\] Los elementos de la diagonal se pueden comparar con los valores del espectro teórico de 0,7548 para el espectro en frecuencia \\(\\omega_{26}=0.102\\) y de 0,7378 para el espectro en \\(\\omega_{27}=0.105\\). Por consiguiente, las transformadas de senos y cosenos produce variables casi no correlacionadas con varianzas aproximadamente igual a un medio del espectro teórico. Para este caso particular, la cota uniforme es determinada por \\(\\theta=8/9\\) obteniéndose \\(|\\epsilon_{256}|\\leq0.0035\\) para la cota del error de aproximación. Si \\(x_t\\sim\\text{iid}(0,\\sigma^2)\\), entonces se sigue de (6.168) a (6.174) y del Teorema Central del Límite 19 que \\[\\begin{equation} d_c(\\omega_{j:n})\\sim AN(0,\\sigma^2/2)\\text{ y }d_s(\\omega_{j:n})\\sim AN(0,\\sigma^2/2) \\tag{6.176} \\end{equation}\\] conjunta e independientemente, e independiente de \\(d_c(\\omega_{k:n})\\) y \\(d_s(\\omega_{k:n})\\) siempre que \\(\\omega_{j:n}\\to\\omega_1\\) y \\(\\omega_{k:n}\\to\\omega_2\\) donde \\(0&lt;\\omega_1\\neq\\omega_2&lt;1/2\\). Note que en este caso \\(f(\\omega)=\\sigma^2\\). En vista de (6.176) se sigue inmediatamente que cuando \\(n\\to\\infty\\) \\[\\begin{equation} \\frac{2I(\\omega_{j:n})}{\\sigma^2}\\overset{d}{\\to}\\chi_2^2\\text{ y }\\frac{2I(\\omega_{k:n})}{\\sigma^2}\\overset{d}{\\to}\\chi_2^2 \\tag{6.177} \\end{equation}\\] con \\(I(\\omega_{j:n})\\) e \\(I(\\omega_{k:n})\\) siendo asintóticamente independientes, donde \\(\\chi^2_{\\nu}\\) denota una variable aleatoria chi-cuadrado con \\(\\nu\\) grados de libertad. Usando el Teorema Central del Límite es bastante fácil extender los resultados del caso iid al caso de procesos lineales. Proposición 6.11 (Distribución de las Ordenadas de un Periodograma) Si \\[\\begin{equation} x_t=\\sum_{j=-\\infty}^{\\infty}\\psi_jw_{t-j}\\text{, }\\sum_{j=-\\infty}^{\\infty}|\\psi_j|&lt;\\infty \\tag{6.178} \\end{equation}\\] donde \\(w_t\\sim\\text{iid}(0,\\sigma_w^2)\\) y (6.168) vale, entonces para cada sucesión de \\(m\\) frecuencias distintas \\(\\omega_j\\) con \\(\\omega_{j:n}\\to\\omega_j\\) \\[\\begin{equation} \\frac{2I(\\omega_{j:n})}{f(\\omega_j)}\\overset{d}{\\to}\\text{iid}\\chi^2_2 \\tag{6.179} \\end{equation}\\] siempre que \\(f(\\omega_j)&gt;0\\) para \\(j=1,2,\\ldots,m\\). La distribución resultante en (6.179) se puede usar para obtener un intervalo de confianza aproximado para el espectro en la manera usual. Sea \\(\\chi^2_{\\nu}(\\alpha)\\) la probabilidad \\(\\alpha\\) de cola inferior para la distribución chi-cuadrado con \\(\\nu\\) grados de libertad, esto es, \\[\\begin{equation} P\\{\\chi^2_{\\nu}\\leq\\chi^2_{\\nu}(\\alpha)\\}=\\alpha. \\tag{6.180} \\end{equation}\\] Entonces, un intervalo de confianza aproximado del \\(100(1-\\alpha)\\%\\) para la función de densidad espectral es de la forma \\[\\begin{equation} \\frac{2I(\\omega_{j:n})}{\\chi^2_2(1-\\alpha/2)}\\leq f(\\omega)\\leq\\frac{2I(\\omega_{j:n})}{\\chi^2_2(\\alpha/2)} \\tag{6.181} \\end{equation}\\] Ejemplo 6.40 (Periodograma de SOI y serie de reclutamiento (nuevos peces)) La Figura ?? muestra el periodograma de las series SOI y nuevos peces. Note que \\(\\chi^2_2(0.025)=0.0506\\) y \\(\\chi^2_2(0.975)=7.3778\\), de allí podemos obtener un intervalo de confianza aproximado del 95% para las frecuencias de interés, en este caso \\(\\omega_j=1/12\\). Para este valor, se tiene \\(I_S(1/12)=2.6084\\), luego un intervalo de confianza aproximado del 95% para el espectro \\(f_S(1/12)\\) es \\[[2(2.6084)/7.3778; 2(2.6084)/0.0506]=[0.7071;103.0254]\\] lo cual es muy amplio para que sea de utilidad, sin embargo ese valor es mayor que cualquier otro valor de la ordenada del periodograma, así podemos decir que este valor es significativo. Por otra parte un intervalo de confianza aproximado del 95% para la otra frecuencia de interés (\\(\\omega_j=1/48\\)) para \\(f_S(1/48)\\) es de la forma \\[[2(0.3804)/7.3778; 2(0.3804)/0.0506]=[0.1031; 15.0355]\\] el cual también es bastante amplio, pero en este caso no es posible establecer una significancia para el pico espectral. Los comandos en R para calcular los periodogramas y generar los gráficos son los siguientes: #soi=scan(&#39;data/soi.txt&#39;) #rec=scan(&#39;data/recruit.txt&#39;) #par(mfrow=c(2,1)) #soi.per=spec.pgram(soi,taper=0,log=&#39;no&#39;) #abline(v=1/12,lty=&#39;dotted&#39;) #abline(v=1/48,lty=&#39;dotted&#39;) #rec.per=spec.pgram(rec,taper=0,log=&#39;no&#39;) #abline(v=1/12,lty=&#39;dotted&#39;) #abline(v=1/48,lty=&#39;dotted&#39;) Los intervalos de confianza de la serie SOI para el ciclo anual \\(w=1/12=40/480\\) y los posibles ciclos de cuatro años de El Niño con \\(w=1/48=10/480\\) se pueden calcular en Matlab y R con los siguientes comandos: #li=qchisq(0.975,2) #ls=qchisq(0.025,2) #2*soi.per$spec[10]/li #2*soi.per$spec[10]/ls #2*soi.per$spec[40]/li #2*soi.per$spec[40]/ls 6.5 Estimación Espectral No-paramétrica Definamos una banda de frecuencia \\(\\mathcal{B}\\) de \\(L\\ll n\\) frecuencias fundamentales contiguas centradas alrededor \\(\\omega_j=j/n\\) que estén cercanas a la frecuencia de interés \\(\\omega\\) como \\[\\begin{equation} \\mathcal{B}=\\left\\{\\omega:\\omega_j\\frac{m}{n}\\leq\\omega\\leq\\omega_j+\\frac{m}{n}\\right\\} \\tag{6.182} \\end{equation}\\] donde \\[\\begin{equation} L=2m+1 \\tag{6.183} \\end{equation}\\] es un número impar, elegido tal que los valores espectrales en el intervalo \\(\\mathcal{B}\\) \\[f(\\omega_j+k/n)\\text{, }k=-m,\\ldots,0,\\ldots,m\\] son aproximadamente igual a \\(f(\\omega)\\). Esta estructura se puede desarrollar para un muestra grande. Los valores del espectro en esta banda de frecuencia serán relativamente constantes, así también será un buen estimador para el espectro suavizado que definimos a continuación. Usando la banda anterior, podemos definir un periodograma suavizado o de media como el promedio de los valores del periodograma, esto es, \\[\\begin{equation} \\bar{f}(\\omega)=\\frac{1}{L}\\sum_{k=-m}^{m}I(\\omega_j+k/n) \\tag{6.184} \\end{equation}\\] como el promedio sobre la banda \\(\\mathcal{B}\\). Bajo la suposición que la densidad espectral es casi constante en la banda \\(\\mathcal{B}\\) y en vista de (6.179) podemos demostrar que bajo condiciones apropiadas, 20 para \\(n\\) grande, los periodogramas en (6.184) son variables aleatorias distribuidas aproximadamente como variables \\(f(\\omega)\\chi^2_2/2\\) independientes, para \\(0&lt;\\omega&lt;1/2\\), siempre y cuando mantengamos \\(L\\) bastante pequeño con relación a \\(n\\). Por consiguiente, bajo estas condiciones, \\(L\\bar{f}(\\omega)\\) es la suma de \\(L\\) variables aleatorias \\(f(\\omega)\\chi^2_2/2\\) aproximadamente independientes. Se sigue que para \\(n\\) grande \\[\\begin{equation} \\frac{2L\\bar{f}(\\omega)}{f(\\omega)}\\overset{\\cdot}{\\sim}\\chi^2_{2L} \\tag{6.185} \\end{equation}\\] donde \\(\\overset{\\cdot}{\\sim}\\) significa aproximadamente distribuida como. De esta manera, es razonable llamar a la longitud del intervalo definido por (6.182) \\[\\begin{equation} B_w=\\frac{L}{n} \\tag{6.186} \\end{equation}\\] el ancho de banda. El ancho de banda en este caso, se refiere al ancho de la banda de frecuencia usada para suavizar el periodograma. El concepto de ancho de banda, sin embargo, se hace más complicado con la introducción de los estimadores espectrales que suavizan con pesos desiguales. Note que (6.186) implica que los grados de libertad los podemos expresar como \\[\\begin{equation} 2L=2B_wn \\tag{6.187} \\end{equation}\\] o dos veces el producto del ancho de banda por tiempo. El resultado (6.185) se puede reordenar para obtener un intervalo de confianza aproximado del \\(100(1-\\alpha)\\%\\) de la forma \\[\\begin{equation} \\frac{2L\\bar{f}(\\omega)}{\\chi^2_{2L}(1-\\alpha/2)}\\leq f(\\omega)\\leq\\frac{2L\\bar{f}(\\omega)}{\\chi^2_{2L}(\\alpha/2)} \\tag{6.188} \\end{equation}\\] para el espectro verdadero \\(f(\\omega)\\). Muchas veces el impacto visual del gráfico de la densidad espectral se puede mejorar, graficando el logaritmo del espectro en vez del espectro. 21 Este fenómeno puede ocurrir cuando en algunas regiones del espectro existen picos de interés mucho más pequeños que los de las componentes principales. Para el logaritmo del espectro obtenemos un intervalo de confianza de la forma \\[\\begin{equation} \\left[\\ln\\bar{f}(\\omega)+\\ln2L-\\ln\\chi^2_{2L}(1-\\alpha/2),\\ln\\bar{f}(\\omega)+\\ln2L-\\ln\\chi^2_{2L}(\\alpha/2)\\right]. \\tag{6.189} \\end{equation}\\] Podemos realizar también una prueba de hipótesis relativa a la igualdad del espectro usando el hecho de que la distribución resultante (6.185) implica que el radio del espectro basado en una muestra aproximadamente independiente tiene distribución aproximada \\(F_{2L}^{2L}\\). Ejemplo 6.41 (Periodograma suavizado de las series SOI y reclutamiento (nuevos peces)) En la Figura ?? graficamos los periodogramas para las series SOI y Reclutamiento (nuevos peces). En la gráfica se puede notar una frecuencia baja en el efecto El Niño, lo que sugiere que un suavizado nos permitirá identificar las frecuencias dominantes sobre todos los periodos. La elección del valor de \\(L=9\\) luce razonable para el suavizado. El ancho de banda en este caso es \\(B_w=9/480=0.01875\\) ciclos por meses para el espectro estimado. La Figura ?? muestra los periodogramas suavizados de ambas series. Allí se puede notar, (líneas punteadas) las cuatro frecuencias dominantes, estas son \\(\\omega_j=1/12,2/12,3/12\\) y \\(1/48\\). También puede observar el ancho de banda que es \\(B_w=0.00541\\). Ejemplo 6.42 (Serie de Alturas de Olas. Estación 144. ST. PETERSBURG) La Figura ?? muestra el registro de alturas de olas y el correspondiente periodograma. Las alturas de olas fueron registrados por una boya ubicada en el Golfo de México, cercana a las costa de St. Petersburg, Florida, EE.UU, tomadas el 1ro. enero de 2009 con una frecuencia de muestreo de 1.28Hz. Los comandos en R son: # SP=matrix(scan(&quot;data/station14401.txt&quot;), byrow=TRUE, ncol=2) # m&lt;-matrix(c(1,1:3),2,2,byrow=TRUE) # layout(m) # plot(SP[,1]/0.78,SP[,2],type=&quot;l&quot;, xlab=&quot;Tiempo (seg)&quot;,ylab=&quot;Alturas (m)&quot;, main=&quot;Altura de olas, Estacion 431, St. Petersburg, FL&quot;) # I1=spectrum(SP[,2],spans=3,log=&quot;no&quot;, main=&quot;Periodograma suavizado estacion 144&quot;) # I2=spectrum(SP[,2],log=&quot;no&quot;, main=&quot;Periodograma estacion 144&quot;) Ejemplo 6.43 (Periodogramas para las series de Terremotos y Explosiones) La Figura ?? muestra el espectro calculado por separado de las dos fases del terremoto y explosión en la Figura 2.7 del capítulo 2. # x=matrix(scan(&quot;data/eq5exp6.txt&quot;),ncol=2) # eqP=x[1:1024,1]; eqS=x[1025:2048,1] # exP=x[1:1024,2]; exS=x[1025:2048,2] # par(mfrow=c(2,2)) # eqPs=spectrum(eqP, main=&quot;Espectro del sismo (fase P)&quot;, log=&quot;no&quot;, xlim=c(0,0.25), ylim=c(0,0.04)) # eqSs=spectrum(eqS, main=&quot;Espectro del sismo (fase S)&quot;, log=&quot;no&quot;, xlim=c(0,0.25), ylim=c(0,0.4)) # exPs=spectrum(exP, main=&quot;Espectro de explosiones (fase P)&quot;, log=&quot;no&quot;, xlim=c(0,0.25), ylim=c(0,0.04)) # exSs=spectrum(exS, main=&quot;Espectro de explosiones (fase S)&quot;, log=&quot;no&quot;, xlim=c(0,0.25), ylim=c(0,0.4)) 6.6 Procesos de Incremento Ortogonal sobre \\([-\\pi,\\pi]\\) Con el fin de dar un significado preciso a la representación espectral (6.145) mencionada anteriormente, es necesario introducir el concepto de integración estocástica de una función no-aleatoria con respecto a un proceso de incremento ortogonal \\(\\{Z(\\lambda)\\}\\) . Definición 6.15 Un proceso \\(\\{x_t\\}\\) es un proceso estacionario a valores complejos si \\(\\mathbb{E}|x_t^2|&lt;\\infty\\), \\(\\mathbb{E}(X_t)\\) es independiente de \\(t\\) y \\(\\mathbb{E}(x_{t+h}\\bar{x}_t)\\) es independiente de \\(t\\) Definición 6.16 La función de autocovarianza \\(\\gamma(\\cdot)\\) de un proceso estacionario a valores complejos \\(\\{x_t\\}\\) es \\[\\begin{equation} \\gamma(h) = \\mathbb{E}(x_{t+h}\\bar{x}_t) - \\mathbb{E}(x_{t+h})\\mathbb{E}(\\bar{x}_t). \\tag{6.190} \\end{equation}\\] Teorema 6.4 Una función \\(K(\\cdot)\\) definida sobre los enteros en la función de autocovarianza de una serie estacionaria (posiblemente a valores complejos) si y solo si \\(K(\\cdot)\\) es Hermitiana y no-negativa definida, esto es, si y solo si \\(K(n)=\\overline{K(-n)}\\) y \\[\\begin{equation} \\sum_{i,j=1}^na_iK(i-j)\\bar{a}_j\\geq0, \\tag{6.191} \\end{equation}\\] para todo entero positivo \\(n\\) y todo vector \\(\\mathbf{a}=(a_1,\\ldots,a_n)^t\\in\\mathbb{C}^n\\). El Teorema 6.4 caracteriza la función de autocovarianza a valores complejos sobre los enteros como aquellas funciones que son Hermitianas y no-negativa definida. El Teorema de Herglotz, el cual presentaremos a continuación, caracteriza estas como las funciones que pueden ser escritas en la forma (6.145) para alguna función de distribución acotada \\(F\\) con masa concentrada en \\((-\\pi,\\pi]\\). Teorema 6.5 (Teorema de Herglotz) Una función a valores complejos \\(\\gamma(\\cdot)\\) definida sobre los enteros es no-negativa definida si y solo si \\[\\begin{equation} \\gamma(h) = \\int_{-\\pi}^{\\pi}e^{ihv}dF(v)\\text{ para todo }h=0,\\pm1,\\pm2,\\ldots, \\tag{6.192} \\end{equation}\\] donde \\(F(\\cdot)\\) es una función acotada en \\([-\\pi,\\pi]\\) continua a la derecha, no decreciente y \\(F(-\\pi)=0\\). Definición 6.17 Un proceso de incremento ortogonal sobre \\([-\\pi,\\pi]\\) es un proceso estocástico a valores complejos \\(\\{Z(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\) tal que \\[\\begin{eqnarray} \\langle Z(\\lambda),Z(\\lambda)\\rangle &amp;&lt;&amp; \\infty\\text{, con }-\\pi\\leq\\lambda\\leq\\pi \\tag{6.193} \\\\ \\langle Z(\\lambda),1\\rangle &amp;=&amp; 0 \\text{, con }-\\pi\\leq\\lambda\\leq\\pi \\tag{6.194} \\end{eqnarray}\\] y \\[\\begin{equation} \\langle Z(\\lambda_4)-Z(\\lambda_3),Z(\\lambda_2)-Z(\\lambda_1)\\rangle=0\\text{, si }(\\lambda_1,\\lambda_2]\\cap(\\lambda_3,\\lambda_4]=\\emptyset \\tag{6.195} \\end{equation}\\] donde el producto interno se define como \\(\\langle X,Y\\rangle=\\mathbb{E}(X\\bar{Y})\\). El proceso \\(\\{Z(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\) se llamará continuo a la derecha si para todo \\(\\lambda\\in[-\\pi,\\pi)\\) \\[\\|Z(\\lambda+\\delta)-Z(\\lambda)\\|^2=\\mathbb{E}|Z(\\lambda+\\delta)-Z(\\lambda)|^2\\to0\\text{ cuando }\\delta\\downarrow0.\\] Proposición 6.12 Si \\(\\{Z(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\) es un proceso de incremento ortogonal, entonces existe una única función de distribución \\(F\\) (es decir, una única función continua a derecha no decreciente) tal que \\[\\begin{equation} \\begin{array}{lclc} F(\\lambda) &amp;=&amp;0, &amp; \\lambda\\leq-\\pi \\\\ F(\\lambda) &amp;=&amp; F(\\pi), &amp; \\lambda\\geq\\pi \\\\ F(\\mu)-F(\\lambda) &amp;=&amp; \\|Z(\\mu)-Z(\\lambda)\\|^2, &amp; -\\pi\\leq\\lambda\\leq\\mu\\leq\\pi\\\\ \\end{array} \\tag{6.196} \\end{equation}\\] Demostración. Para \\(F\\) satisfaciendo las condiciones prescritas es claro, haciendo \\(\\lambda=-\\pi\\) que \\[\\begin{equation} F(\\mu)=\\|Z(\\mu)-Z(-\\pi)\\|^2\\text{, }-\\pi\\leq\\mu\\leq\\pi \\tag{6.197} \\end{equation}\\] Para verificar que la función así definida es no decreciente, usamos la ortogonalidad de \\(Z(\\mu)-Z(\\lambda)\\) y \\(Z(\\lambda)-Z(-\\pi), -\\pi\\leq\\lambda\\leq\\mu\\leq\\pi\\) para escribir \\[\\begin{eqnarray*} F(\\mu) &amp;=&amp; \\|Z(\\mu)-Z(\\lambda)+Z(\\lambda)-Z(-\\pi)\\|^2 \\\\ &amp;=&amp; \\|Z(\\mu)-Z(\\lambda)\\|^2+\\|Z(\\lambda)-Z(-\\pi)\\|^2 \\\\ &amp;\\geq&amp; F(\\lambda) \\end{eqnarray*}\\] El mismo procedimiento nos da para \\(-\\pi\\leq\\mu\\leq\\mu+\\delta\\leq\\pi\\) \\[F(\\mu+\\delta)-F(\\mu)=\\|Z(\\mu+\\delta)-Z(\\mu)\\|^2\\to0\\text{, cuando }\\delta\\downarrow0,\\] por la suposición de continuidad a derecha de \\(\\{Z(\\lambda)\\}\\) Nota. La función de distribución \\(F\\) de la Proposición 6.12, definida en \\([-\\pi,\\pi]\\) por (6.197) será referida como la función de distribución asociada con el proceso de incremento ortogonal \\(\\{Z(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\). Es común en la práctica en el análisis de series de tiempo usar la notación corta \\[\\mathbb{E}(dZ(\\lambda),d\\bar{Z(\\mu)})=\\delta_{\\lambda,\\mu}dF(\\lambda)\\] para las ecuaciones (6.195) y (6.196). Definición 6.18 Un Movimiento Browniano Estándar iniciando en nivel cero es un proceso \\(\\{B(t),t\\geq0\\}\\) que satisface las siguientes condiciones: \\(B(0)=0\\), \\(B(t_2)-B(t_1),B(t_3)-B(t_2),\\ldots,B(t_n)-B(t_{n-1})\\) son independientes para cada \\(n\\in\\{3,4,\\ldots\\}\\) y cada \\(t=(t_1,\\ldots,t_n)^t\\) tal que \\(0\\leq t_1&lt;t_2&lt;\\ldots&lt;t_n\\), \\(B(t)-B(s)\\sim N(0,t-s)\\) para \\(t\\geq s\\). Ejemplo 6.44 Un movimiento browniano \\(\\{B(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\) con \\(\\mathbb{E}B(\\lambda)=0\\) y \\(\\text{var}(B(\\lambda))=\\sigma^2(\\lambda+\\pi)/2\\pi\\text{, }-\\pi\\leq\\lambda\\leq\\pi\\), es un proceso de incremento ortogonal sobre \\([-\\pi,\\pi]\\). La función de distribución asociada satisface \\(F(\\lambda)=0\\text{, para }\\lambda\\leq-\\pi, F(\\lambda)=\\sigma^2\\text{, para }\\lambda\\geq\\pi\\) y \\[F(\\lambda)=\\sigma^2(\\lambda+\\pi)/2\\pi\\text{, para }-\\pi\\leq\\lambda\\leq\\pi.\\] Definición 6.19 Un Proceso de Poisson con media \\(\\lambda&gt;0\\) es un proceso \\(\\{N(t),t\\geq0\\}\\) que satisface: \\(N(0)=0\\), \\(N(t_2)-N(t_1),N(t_3)-N(t_2),\\ldots,N(t_n)-N(t_{n-1})\\) son independientes para cada \\(n\\in\\{3,4,\\ldots\\}\\) y cada \\(t=(t_1,\\ldots,t_n)\\) tal que \\(0\\leq t_1&lt;t_2&lt;\\ldots&lt;t_n\\), \\(N(t)-N(s)\\) tiene distribución de Poisson con media \\(\\lambda(t-s)\\) para \\(t\\geq s\\). Ejemplo 6.45 Si \\(\\{N(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\) es un proceso de Poisson sobre \\([-\\pi,\\pi]\\) con intensidad constante \\(c\\) entonces el proceso \\(Z(\\lambda)=N(\\lambda)-\\mathbb{E}N(\\lambda)\\text{, }-\\pi\\leq\\lambda\\leq\\pi\\), es un proceso de incremento ortogonal con función de distribución asociada \\[F(\\lambda)=\\begin{cases} 0&amp;\\text{, para }\\lambda\\leq-\\pi\\\\ 2\\pi c&amp;\\text{, para }\\lambda\\geq\\pi\\\\ c(\\lambda+\\pi)&amp;\\text{, para }-\\pi\\leq\\lambda\\leq\\pi \\end{cases} \\] Si escogemos \\(c\\) como \\(\\sigma^2/2\\pi\\) entonces \\(\\{Z(\\lambda)\\}\\) tiene exactamente la misma función de distribución asociada como la de \\(\\{B(\\lambda)\\}\\) del Ejemplo 6.44. 6.7 Integración con Respecto a un Proceso de Incremento Ortogonal En esta sección demostraremos como definir la integral estocástica \\[I(f)=\\int_{(-\\pi,\\pi]}f(v)dZ(v)\\] donde \\(\\{Z(\\lambda)\\text{, }-\\pi\\leq\\lambda\\leq\\pi\\}\\) es un proceso de incremento ortogonal definido sobre el espacio de probabilidad \\((\\Omega,\\mathcal{F},P)\\) y \\(f\\) es cada función en \\([-\\pi,\\pi]\\) cuadrado integrable con respecto a la función de distribución \\(F\\) asociada con \\(Z(\\lambda)\\). Procederemos paso por paso, primero definiremos \\(I(f)\\) para cada función \\(f\\) de la forma \\[\\begin{equation} f(\\lambda)=\\sum_{i=0}^{n}f_iI_{(\\lambda_i,\\lambda_{i+1}]}(\\lambda)\\text{, }-\\pi=\\lambda_0&lt;\\lambda_1&lt;\\cdots&lt;\\lambda_{n+1}=\\pi \\tag{6.198} \\end{equation}\\] como \\[\\begin{equation} I(f)=\\sum_{i=0}^{n}f_i[Z(\\lambda_{i+1})-Z(\\lambda_i)] \\tag{6.199} \\end{equation}\\] Entonces, extendemos la aplicación \\(I\\) a un isomorfismo de \\(L^2([-\\pi,\\pi],\\mathcal{B},F)\\equiv L^2(F)\\) a un subespacio de \\(L^2(\\Omega,\\mathcal{F},P)\\). Sea \\(\\mathcal{D}\\) la clase de todas las funciones que tiene la forma (6.198) para algún \\(n\\in\\{0,1,2,\\ldots\\}\\). Entonces la definición (6.199) es consistente en \\(\\mathcal{D}\\) dado que para cada \\(f\\in\\mathcal{D}\\) existe una única representación de \\(f\\), \\[f(\\lambda)=\\sum_{i=0}^{n}r_iI_{(v_i,v_{i+1}]}(\\lambda)\\text{, }-\\pi=v_0&lt;v_1&lt;\\cdots&lt;v_{m+1}=\\pi,\\] en la cual \\(r_i\\neq r_{i+1}, 0\\leq i&lt;m\\). Todas las otras representaciones de \\(f\\) que tienen la forma (6.198) son obtenidas por medio de reexpresar una o más funciones indicatrices \\(I_{(v_i,v_{i+1}]}\\) como una suma de funciones indicatrices de intervalos adjuntos. Sin embargo, esto no hace ninguna diferencia en el valor de \\(I(f)\\), y por consiguiente la definición (6.199) es la misma para todas las representaciones (6.198) de \\(f\\). Es claro que (6.199) define \\(I\\) como una aplicación lineal sobre \\(\\mathcal{D}\\). Más aún, la aplicación preserva el producto interno ya que si \\(f\\in\\mathcal{D}\\) y \\(g\\in\\mathcal{D}\\) entonces existen representaciones \\[f(\\lambda)=\\sum_{i=0}^{n}f_iI_{(\\lambda_i,\\lambda_{i+1}]}(\\lambda)\\] \\[g(\\lambda)=\\sum_{i=0}^{n}g_iI_{(\\lambda_i,\\lambda_{i+1}]}(\\lambda)\\] en términos de una partición simple \\(-\\pi=\\lambda_0&lt;\\lambda_1&lt;\\cdots&lt;\\lambda_{n+1}=\\pi\\). Por lo tanto, el producto interno de \\(I(f)\\) e \\(I(g)\\) en \\(L^2(\\Omega,\\mathcal{F},P)\\) es \\[\\begin{eqnarray*} \\langle I(f),I(g)\\rangle &amp;=&amp; \\left\\langle\\sum_{i=0}^{n}f_i[Z(\\lambda_{i+1})-Z(\\lambda_i)],\\sum_{i=0}^{n}g_i[Z(\\lambda_{i+1})-Z(\\lambda_i)\\right\\rangle \\\\ &amp;=&amp; \\sum_{i=0}^{n}f_i\\bar{g}_i(F(\\lambda_{i+1})-F(\\lambda_i)) \\end{eqnarray*}\\] por la ortogonalidad de los incrementos de \\(\\{Z(\\lambda)\\}\\) y la Proposición 6.12. La última expresión la podemos escribir como \\[\\int_{(-\\pi,\\pi]}f(v)\\bar{g}(v)dF(v)=\\langle f,g\\rangle_{L^2(F)}\\] el producto interno en \\(L^2(F)\\) de \\(f\\) y \\(g\\). Por lo tanto la aplicación I sobre \\(\\mathcal{D}\\) preserva los productos internos. Ahora, denotemos \\(\\bar{\\mathcal{D}}\\) la clausura en \\(L^2(F)\\) del conjunto \\(\\mathcal{D}\\). Si \\(f\\in\\bar{\\mathcal{D}}\\) entonces existe una sucesión \\(\\{f_n\\}\\) de elementos de \\(\\mathcal{D}\\) tal que \\(\\|f_n-f\\|_{L^2(f)}\\to0\\). Por lo tanto definimos \\(I(f)\\) como el límite en media cuadrado \\[\\begin{equation} I(f)=\\underset{n\\to\\infty}{m.s.\\lim}I(f_n) \\tag{6.200} \\end{equation}\\] Primero comprobemos (a) que el límite existe y (b) que el límite es el mismo para todas las sucesiones \\(\\{f_n\\}\\) tal que \\(\\|f_n-f\\|_{L^2(F)}\\to0\\). Para comprobar (a) simplemente observe que para \\(f_m,f_n\\in\\mathcal{D}\\), \\[\\begin{eqnarray*} \\|I(f_n)-I(f_m)\\| &amp;=&amp; \\|I(f_n-f_m)\\| \\\\ &amp;=&amp; \\|f_n-f_m\\|_{L^2(F)}, \\end{eqnarray*}\\] de modo que si \\(\\|f_n-f_m\\|_{L^2(F)}\\to0\\), la sucesión \\(\\{I(f_n)\\}\\) es una sucesión de Cauchy y por lo tanto converge en \\(L^2(\\Omega,\\mathcal{F},P)\\). Para comprobar (b), supóngase que \\(\\|f_n-f\\|_{L^2(F)}\\to0\\) y \\(\\|g_n-f\\|_{L^2(F)}\\to0\\) donde \\(f_n,g_n\\in\\mathcal{D}\\). Entonces la sucesión \\(f_1,g_1,f_2,g_2,\\ldots\\), debe converger en norma y por lo tanto la sucesión \\(I(f_1), I(g_1)\\), \\(I(f_2), I(g_2), \\ldots\\), debe converger en \\(L^2(\\Omega,\\mathcal{D},P)\\). Sin embargo, esto no es posible a menos que las subsucesiones \\(I(f_n)\\) e \\(I(g_n)\\) tengan el mismo límite en media cuadrado. Esto completa la prueba de que la definición (6.200) es válida y consistente para \\(f\\in\\bar{\\mathcal{D}}\\). La aplicación \\(I\\) sobre \\(\\bar{\\mathcal{D}}\\) es lineal y preserva el producto interno ya que si \\(f^{(i)}\\in\\bar{\\mathcal{D}}\\) y \\ \\(\\|f_n^{(i)}-f^{(i)}\\|_{L^2(F)}\\to0, f_n^{(i)}\\in\\mathcal{D},i=1,2\\), entonces por linealidad de \\(I\\) en \\(\\mathcal{D}\\) \\[\\begin{eqnarray*} I(a_1f^{(1)}+a_2f^{(2)}) &amp;=&amp; \\lim_{n\\to\\infty}I(a_1f_n^{(1)}+a_2f_n^{(2)}) \\\\ &amp;=&amp; \\lim_{n\\to\\infty}(a_1I(f_n^{(1)})+a_2I(f_n^{(2)})) \\\\ &amp;=&amp; a_1I(f^{(1)})+a_2I(f^{(2)}) \\end{eqnarray*}\\] y por continuidad del producto interno \\[\\begin{eqnarray*} \\langle I(f^{(1)},I(f^{(2)}\\rangle &amp;=&amp; \\lim_{n\\to\\infty}\\langle I(f_n^{(1)}),I(f_n^{(2)})\\rangle \\\\ &amp;=&amp; \\lim_{n\\to\\infty}\\langle f_n^{(1)},f_n^{(2)}\\rangle_{L^2(F)} \\\\ &amp;=&amp; \\langle f^{(1)},f^{(2)}\\rangle_{L^2(F)}\\rangle. \\end{eqnarray*}\\] Falta solo demostrar que \\(\\bar{\\mathcal{D}}=L^2(F)\\). Para hacer esto primero observe que las funciones continuas en \\([-\\pi,\\pi]\\) son densas en \\(L^2(F)\\) ya que \\(F\\) es una función de distribución acotada. Más aún \\(\\mathcal{D}\\) es un subconjunto denso (en el sentido \\(L^2(F)\\)) del conjunto de funciones continuas sobre \\([-\\pi,\\pi]\\). Por consiguiente \\(\\bar{\\mathcal{D}}=L^2(F)\\). Las ecuaciones (6.199) y (6.200) entonces definen \\(I\\) como una aplicación lineal que preserva el producto interno sobre \\(\\bar{\\mathcal{D}}=L^2(F)\\) en \\(L^2(\\Omega,\\mathcal{F},P)\\). La imagen \\(I(\\bar{\\mathcal{D}})\\) de \\(\\bar{\\mathcal{D}}\\) es claramente un subespacio lineal cerrado de \\(L^2(\\Omega,\\mathcal{F},P)\\) y la aplicación \\(I\\) es un isomorfismo de \\(\\bar{\\mathcal{D}}\\) en \\(I(\\bar{\\mathcal{D}})\\). La aplicación \\(I\\) que nos proporciona la definición necesita de la integral estocástica. Definición 6.20 Si \\(\\{Z(\\lambda)\\}\\) es un proceso de incremento ortogonal sobre \\([-\\pi,\\pi]\\) con función de distribución asociada \\(F\\) y si \\(f\\in L^2(F)\\), entonces \\(\\int_{(-\\pi,\\pi]}f(\\lambda)dZ(\\lambda)\\) se define como la variable aleatoria \\(I(f)\\) construida arriba, esto es, \\[\\begin{equation} \\int_{(-\\pi,\\pi]}f(v)dZ(v):=I(f). \\tag{6.201} \\end{equation}\\] 6.7.1 Propiedades de la Integral Estocástica Para cada par de funciones \\(f\\) y \\(g\\) en \\(L^2(F)\\) hemos establecidos las propiedades \\[\\begin{eqnarray} I(a_1f+a_2g) &amp;=&amp; a_1I(f)+a_2I(g)\\text{, }a_1,a_2\\in\\mathbb{C} \\tag{6.202}\\\\ \\mathbb{E}(I(f)\\bar{I(g)}) &amp;=&amp; \\int_{(-\\pi,\\pi]}f(v)\\bar{g(v)}dF(v) \\tag{6.203} \\end{eqnarray}\\] Más aún, si \\(\\{f_n\\}\\) y \\(\\{g_n\\}\\) son sucesiones en \\(L^2(F)\\) tal que \\(\\|f_n-f\\|_{L^2(F)}\\to0\\) y \\(\\|g_n-g\\|_{L^2(F)}\\to0\\), entonces por continuidad del producto interno \\[\\begin{equation} \\mathbb{E}(I(f_n)\\bar{I(g_n)})\\to\\mathbb{E}(I(f)\\bar{I(g)})=\\int_{(-\\pi,\\pi]}f(v)\\bar{g(v)}dF(v) \\tag{6.204} \\end{equation}\\] De (6.199) es claro que \\[\\begin{equation} \\mathbb{E}(I(f))=0 \\tag{6.205} \\end{equation}\\] para todo \\(f\\in\\mathcal{D}\\); si \\(f\\in\\bar{\\mathcal{D}}\\) entonces existe una sucesión \\(\\{f_n\\}, f_n\\in\\mathcal{D}\\) tal que \\(f_n\\overset{L^2(F)}{\\longrightarrow}f\\) y \\(I(f_n)\\overset{m.s.}{\\longrightarrow}I(f)\\), de modo que \\(\\mathbb{E}(I(f))=\\lim_{n\\to\\infty}\\mathbb{E}(I(f_n))\\) y (6.205) sigue siendo válido. Este argumento es frecuentemente usado para establecer las propiedades de integral estocástica. Finalmente notamos de (6.203) y (6.205) que si \\(\\{Z(\\lambda)\\}\\) es cada proceso de incremento ortogonal sobre \\([-\\pi,\\pi]\\) con función de distribución asociada \\(F\\), entonces \\[\\begin{equation} X_t=I(e^{it})=\\int_{(-\\pi,\\pi]}e^{itv}dZ(v)\\text{, }t\\in\\mathbb{Z}, \\tag{6.206} \\end{equation}\\] es un proceso estacionario con media cero y función de autocovarianza \\[\\begin{equation} \\mathbb{E}(X_{t+h}\\bar{X}_t)=\\int_{(-\\pi,\\pi]}e^{ivh}dF(v). \\tag{6.207} \\end{equation}\\] 6.8 La Representación Espectral Sea \\(\\{X_t\\}\\) un proceso estacionario de media cero con función de distribución espectral \\(F\\). Para establecer la representación espectral \\[\\begin{equation} X_t=\\int_{(-\\pi,\\pi]}e^{itv}dZ(v) \\tag{6.208} \\end{equation}\\] del proceso \\(\\{X_t\\}\\) necesitamos primero identificar un proceso de incremento ortogonal apropiado \\(\\{Z(\\lambda),\\lambda\\in[-\\pi,\\pi]\\}\\). La identificación de \\(\\{Z(\\lambda)\\}\\) y la demostración de la representación se logrará mediante la definición de un isomorfismo entre ciertos subespacios \\(\\overline{\\mathcal{H}}=\\overline{sp}\\{X_t,t\\in\\mathbb{Z}\\}\\) de \\(L^2(\\Omega,\\mathfrak{F},P)\\) y \\(\\overline{\\mathcal{K}}=\\overline{sp}\\{e^{it},t\\in\\mathbb{Z}\\}\\) 22 de \\(L^2(F)\\). Este isomorfismo proporcionará un vínculo entre las variables aleatorias en el dominio del tiempo y las funciones sobre \\([-\\pi, \\pi]\\) en el dominio de la frecuencia. Sean \\(\\mathcal{H}=\\{X_t,t\\in\\mathbb{Z}\\}\\) y \\(\\mathcal{K}=sp\\{e^{it},t\\in\\mathbb{Z}\\}\\) subespacios (no necesariamente cerrados) de \\(\\mathcal{H}\\subset L^2(\\Omega,\\mathfrak{F},P)\\) y \\(\\mathcal{K}\\subset L^2(F)\\) consistentes de combinaciones lineales finitas de \\(X_t,t\\in\\mathbb{Z}\\) y \\(e^{it},t\\in\\mathbb{Z}\\), respectivamente. Demostraremos primero que la aplicación \\[\\begin{equation} T\\left(\\sum_{j=1}^{n}a_jX_{t_j}\\right)=\\sum_{j=1}^{n}a_je^{it_j} \\tag{6.209} \\end{equation}\\] define un isomorfismo entre \\(\\mathcal{H}\\) y \\(\\mathcal{K}\\). Para verificar que \\(T\\) está bien definida, supóngase que \\(\\|\\sum_{j=1}^{n}a_jX_{t_j}-\\sum_{k=1}^{m}b_kX_{t_k}\\|=0\\). Entonces por definición de la norma \\(L^2(F)\\) y el teorema de Herglotz (Teorema 6.5) \\[\\begin{eqnarray*} \\left\\|T\\left(\\sum_{j=1}^{n}a_jX_{t_j}\\right)-T\\left(\\sum_{k=1}^{m}b_kX_{t_k}\\right)\\right\\|^2_{L^2(F)}&amp;=&amp;\\int_{(-\\pi,\\pi]}\\left|\\sum_{j=1}^{n}a_je^{it_jv}-\\sum_{k=1}^{m}b_ke^{it_kv}\\right|^2dF(v)\\\\ &amp;=&amp;\\mathbb{E}\\left|\\sum_{j=1}^{n}a_jX_{t_j}-\\sum_{k=1}^{m}b_kX_{t_k}\\right|^2=0, \\end{eqnarray*}\\] muestra que (6.209) define \\(T\\) consistentemente en \\(\\mathcal{H}\\). La linealidad de \\(T\\) se sigue de este hecho. Adicionalmente \\[\\begin{eqnarray*} \\left\\langle T\\left(\\sum_{j=1}^{n}a_jX_{t_j}\\right),T\\left(\\sum_{k=1}^{m}b_kX_{s_k}\\right)\\right\\rangle &amp;=&amp; \\sum_{j=1}^{n}\\sum_{k=1}^{m}a_j\\bar{b}_k\\langle e^{it_j},e^{is_k}\\rangle_{L^2(F)} \\\\ &amp;=&amp; \\sum_{j=1}^{n}\\sum_{k=1}^{m}a_j\\bar{b}_k\\int_{(-\\pi,\\pi]}e^{i(t_j-s_k)v}dF(v) \\\\ &amp;=&amp; \\sum_{j=1}^{n}\\sum_{k=1}^{m}a_j\\bar{b}_k\\langle X_{t_j},X_{s_k}\\rangle \\\\ &amp;=&amp; \\left\\langle\\sum_{j=1}^{n}a_jX_{t_j},\\sum_{k=1}^{m}b_kX_{s_k}\\right\\rangle \\end{eqnarray*}\\] mostrando que \\(T\\) de hecho define un isomorfismo entre \\(\\mathcal{H}\\) y \\(\\mathcal{K}\\). Demostraremos ahora que la aplicación \\(T\\) se puede extender de manera única a un isomorfismo de \\(\\overline{\\mathcal{H}}\\) en \\(\\overline{\\mathcal{K}}\\). Si \\(Y\\in\\overline{\\mathcal{H}}\\) entones existe una sucesión \\(Y_n\\in\\mathcal{H}\\) tal que \\(\\|Y_n-Y\\|\\to0\\). Esto implica que \\(\\{Y_n\\}\\) es una sucesión de Cauchy y por consiguiente, dado que \\(T\\) preserva la norma, la sucesión \\(\\{TY_n\\}\\) es Cauchy en \\(L^2(F)\\). La sucesión \\(\\{TY_n\\}\\) por lo tanto converge en norma a un elemento de \\(\\overline{\\mathcal{K}}\\). Si \\(T\\) preserva la norma sobre \\(\\overline{\\mathcal{H}}\\) definimos \\[TY=m.s.\\lim_{n\\to\\infty}TY_n.\\] Esta es una definición consistente de \\(T\\) en \\(\\overline{\\mathcal{H}}\\) ya que si \\(\\|\\tilde{Y}_n-Y\\|\\to0\\) entonces la sucesión \\(TY_1,T\\tilde{Y}_1\\), \\(TY_2,T\\tilde{Y}_2,\\ldots\\) es convergente, lo que implica que las subsucesiones \\(\\{TY_n\\}\\) y \\(\\{T\\tilde{Y}_n\\}\\) tienen el mismo límite, llamémoslo \\(TY\\). Más aún, usando el mismo argumento dado en la sección Integración con Respecto a un Proceso de Incremento Ortogonal es fácil demostrar que la aplicación \\(T\\) extendida a \\(\\overline{\\mathcal{H}}\\) es lineal y preserva el producto interno. Finalmente, del teorema siguiente se tiene que \\(\\mathcal{K}\\) es uniformemente denso en el espacio de funciones continuas \\(\\phi\\) en \\([-\\pi,\\pi]\\) con \\(\\phi(\\pi)=\\phi(-\\pi)\\), que a su vez es denso en \\(L^2(F)\\). Por consiguiente \\(\\overline{\\mathcal{K}}=L^2(F)\\). Teorema 6.6 Sea \\(f\\) una función continua en \\([-\\pi,\\pi]\\) tal que \\(f(\\pi)=f(-\\pi)\\). Entonces \\[\\begin{equation} n^{-1}(S_0f+S_1f+\\cdots+S_{n-1}f)\\to f \\tag{6.210} \\end{equation}\\] uniformemente en \\([-\\pi,\\pi]\\) cuando \\(n\\to\\infty\\). De los hechos anteriores, se tiene el siguiente teorema Teorema 6.7 Si \\(F\\) es la función de distribución espectral del proceso estacionario \\(\\{X_t,t\\in\\mathbb{Z}\\}\\), entonces existe un único isomorfismo \\(T\\) de \\(\\overline{sp}\\{X_t,t\\in\\mathbb{Z}\\}\\) en \\(L^2(F)\\) tal que \\[TX_t=e^{it}\\text{, }t\\in\\mathbb{Z}.\\] El Teorema 6.7 es particularmente útil en la teoría de predicción lineal. También es la clave para la identificación de los procesos de incremento ortogonal \\(\\{Z(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\) que aparecen en la representación espectral (6.208). Introducimos el proceso \\(\\{Z(\\lambda)\\}\\) en la siguiente proposición. Proposición 6.13 Si \\(T\\) es definimos como en el Teorema 6.7 entonces el proceso \\(\\{Z(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\) definido por \\[Z(\\lambda)=T^{-1}(I_{(-\\pi,\\lambda]}(\\cdot))\\text{, }-\\pi\\leq\\lambda\\leq\\pi,\\] es un proceso de incremento ortogonal. Más aún, la función de distribución asociada con \\(\\{Z(\\lambda)\\}\\) es exactamente la función de distribución espectral \\(F\\) de \\(\\{X_t\\}\\). Demostración. Para cada \\(\\lambda\\in[-\\pi,\\pi]\\), \\(Z(\\lambda)\\) es un elemento bien definido de \\(\\overline{sp}\\{X_t,t\\in\\mathbb{Z}\\}\\) por el Teorema 6.7. Por lo tanto \\(\\langle Z(\\lambda),Z(\\lambda)\\rangle&lt;\\infty\\). Dado que \\(Z(\\lambda)\\in\\overline{sp}\\{X_t,t\\in\\mathbb{Z}\\}\\) existe una sucesión \\(\\{Y_n\\}\\) de elementos de \\(\\overline{sp}\\{X_t,t\\in\\mathbb{Z}\\}\\) tal que \\(\\|Y_n-Z(\\lambda)\\|\\to0\\) cuando \\(n\\to\\infty\\). Por la continuidad del producto interior tenemos \\[\\langle Z(\\lambda),1\\rangle=\\lim_{n\\to\\infty}\\langle Y_n,1\\rangle=0\\] ya que cada \\(X_t\\), y por consiguiente cada \\(Y_t\\) tiene media cero. Finalmente, si \\(-\\pi\\leq\\lambda_1\\leq\\lambda_2\\leq\\lambda_3\\leq\\lambda_4\\leq\\pi\\), \\[\\begin{eqnarray*} \\langle Z(\\lambda_4)-Z(\\lambda_3),Z(\\lambda_2)-Z(\\lambda_1)\\rangle &amp;=&amp; \\langle TZ(\\lambda_4)-TZ(\\lambda_3),TZ(\\lambda_2)-TZ(\\lambda_1)\\rangle \\\\ &amp;=&amp; \\langle I_{(\\lambda_3,\\lambda_4]}(\\cdot),I_{(\\lambda_1,\\lambda_2]}(\\cdot)\\rangle_{L^2(F)} \\\\ &amp;=&amp; \\int_{(-\\pi,\\pi]}I_{(\\lambda_3,\\lambda_4]}(v)I_{(\\lambda_1,\\lambda_2]}(v)dF(v)=0 \\end{eqnarray*}\\] completando la demostración de que \\(\\{Z(\\lambda)\\}\\) tiene incrementos ortogonales. Un cálculo casi idéntico a los cálculos previos nos da \\[\\langle Z(\\mu)-Z(\\lambda),Z(\\mu)-Z(\\mu)\\rangle=F(\\mu_-F(\\lambda),\\] demostrando que \\(\\{Z(\\lambda)\\}\\) es continua a derecha con función de distribución asociada \\(F\\) como afirma la proposición Ahora es fácil establecer la representación espectral (6.208). Teorema 6.8 (El Teorema de Representación Espectral) Si \\(\\{X_t\\}\\) es una sucesión estacionaria con media cero y función de distribución espectral \\(F\\), entonces existe un proceso de incremento ortogonal continua a la derecha \\(\\{Z(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\) tal que \\[\\text{(i) }\\mathbb{E}|Z(\\lambda)-Z(-\\pi)|^2=F(\\lambda)\\text{, }-\\pi\\leq\\lambda\\leq\\pi,\\] y \\[\\text{(ii) }X_t=\\int_{(-\\pi,\\pi]}E^{itv}dZ(v)\\text{ con probabilidad uno.}\\] Demostración. Sea \\(\\{Z(\\lambda)\\}\\) el proceso definido en la Proposición 6.13 y sea \\(I\\) el isomorfismo \\[I(f)=\\int_{(-\\pi,\\pi]}f(v)dZ(v),\\] de \\(\\overline{\\mathcal{D}}=L^2(F)\\) en \\(I(\\overline{\\mathcal{D}}\\subseteq L^2(\\Omega,\\mathfrak{F},P)\\) discutido en la Sección Integración con Respecto a un Proceso de Incremento Ortogonal. Si \\(f\\in\\mathcal{D}\\) tiene la representación (6.198) entonces \\[\\begin{eqnarray*} I(f) &amp;=&amp; \\sum_{i=0}^{n}f_i(Z(\\lambda_{i+1})-Z(\\lambda_i)) \\\\ &amp;=&amp; T^{-1}(f). \\end{eqnarray*}\\] Esta relación se mantiene válida para toda \\(f\\in\\overline{\\mathcal{D}}=L^2(F)\\) ya que tanto \\(I\\) como \\(T^{-1}\\) son isomorfismos. Por lo tanto tenemos que \\(I=T^{-1}\\) (i.e. \\(TI(f)=f\\) para todo \\(f\\in L^2(F)\\)) y por consiguiente del Teorema 6.7 \\[X_t=I(e^{it\\cdot})=\\int_{(-\\pi,\\pi]}e^{itv}dZ(v),\\] dando la representación requerida para \\(\\{X_t\\}\\). La primera afirmación del Teorema es una consecuencia inmediata de la Proposición 6.13. Corolario 6.1 Si \\(\\{X_t\\}\\) es una sucesión estacionaria de media cero entonces existe un proceso de incremento ortogonal continuo a la derecha \\(\\{Z(\\lambda),-\\pi\\leq\\lambda\\leq\\pi\\}\\) tal que \\(Z(-\\pi)=0\\) y \\[X_t=\\int_{(-\\pi,\\pi]}e^{itv}dZ(v)\\text{ con probabilidad uno.}\\] Si \\(\\{Y(\\lambda)\\}\\) y \\(\\{Z(\\lambda)\\}\\) son dos de tales procesos entonces \\[P(Y(\\lambda)=Z(\\lambda))=1\\text{ para cada}\\lambda\\in[-\\pi,\\pi].\\] Demostración. Si denotamos por \\(\\{Z^{\\star}(\\lambda)\\}\\) el proceso de incremento ortogonal definido por la Proposición 6.13, entonces el proceso \\[Z(\\lambda)=Z^{\\star}(\\lambda)-Z^{\\star}(-\\pi)\\text{, }-\\pi\\leq\\lambda\\leq\\pi,\\] no solo satisface \\(Z(-\\pi)=0\\), sino también tiene exactamente el mismo incremento que \\(\\{Z^{\\star}(\\lambda)\\}\\). Por consiguiente \\[X_t=\\int_{(-\\pi,\\pi]}e^{itv}dZ^{\\star}(v)=\\int_{(-\\pi,\\pi]}e^{itv}dZ(v).\\] Supóngase ahora que \\(\\{Y(\\lambda)\\}\\) es otro proceso de incremento ortogonal tal que \\(Y(-\\pi)=0\\) y \\[\\begin{equation} X_t=\\int_{(-\\pi,\\pi]}e^{itv}dY(v)=\\int_{(-\\pi,\\pi]}e^{itv}dZ(v)\\text{ con probabilidad uno.} \\tag{6.211} \\end{equation}\\] Si definimos para \\(f\\in L^2(F)\\) \\[I_Y(f)=\\int_{(-\\pi,\\pi]}f(v)dY(v)\\] e \\[I_Z(f)=\\int_{(-\\pi,\\pi]}f(v)dZ(v)\\] entonces tenemos de (6.211) \\[\\begin{equation} I_y(e^{it\\cdot})=I_z(e^{it\\cdot})\\text{ para todo }t\\in\\mathbb{Z}. \\tag{6.212} \\end{equation}\\] Dado que \\(I_Y\\) e \\(I_Z\\) son iguales en \\(sp\\{e^{it\\cdot},t\\in\\mathbb{Z}\\}\\) el cual es denso en \\(L^2(F)\\), se sigue que \\(I_Y(f)=I_Z(f)\\) para todo \\(f\\in L^2(F)\\). Eligiendo \\(f(v)=I_{(-\\pi,\\lambda]}(v)\\) obtenemos (con probabilidad uno) \\[Y(\\lambda)=\\int_{(-\\pi,\\pi]}f(v)dZ(v)=Z(\\lambda)\\text{, }-\\pi\\leq\\lambda\\leq\\pi\\] Observación 1. En el transcurso de la demostración del Teorema 6.8 se estableció el siguiente resultado: \\(Y\\in\\overline{sp}\\{X_t,t\\in\\mathbb{Z}\\}\\) si y solo si existe una función \\(f\\in L^2(F)\\) tal que \\(Y=I(f)=\\int_{(-\\pi,\\pi]}f(v)dZ(v)\\). Esto significa que \\(I\\) es un isomorfismo de \\(L^2(F)\\) en \\(\\overline{sp}\\{X_t,t\\in\\mathbb{Z}\\}\\) (con la propiedad de que \\(I(e^{it\\cdot})=X_t\\)). Observación 2. Los argumentos aportados por el Teorema 6.8 es una prueba de existencia que no revela de manera explícita cómo se construye \\(\\{Z(\\lambda)\\}\\). Observación 3. El corolario establece que el proceso de incremento ortogonal en la representación espectral es único si usamos la normalización de \\(Z(-\\pi)=0\\). Dos proceso estacionarios diferentes pueden tener la misma función de distribución espectral, por ejemplo los procesos \\(X_t=\\int_{(-\\pi,\\pi]}e^{it\\lambda}dB(\\lambda)\\) e \\(Y_t=\\int_{(-\\pi,\\pi]}e^{it\\lambda}dN(\\lambda)\\) con \\(\\{B(\\lambda)\\}\\) y \\(\\{N(\\lambda)\\}\\) definidos como en los Ejemplos 6.44 y 6.45. En tales casos los procesos deben de hecho tener la misma función de autocovarianza. Ejemplo 6.46 (Representación espectral de un movimiento browniano) Sea \\(Z(\\lambda)=B(\\lambda)\\) un movimiento browniano en \\([-\\pi,\\pi]\\) como el definido en el Ejemplo con \\(\\mathbb{E}Z(\\lambda)=0\\) y \\(Var(Z(\\lambda))=\\sigma^2(\\lambda+\\pi)/2\\pi,-\\pi\\leq\\lambda\\leq\\pi\\). Para \\(t\\in\\mathbb{Z}\\), hagamos \\(g_t(v)=\\sqrt{2}\\cos(tv)I_{(-\\pi,0]}(v)+\\sqrt{2}\\sin(tv)I_{(0,\\pi]}(v)\\) y \\[\\begin{equation} X_t=\\int_{(-\\pi,\\pi]}g_t(v)dB(v)=\\sqrt{2}\\left(\\int_{(-\\pi,0]}\\cos(tv)dB(v)+\\int_{(0,\\pi]}\\sin(tv)dB(v)\\right). \\tag{6.213} \\end{equation}\\] Entonces \\(\\mathbb{E}X_t=0\\) por (6.205) y por (6.203), \\[\\begin{equation} \\mathbb{E}(X_{t+h}X_t)=\\frac{\\sigma^2}{2\\pi}\\int_{(-\\pi,\\pi]}g_{t+h}(v)g_t(v)dv=\\frac{\\sigma^2}{2\\pi}2\\int_0^{\\pi}\\cos(hv)dv. \\tag{6.214} \\end{equation}\\] Por lo tanto \\(\\mathbb{E}(X_{t+h}X_t)=\\sigma^2\\delta_{h,0}\\) y en consecuencia \\(\\{X_t\\}\\sim WN(0,\\sigma^2)\\). Sin embargo, dado que \\(B(\\lambda)\\) es gaussiano podemos ir más allá y demostrar que las variables aleatorias \\(X_t,t=0,\\pm1,\\ldots\\), son independientes con \\(X_t\\sim N(0,\\sigma^2)\\). Para demostrar esto, sea \\(s_1,\\ldots,s_k\\), \\(k\\) enteros distintos y para cada \\(j\\) fijo sea \\(\\{f_j^{(n)}\\}\\) una sucesión de elementos de \\(\\mathcal{D}\\), esto es, funciones de la forma (), tal que \\(f_j^{(n)}\\to g_{sj}(\\cdot)\\) en \\(L^2(F)\\). Dado que la aplicación \\(I_n\\) es un isomorfismo de \\(\\overline{\\mathcal{D}}=L^2(F)\\) en \\(I_B(\\overline{\\mathcal{D}})\\), concluimos de (6.213) que \\[\\begin{equation} \\theta_1I_B(f_1^{(n)})+\\cdots+\\theta_kI_B(f_k^{(n)})\\overset{m.s}{\\to}\\theta_1X_{s_1}+\\cdots+\\theta_kX_{s_k}. \\tag{6.215} \\end{equation}\\] El lado izquierdo \\(I_B(\\sum_{j=1}^{k}\\theta_jf_j^{(n)})\\) es claramente normalmente distribuido con media cero y varianza \\(\\|\\sum_{j=1}^{k}\\theta_jf_j^{(n)}\\|^2\\). La función característica de \\(I_B(\\sum_{j=1}^{k}\\theta_jf_j^{(n)})\\) es por lo tanto \\[\\phi_n(u)=\\exp\\left[-\\frac{1}{2}u^2\\left\\|\\sum_{j=1}^{k}\\theta_jf_j^{(n)}\\right\\|_{L^2(F)}^2\\right].\\] Por continuidad del producto interior en \\(L^2(F)\\), cuando \\(n\\to\\infty\\) \\[\\left\\|\\sum_{j=1}^{k}\\theta_jf_j^{(n)}\\right\\|_{L^2(F)}^2\\to\\left\\|\\sum_{j=1}^{n}\\theta_je^{s_j\\cdot}\\right\\|_{L^2(F)}^2=\\sigma^2\\sum_{j=1}^{k}\\theta_j^2.\\] De (6.215) concluimos por lo tanto que \\(\\sum_{j=1}^{k}\\theta_jX_{s_j}\\) tiene función característica gaussiana \\[\\phi(u)=\\lim_{n\\to\\infty}\\phi_n(u)=\\exp\\left[-\\frac{1}{2}u^2\\sigma^2\\sum_{j=1}^{k}\\theta_j^2\\right].\\] Ya que esto es cierto para toda elección de \\(\\theta_1,\\ldots,\\theta_k\\) deducimos que \\(X_{s_1},\\ldots,X_{s_k}\\) son conjuntamente normal. De la covarianza (6.214) se sigue entonces que las variables aleatorias \\(X_t,t=0,\\pm1,\\ldots\\), son iid\\(N(0,\\sigma^2)\\). Observación 4. Si \\(A\\) es un subconjunto de Borel de \\([-\\pi,\\pi]\\), es conveniente en la siguiente proposición (y en otros lugares) definir \\[\\begin{equation} \\int_A f(v)dZ(v)=\\int_{(-\\pi,\\pi]}f(v)I_A(v)dZ(v), \\tag{6.216} \\end{equation}\\] donde el lado derecho ya ha sido definido en la Sección Integración con Respecto a un Proceso de Incremento Ortogonal. Proposición 6.14 Supóngase que la función de distribución espectral \\(F\\) de un proceso estacionario \\(\\{X_t\\}\\) tiene un punto de discontinuidad en \\(\\lambda_0\\) donde \\(-\\pi&lt;\\lambda_0&lt;\\pi\\). Entonces con probabilidad uno, \\[X_t=\\int_{(-\\pi,\\pi]\\backslash\\{\\lambda_0\\}}e^{itv}dZ(v)+(Z(\\lambda_0)-Z(\\lambda_0^-))e^{it\\lambda_0}\\] donde los dos términos del lado derecho son no-correlacionados y \\[Var(Z(\\lambda_0)-Z(\\lambda_0^-))=F(\\lambda_0)-F(\\lambda_0^-).\\] Demostración. El límite izquierdo \\(Z(\\lambda_0^-)\\) se define como \\[\\begin{equation} Z(\\lambda_0^-)=m.s.\\lim_{n\\to\\infty}Z(\\lambda_n) \\tag{6.217} \\end{equation}\\] donde \\(\\lambda_n\\) es una sucesión tal que \\(\\lambda_n\\uparrow\\lambda_0\\).\\ Para verificar que (6.217) tiene sentido, note que \\(\\{Z(\\lambda_n)\\}\\) es una sucesión de Cauchy ya que \\(\\|Z(\\lambda_n)-Z(\\lambda_m)\\|^2=|F(\\lambda_n)-F(\\lambda_m)|\\to0\\) cuando \\(m,n\\to\\infty\\). Por lo tanto el límite en (6.217) existe. Más aún, si \\(\\nu_n\\uparrow\\lambda_0\\) cuando \\(n\\to\\infty\\) entonces \\(\\|Z(\\lambda_n)-Z(\\nu_n)\\|^2=|F(\\lambda_n)-F(\\nu_n)|\\to0\\) cuando \\(n\\to\\infty\\), y en consecuencia el límite () es el mismo para toda sucesión no decreciente con límite \\(\\lambda_0\\).\\ Para \\(\\delta&gt;0\\) definamos \\(\\lambda_{\\pm\\delta}=\\lambda_0\\pm\\delta\\). Ahora por representación espectral, si \\(0&lt;\\delta&lt;\\pi-|\\lambda_0|\\), \\[\\begin{equation} X_t=\\int_{(-\\pi,\\pi]\\backslash(\\lambda_{-\\delta},\\lambda_{\\delta}]}e^{itv}dZ(v)+\\int_{(\\lambda_{-\\delta},\\lambda_{\\delta}]}e^{itv}dZ(v). \\tag{6.218} \\end{equation}\\] Note que los dos términos son no-correlacionados ya que las regiones de integración son disjuntas. Ahora cuando \\(\\delta\\to0\\) el primer término converge en media cuadrado a \\(\\int_{(-\\pi,\\pi]\\backslash\\{\\lambda_0\\}}e^{itv}dZ(v)\\) dado que \\[e^{it\\cdot}I_{90-\\pi,\\pi]\\backslash(\\lambda_{-\\delta},\\lambda_{\\delta}]}\\to e^{it\\cdot}I_{(-\\pi,\\pi]\\backslash\\{\\lambda_0\\}}\\text{ en }L^2(F).\\] Para ver cómo el último término de (6.218) se comporta como \\(\\delta\\to0\\) usamos la desigualdad \\[\\begin{eqnarray} \\left\\|\\int_{(\\lambda_{-\\delta},\\lambda_{\\delta}]}e^{itv}dZ(v)-e^{it\\lambda_0}(Z(\\lambda_0)-Z(\\lambda_0^-))\\right\\|&amp;\\leq&amp;\\left\\|\\int_{(\\lambda_{-\\delta},\\lambda_{\\delta}]}e^{itv}dZ(v)-e^{it\\lambda_0}(Z(\\lambda_{\\delta})-Z(\\lambda_{-\\delta}))\\right\\| \\nonumber\\\\ &amp;+&amp;\\left\\|Z(\\lambda_{\\delta})-Z(\\lambda_{-\\delta})-(Z(\\lambda_0)-Z(\\lambda_0^-))\\right\\| \\tag{6.219} \\end{eqnarray}\\] Cuando \\(\\delta\\to0\\) el segundo término de la derecha de (6.219) tiende a cero por la continuidad a la derecha de \\(\\{Z(\\lambda)\\}\\) y la definición de \\(Z(\\lambda_0^-)\\). El primer término del lado derecho de (6.219) se puede escribir como \\[\\begin{eqnarray*} \\left\\|\\int_{(-\\pi,\\pi]}(e^{itv}-e^{it\\lambda_0})I_{(\\lambda_{-\\delta},\\lambda_{\\delta}]}(v)dZ(v)\\right\\|&amp;=&amp;\\left\\|(e^{it\\cdot}-e^{it\\lambda_0})I_{(\\lambda_{-\\delta},\\lambda_{\\delta}]}(\\cdot)\\right\\|_{L^2(F)}\\\\ &amp;\\leq&amp;\\left[\\sup_{\\lambda_{-\\delta}\\leq\\lambda\\leq\\lambda_{\\delta}}|e^{it\\lambda}-e^{it\\lambda_0}|^2(F(\\lambda_{\\delta})-F(\\lambda_{-\\delta}))\\right]^{1/2} \\end{eqnarray*}\\] este tiende 0 cuando \\(\\delta\\to0\\), por la continuidad de la función \\(e^{it\\cdot}\\). Por lo tanto, deducimos de (6.219) que \\[\\int_{(\\lambda_{-\\delta},\\lambda_{\\delta}]}e^{itv}dZ(v)\\overset{m.s.}{\\to}e^{it\\lambda_0}(Z(\\lambda_0)-Z(\\lambda_0^-))\\text{ cuando }\\delta\\to0.\\] La continuidad del producto interior y la ortogonalidad de las dos integrales en (6.218) garantiza que sus límites en media cuadrado son también ortogonales. Más aún \\[Var(Z(\\lambda_0)-Z(\\lambda_0^-))=\\lim_{\\lambda_n\\uparrow\\lambda_0}Var(Z(\\lambda_0)-Z(\\lambda_n))=F(\\lambda_0)-F(\\lambda_0^-).\\] Si la función de densidad espectral tiene \\(k\\) puntos de discontinuidad en \\(\\lambda_1,\\ldots,\\lambda_k\\) entonces \\(\\{X_t\\}\\) tiene la representación \\[\\begin{equation} X_t=\\int_{(-\\pi,\\pi]\\backslash\\{\\lambda_1,\\ldots,\\lambda_k\\}}e^{itv}dZ(v)+\\sum_{j=1}^{k}(Z(\\lambda_j)-Z(\\lambda_j^-))e^{it\\lambda_j}, \\tag{6.220} \\end{equation}\\] donde los \\((k+1)\\) términos del lado derecho son no-correlacionados. La importancia de (6.220) en el análisis de series de tiempo es inmenso. El proceso \\(Y_t=(Z(\\lambda_0)-Z(\\lambda_0^-))e^{it\\lambda_0}\\) se dice ser determinístico ya que \\(Y_t\\) está determinado para todo \\(t\\) si \\(Y_{t_0}\\) es conocido para algún \\(t_0\\). La existencia de una discontinuidad en la función de densidad espectral en una frecuencia dada \\(\\lambda_0\\) por lo tanto indica la presencia en la serie de tiempo de una componente determinística sinusoidal con frecuencia \\(\\lambda_0\\). Véase Bhat, R.R. (1985). Modern Probability Theory, 2nd ed. New York, Wiley, pag157.↩ Algunas identidades que pueden ayudar aquí: \\(e^{i\\alpha}=\\cos(\\alpha)+i\\sin(\\alpha)\\), así \\(\\cos(\\alpha)=(e^{i\\alpha}+e^{-i\\alpha})/2\\) y \\(\\sin(\\alpha)=(e^{i\\alpha}-e^{-i\\alpha})/2i\\).↩ \\(\\ln(1+p)=p-\\frac{p^2}{2}+\\frac{p^3}{3}-\\cdots\\) para \\(-1&lt;p\\leq1\\). Si \\(p\\) es un porcentaje de cambio pequeño, entonces los términos de orden superior en el desarrollo son despreciables.↩ Véase Bhat, R.R. (1985). Modern Probability Theory, 2nd ed. New York, Wiley, pag157.↩ Algunas identidades que pueden ayudar aquí: \\(e^{i\\alpha}=\\cos(\\alpha)+i\\sin(\\alpha)\\), así \\(\\cos(\\alpha)=(e^{i\\alpha}+e^{-i\\alpha})/2\\) y \\(\\sin(\\alpha)=(e^{i\\alpha}-e^{-i\\alpha})/2i\\).↩ Note que \\(\\sum_{t=1}^{n}z^t=z\\frac{1-z^n}{1-z}\\) para \\(z\\neq1\\).↩ Recuerde que \\(\\sum_{t=1}^{n}\\cos^2(2\\pi\\omega_jt)=\\sum_{t=1}^{n}\\sin^2(2\\pi\\omega_jt)=n/2\\) para \\(j\\neq0\\) o un múltiplo de \\(n\\). También \\(\\sum_{t=1}^{n}\\cos(2\\pi\\omega_jt)\\sin(2\\pi\\omega_kt)=0\\) para cada \\(j\\) y \\(k\\).↩ Esto significa que \\(\\omega_{j:n}\\) es una frecuencia de la forma \\(j_n/n\\) donde \\(\\{j_n\\}\\) es una sucesión de enteros elegidos de modo que \\(j_n/n\\to\\omega\\) cuando \\(n\\to\\infty\\).↩ De la definición 6.13 tenemos que \\(I(0)=n\\bar{x}^2\\), así, el resultado análogo para el caso \\(\\omega=0\\) es \\(\\mathbb{E}[I(0)]-n\\mu^2=n\\text{var}(\\bar{x})\\to f(0)\\) cuando \\(n\\to\\infty\\).↩ Si \\(Y_j\\sim\\text{iid}(0,\\sigma^2)\\) y \\(\\{a_j\\}\\) son constantes para las cuales \\(\\sum_{j=1}^{n}a_j^2/\\max_{1\\leq j\\leq n}a_j^2\\to\\infty\\) cuando \\(n\\to\\infty\\), entonces \\(\\sum_{j=1}^{n}a_jY_j\\sim AN\\left(0,\\sigma^2\\sum_{j=1}^{n}a_j^2\\right)\\); la notación \\(AN\\) significa asintóticamente normal.↩ Las condiciones, las cuales son suficientes, son que \\(x_t\\) es un proceso lineal, como el descrito en la Proposición 6.11, con \\(\\sum_{j&gt;0}\\sqrt{j}|\\psi_j|&lt;\\infty\\), y \\(w_t\\) tiene momento finito de orden cuarto.↩ La transformación logarítmica es la transformación de estabilización de la varianza en este caso.↩ El espacio cerrado \\(\\overline{sp}\\{x_t,t\\in T\\}\\) de cada subconjunto \\(\\{x_t,t\\in T\\}\\) de un espacio de Hilbert \\(\\mathcal{H}\\) se define como el subespacio cerrado más pequeño de \\(\\mathcal{H}\\) el cual contiene todos los elementos \\(x_t, t\\in T\\).↩ "],
["referencias.html", "Referencias", " Referencias "]
]
